{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/am/anaconda3/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self,n_input=2,n_hidden=4, n_output=1, act_func=[tf.nn.relu, tf.nn.sigmoid], learning_rate= 0.001):\n",
    "        self.n_input = n_input # Number of inputs to the neuron\n",
    "        self.act_fn = act_func\n",
    "        seed = 456\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, name='X', shape=[None,n_input])\n",
    "        self.y = tf.placeholder(tf.float32, name='Y')\n",
    "                \n",
    "        # Build the graph for a single neuron\n",
    "        # Hidden layer\n",
    "        self.W1 = tf.Variable(tf.random_normal([n_input,n_hidden], stddev=2, seed = seed), name = \"weights\")\n",
    "        self.b1 = tf.Variable(tf.random_normal([1, n_hidden], seed = seed), name=\"bias\")\n",
    "        tf.summary.histogram(\"Weights_Layer_1\",self.W1)\n",
    "        tf.summary.histogram(\"Bias_Layer_1\", self.b1)\n",
    "        \n",
    "        \n",
    "        # Output Layer\n",
    "        self.W2 = tf.Variable(tf.random_normal([n_hidden,n_output], stddev=2, seed = seed), name = \"weights\")\n",
    "        self.b2 = tf.Variable(tf.random_normal([1, n_output], seed = seed), name=\"bias\")\n",
    "        tf.summary.histogram(\"Weights_Layer_2\",self.W2)\n",
    "        tf.summary.histogram(\"Bias_Layer_2\", self.b2)\n",
    "               \n",
    "        \n",
    "        activity1 = tf.matmul(self.X, self.W1) + self.b1\n",
    "        h1 = self.act_fn[0](activity1)\n",
    "        \n",
    "        activity2 = tf.matmul(h1, self.W2) + self.b2\n",
    "        self.y_hat = self.act_fn[1](activity2)\n",
    "        \n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.y_hat, labels=self.y))\n",
    "        self.opt =  tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.loss)\n",
    "        \n",
    "        \n",
    "        tf.summary.scalar(\"loss\",self.loss)\n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "        self.merge = tf.summary.merge_all()\n",
    "        self.writer = tf.summary.FileWriter(\"logs/\", graph=tf.get_default_graph())\n",
    "        \n",
    "        \n",
    "        \n",
    "    def train(self, X, Y, X_val, Y_val, epochs=100):\n",
    "        epoch = 0\n",
    "        X, Y = shuffle(X,Y)\n",
    "        loss = []\n",
    "        loss_val = []\n",
    "        while epoch < epochs:\n",
    "            # Run the optimizer for the whole training set batch wise (Stochastic Gradient Descent)    \n",
    "            merge, _, l = self.sess.run([self.merge,self.opt,self.loss], feed_dict={self.X: X, self.y: Y})\n",
    "            l_val = self.sess.run(self.loss, feed_dict={self.X: X_val, self.y: Y_val})\n",
    "            \n",
    "            loss.append(l)\n",
    "            loss_val.append(l_val)\n",
    "            self.writer.add_summary(merge, epoch)\n",
    "                \n",
    "            if epoch % 10 == 0:\n",
    "                print(\"Epoch {}/{}  training loss: {} Validation loss {}\".\\\n",
    "                      format(epoch,epochs,l, l_val ))\n",
    "                \n",
    "               \n",
    "            epoch += 1\n",
    "        return loss, loss_val\n",
    "            \n",
    "    def predict(self, X):\n",
    "        return self.sess.run(self.y_hat, feed_dict={self.X: X})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/am/anaconda3/envs/tensorflow/lib/python3.5/site-packages/pandas/core/indexing.py:194: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "filename = 'winequality-red.csv'  #Download the file from https://archive.ics.uci.edu/ml/datasets/wine+quality\n",
    "df = pd.read_csv(filename, sep=';')\n",
    "columns = df.columns.values\n",
    "# Preprocessing and Categorizing wine into two categories\n",
    "X, Y = df[columns[0:-1]], df[columns[-1]]\n",
    "scaler = MinMaxScaler()\n",
    "X_new = scaler.fit_transform(X)\n",
    "#Y.loc[(Y<3.5)]=3\n",
    "Y.loc[(Y<5.5) ] = 2\n",
    "Y.loc[(Y>=5.5)] = 1\n",
    "Y_new = pd.get_dummies(Y)  # One hot encode\n",
    "X_train, X_val, Y_train, y_val = \\\n",
    "  train_test_split(X_new, Y_new, test_size=0.2, random_state=333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, d = X_train.shape\n",
    "_, n = Y_train.shape\n",
    "model = MLP(n_input=d, n_hidden=5, n_output=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10000  training loss: 0.7775369882583618 Validation loss 0.7752965688705444\n",
      "Epoch 10/10000  training loss: 0.7724501490592957 Validation loss 0.7697949409484863\n",
      "Epoch 20/10000  training loss: 0.7675266861915588 Validation loss 0.7644182443618774\n",
      "Epoch 30/10000  training loss: 0.762906551361084 Validation loss 0.7592542767524719\n",
      "Epoch 40/10000  training loss: 0.7585335969924927 Validation loss 0.7544096112251282\n",
      "Epoch 50/10000  training loss: 0.7543628811836243 Validation loss 0.7501053214073181\n",
      "Epoch 60/10000  training loss: 0.7504240870475769 Validation loss 0.7462259531021118\n",
      "Epoch 70/10000  training loss: 0.7465348243713379 Validation loss 0.7425655722618103\n",
      "Epoch 80/10000  training loss: 0.742756724357605 Validation loss 0.7391285300254822\n",
      "Epoch 90/10000  training loss: 0.7392124533653259 Validation loss 0.7359191179275513\n",
      "Epoch 100/10000  training loss: 0.7360482811927795 Validation loss 0.7330106496810913\n",
      "Epoch 110/10000  training loss: 0.7333075404167175 Validation loss 0.7304995656013489\n",
      "Epoch 120/10000  training loss: 0.7309179902076721 Validation loss 0.7282122373580933\n",
      "Epoch 130/10000  training loss: 0.7287744879722595 Validation loss 0.7262550592422485\n",
      "Epoch 140/10000  training loss: 0.7268343567848206 Validation loss 0.724528968334198\n",
      "Epoch 150/10000  training loss: 0.7250673770904541 Validation loss 0.7229725122451782\n",
      "Epoch 160/10000  training loss: 0.7234334945678711 Validation loss 0.7215310335159302\n",
      "Epoch 170/10000  training loss: 0.7219551801681519 Validation loss 0.7201628684997559\n",
      "Epoch 180/10000  training loss: 0.7205426692962646 Validation loss 0.7188938856124878\n",
      "Epoch 190/10000  training loss: 0.7192139625549316 Validation loss 0.7176607251167297\n",
      "Epoch 200/10000  training loss: 0.7179572582244873 Validation loss 0.7164386510848999\n",
      "Epoch 210/10000  training loss: 0.7167302966117859 Validation loss 0.7152442336082458\n",
      "Epoch 220/10000  training loss: 0.7155331969261169 Validation loss 0.7140769362449646\n",
      "Epoch 230/10000  training loss: 0.7143719792366028 Validation loss 0.712973952293396\n",
      "Epoch 240/10000  training loss: 0.7132657170295715 Validation loss 0.7119488716125488\n",
      "Epoch 250/10000  training loss: 0.7122061252593994 Validation loss 0.7109540700912476\n",
      "Epoch 260/10000  training loss: 0.711207389831543 Validation loss 0.7099873423576355\n",
      "Epoch 270/10000  training loss: 0.7102589011192322 Validation loss 0.7090572118759155\n",
      "Epoch 280/10000  training loss: 0.7093551158905029 Validation loss 0.7081624269485474\n",
      "Epoch 290/10000  training loss: 0.7084948420524597 Validation loss 0.7073277831077576\n",
      "Epoch 300/10000  training loss: 0.7076725363731384 Validation loss 0.7065316438674927\n",
      "Epoch 310/10000  training loss: 0.7068997025489807 Validation loss 0.7057813405990601\n",
      "Epoch 320/10000  training loss: 0.706176221370697 Validation loss 0.7050619721412659\n",
      "Epoch 330/10000  training loss: 0.705501139163971 Validation loss 0.7043936848640442\n",
      "Epoch 340/10000  training loss: 0.704857349395752 Validation loss 0.7037763595581055\n",
      "Epoch 350/10000  training loss: 0.7042520046234131 Validation loss 0.703194260597229\n",
      "Epoch 360/10000  training loss: 0.7036718130111694 Validation loss 0.7026531100273132\n",
      "Epoch 370/10000  training loss: 0.7031150460243225 Validation loss 0.7021345496177673\n",
      "Epoch 380/10000  training loss: 0.702576220035553 Validation loss 0.701634407043457\n",
      "Epoch 390/10000  training loss: 0.7020614147186279 Validation loss 0.7011531591415405\n",
      "Epoch 400/10000  training loss: 0.701576292514801 Validation loss 0.700692892074585\n",
      "Epoch 410/10000  training loss: 0.7011190056800842 Validation loss 0.7002549171447754\n",
      "Epoch 420/10000  training loss: 0.7006850838661194 Validation loss 0.6998386383056641\n",
      "Epoch 430/10000  training loss: 0.7002701163291931 Validation loss 0.6994432806968689\n",
      "Epoch 440/10000  training loss: 0.6998735070228577 Validation loss 0.6990615129470825\n",
      "Epoch 450/10000  training loss: 0.6994931697845459 Validation loss 0.6986933946609497\n",
      "Epoch 460/10000  training loss: 0.6991286277770996 Validation loss 0.6983489990234375\n",
      "Epoch 470/10000  training loss: 0.6987789273262024 Validation loss 0.6980180144309998\n",
      "Epoch 480/10000  training loss: 0.6984452605247498 Validation loss 0.6977044343948364\n",
      "Epoch 490/10000  training loss: 0.6981257796287537 Validation loss 0.6973980665206909\n",
      "Epoch 500/10000  training loss: 0.6978162527084351 Validation loss 0.697100818157196\n",
      "Epoch 510/10000  training loss: 0.6975154280662537 Validation loss 0.6968082189559937\n",
      "Epoch 520/10000  training loss: 0.6972255110740662 Validation loss 0.6965231895446777\n",
      "Epoch 530/10000  training loss: 0.6969466805458069 Validation loss 0.6962423324584961\n",
      "Epoch 540/10000  training loss: 0.6966776847839355 Validation loss 0.6959656476974487\n",
      "Epoch 550/10000  training loss: 0.6964118480682373 Validation loss 0.6956896781921387\n",
      "Epoch 560/10000  training loss: 0.6961490511894226 Validation loss 0.6954091787338257\n",
      "Epoch 570/10000  training loss: 0.6958920955657959 Validation loss 0.6951327919960022\n",
      "Epoch 580/10000  training loss: 0.6956433057785034 Validation loss 0.6948652267456055\n",
      "Epoch 590/10000  training loss: 0.695389449596405 Validation loss 0.6945915818214417\n",
      "Epoch 600/10000  training loss: 0.6951261162757874 Validation loss 0.6943058967590332\n",
      "Epoch 610/10000  training loss: 0.6948657631874084 Validation loss 0.6940203905105591\n",
      "Epoch 620/10000  training loss: 0.694608211517334 Validation loss 0.6937397718429565\n",
      "Epoch 630/10000  training loss: 0.6943535208702087 Validation loss 0.6934615969657898\n",
      "Epoch 640/10000  training loss: 0.6940811276435852 Validation loss 0.6931588053703308\n",
      "Epoch 650/10000  training loss: 0.6937869787216187 Validation loss 0.6928346753120422\n",
      "Epoch 660/10000  training loss: 0.693478524684906 Validation loss 0.6924926042556763\n",
      "Epoch 670/10000  training loss: 0.6931628584861755 Validation loss 0.692142128944397\n",
      "Epoch 680/10000  training loss: 0.6928609609603882 Validation loss 0.691787600517273\n",
      "Epoch 690/10000  training loss: 0.6925711631774902 Validation loss 0.6914433836936951\n",
      "Epoch 700/10000  training loss: 0.692284345626831 Validation loss 0.6911207437515259\n",
      "Epoch 710/10000  training loss: 0.6919876933097839 Validation loss 0.6907671689987183\n",
      "Epoch 720/10000  training loss: 0.6916794776916504 Validation loss 0.690395712852478\n",
      "Epoch 730/10000  training loss: 0.6913600564002991 Validation loss 0.6900228261947632\n",
      "Epoch 740/10000  training loss: 0.6910244226455688 Validation loss 0.6896693706512451\n",
      "Epoch 750/10000  training loss: 0.6906902194023132 Validation loss 0.6893244981765747\n",
      "Epoch 760/10000  training loss: 0.6903477907180786 Validation loss 0.6889756917953491\n",
      "Epoch 770/10000  training loss: 0.6900126338005066 Validation loss 0.6886249780654907\n",
      "Epoch 780/10000  training loss: 0.6896826028823853 Validation loss 0.6882598400115967\n",
      "Epoch 790/10000  training loss: 0.689351499080658 Validation loss 0.6878962516784668\n",
      "Epoch 800/10000  training loss: 0.6889985203742981 Validation loss 0.6875163316726685\n",
      "Epoch 810/10000  training loss: 0.6886302828788757 Validation loss 0.6871249675750732\n",
      "Epoch 820/10000  training loss: 0.6882596015930176 Validation loss 0.6867390871047974\n",
      "Epoch 830/10000  training loss: 0.6878810524940491 Validation loss 0.686356246471405\n",
      "Epoch 840/10000  training loss: 0.6874903440475464 Validation loss 0.6859639883041382\n",
      "Epoch 850/10000  training loss: 0.6870831251144409 Validation loss 0.6855558156967163\n",
      "Epoch 860/10000  training loss: 0.68666011095047 Validation loss 0.6851388812065125\n",
      "Epoch 870/10000  training loss: 0.6862279772758484 Validation loss 0.6847245693206787\n",
      "Epoch 880/10000  training loss: 0.6858022809028625 Validation loss 0.6843106746673584\n",
      "Epoch 890/10000  training loss: 0.6853739023208618 Validation loss 0.683894157409668\n",
      "Epoch 900/10000  training loss: 0.6849409937858582 Validation loss 0.6834773421287537\n",
      "Epoch 910/10000  training loss: 0.6845031380653381 Validation loss 0.6830512285232544\n",
      "Epoch 920/10000  training loss: 0.6840565204620361 Validation loss 0.6826177835464478\n",
      "Epoch 930/10000  training loss: 0.683602511882782 Validation loss 0.682170569896698\n",
      "Epoch 940/10000  training loss: 0.6831376552581787 Validation loss 0.6817119121551514\n",
      "Epoch 950/10000  training loss: 0.6826608180999756 Validation loss 0.6812425851821899\n",
      "Epoch 960/10000  training loss: 0.6821721196174622 Validation loss 0.6807616353034973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 970/10000  training loss: 0.6816730499267578 Validation loss 0.680275559425354\n",
      "Epoch 980/10000  training loss: 0.6811636686325073 Validation loss 0.6797801852226257\n",
      "Epoch 990/10000  training loss: 0.6806437969207764 Validation loss 0.6792728304862976\n",
      "Epoch 1000/10000  training loss: 0.6801129579544067 Validation loss 0.678753137588501\n",
      "Epoch 1010/10000  training loss: 0.6795722842216492 Validation loss 0.6782246232032776\n",
      "Epoch 1020/10000  training loss: 0.6790160536766052 Validation loss 0.6776813268661499\n",
      "Epoch 1030/10000  training loss: 0.6784425973892212 Validation loss 0.6771241426467896\n",
      "Epoch 1040/10000  training loss: 0.6778583526611328 Validation loss 0.6765502691268921\n",
      "Epoch 1050/10000  training loss: 0.6772593855857849 Validation loss 0.6759544014930725\n",
      "Epoch 1060/10000  training loss: 0.6766466498374939 Validation loss 0.6753301024436951\n",
      "Epoch 1070/10000  training loss: 0.6760187149047852 Validation loss 0.6746846437454224\n",
      "Epoch 1080/10000  training loss: 0.6753714084625244 Validation loss 0.6740164756774902\n",
      "Epoch 1090/10000  training loss: 0.6747003197669983 Validation loss 0.6733242273330688\n",
      "Epoch 1100/10000  training loss: 0.6740007996559143 Validation loss 0.6726053357124329\n",
      "Epoch 1110/10000  training loss: 0.6732738018035889 Validation loss 0.6718573570251465\n",
      "Epoch 1120/10000  training loss: 0.6725198030471802 Validation loss 0.6710771322250366\n",
      "Epoch 1130/10000  training loss: 0.6717360019683838 Validation loss 0.6702603101730347\n",
      "Epoch 1140/10000  training loss: 0.670916736125946 Validation loss 0.6694024801254272\n",
      "Epoch 1150/10000  training loss: 0.6700562834739685 Validation loss 0.6684986352920532\n",
      "Epoch 1160/10000  training loss: 0.669154942035675 Validation loss 0.6675443649291992\n",
      "Epoch 1170/10000  training loss: 0.6682115197181702 Validation loss 0.66656094789505\n",
      "Epoch 1180/10000  training loss: 0.6672243475914001 Validation loss 0.6655251979827881\n",
      "Epoch 1190/10000  training loss: 0.666189968585968 Validation loss 0.6644324660301208\n",
      "Epoch 1200/10000  training loss: 0.6651027202606201 Validation loss 0.6632854342460632\n",
      "Epoch 1210/10000  training loss: 0.6639546751976013 Validation loss 0.6620787382125854\n",
      "Epoch 1220/10000  training loss: 0.6627371907234192 Validation loss 0.6608046293258667\n",
      "Epoch 1230/10000  training loss: 0.6614529490470886 Validation loss 0.6594510078430176\n",
      "Epoch 1240/10000  training loss: 0.6601003408432007 Validation loss 0.6580446362495422\n",
      "Epoch 1250/10000  training loss: 0.65867018699646 Validation loss 0.6565731763839722\n",
      "Epoch 1260/10000  training loss: 0.657176673412323 Validation loss 0.6550328731536865\n",
      "Epoch 1270/10000  training loss: 0.6556225419044495 Validation loss 0.6534391641616821\n",
      "Epoch 1280/10000  training loss: 0.6540169715881348 Validation loss 0.6517978310585022\n",
      "Epoch 1290/10000  training loss: 0.6523528099060059 Validation loss 0.6501083374023438\n",
      "Epoch 1300/10000  training loss: 0.6506348848342896 Validation loss 0.6483561992645264\n",
      "Epoch 1310/10000  training loss: 0.6488611102104187 Validation loss 0.6465570330619812\n",
      "Epoch 1320/10000  training loss: 0.6470299363136292 Validation loss 0.6447125673294067\n",
      "Epoch 1330/10000  training loss: 0.6451701521873474 Validation loss 0.6428247690200806\n",
      "Epoch 1340/10000  training loss: 0.6432774066925049 Validation loss 0.6409096121788025\n",
      "Epoch 1350/10000  training loss: 0.6413638591766357 Validation loss 0.6389781832695007\n",
      "Epoch 1360/10000  training loss: 0.6394242644309998 Validation loss 0.6370306611061096\n",
      "Epoch 1370/10000  training loss: 0.6374721527099609 Validation loss 0.635070264339447\n",
      "Epoch 1380/10000  training loss: 0.6355290412902832 Validation loss 0.6331131458282471\n",
      "Epoch 1390/10000  training loss: 0.6335909366607666 Validation loss 0.6311576962471008\n",
      "Epoch 1400/10000  training loss: 0.6316553354263306 Validation loss 0.6292079091072083\n",
      "Epoch 1410/10000  training loss: 0.6297215819358826 Validation loss 0.6272827386856079\n",
      "Epoch 1420/10000  training loss: 0.6277888417243958 Validation loss 0.6253961324691772\n",
      "Epoch 1430/10000  training loss: 0.6258684992790222 Validation loss 0.6235308647155762\n",
      "Epoch 1440/10000  training loss: 0.6239809393882751 Validation loss 0.6216673254966736\n",
      "Epoch 1450/10000  training loss: 0.6221318244934082 Validation loss 0.6198267936706543\n",
      "Epoch 1460/10000  training loss: 0.6203224062919617 Validation loss 0.6180254220962524\n",
      "Epoch 1470/10000  training loss: 0.6185567378997803 Validation loss 0.6162697672843933\n",
      "Epoch 1480/10000  training loss: 0.6168376803398132 Validation loss 0.614535927772522\n",
      "Epoch 1490/10000  training loss: 0.6151692867279053 Validation loss 0.6128671765327454\n",
      "Epoch 1500/10000  training loss: 0.6135504245758057 Validation loss 0.6112552881240845\n",
      "Epoch 1510/10000  training loss: 0.6119757890701294 Validation loss 0.6096867322921753\n",
      "Epoch 1520/10000  training loss: 0.610449492931366 Validation loss 0.6081711053848267\n",
      "Epoch 1530/10000  training loss: 0.6089712977409363 Validation loss 0.6067065000534058\n",
      "Epoch 1540/10000  training loss: 0.6075425148010254 Validation loss 0.6052870750427246\n",
      "Epoch 1550/10000  training loss: 0.6061740517616272 Validation loss 0.6039206981658936\n",
      "Epoch 1560/10000  training loss: 0.6048561334609985 Validation loss 0.6026203632354736\n",
      "Epoch 1570/10000  training loss: 0.603588342666626 Validation loss 0.6013760566711426\n",
      "Epoch 1580/10000  training loss: 0.6023622751235962 Validation loss 0.6001743078231812\n",
      "Epoch 1590/10000  training loss: 0.6011796593666077 Validation loss 0.5990142822265625\n",
      "Epoch 1600/10000  training loss: 0.6000351905822754 Validation loss 0.5979019999504089\n",
      "Epoch 1610/10000  training loss: 0.5989284515380859 Validation loss 0.596837043762207\n",
      "Epoch 1620/10000  training loss: 0.5978564620018005 Validation loss 0.5957979559898376\n",
      "Epoch 1630/10000  training loss: 0.5968278646469116 Validation loss 0.5947951078414917\n",
      "Epoch 1640/10000  training loss: 0.5958288908004761 Validation loss 0.5938311815261841\n",
      "Epoch 1650/10000  training loss: 0.5948677659034729 Validation loss 0.5929059386253357\n",
      "Epoch 1660/10000  training loss: 0.5939468145370483 Validation loss 0.592014491558075\n",
      "Epoch 1670/10000  training loss: 0.5930610299110413 Validation loss 0.5911562442779541\n",
      "Epoch 1680/10000  training loss: 0.5922079682350159 Validation loss 0.5903257131576538\n",
      "Epoch 1690/10000  training loss: 0.5913807153701782 Validation loss 0.5895296335220337\n",
      "Epoch 1700/10000  training loss: 0.5905786752700806 Validation loss 0.5887742042541504\n",
      "Epoch 1710/10000  training loss: 0.5898041725158691 Validation loss 0.5880497694015503\n",
      "Epoch 1720/10000  training loss: 0.5890566110610962 Validation loss 0.5873425006866455\n",
      "Epoch 1730/10000  training loss: 0.5883341431617737 Validation loss 0.5866497159004211\n",
      "Epoch 1740/10000  training loss: 0.5876360535621643 Validation loss 0.5859713554382324\n",
      "Epoch 1750/10000  training loss: 0.5869634747505188 Validation loss 0.5853027105331421\n",
      "Epoch 1760/10000  training loss: 0.5863146185874939 Validation loss 0.5846511125564575\n",
      "Epoch 1770/10000  training loss: 0.5856866836547852 Validation loss 0.5840250253677368\n",
      "Epoch 1780/10000  training loss: 0.5850741863250732 Validation loss 0.5834289789199829\n",
      "Epoch 1790/10000  training loss: 0.5844809412956238 Validation loss 0.5828471183776855\n",
      "Epoch 1800/10000  training loss: 0.5839072465896606 Validation loss 0.5822693109512329\n",
      "Epoch 1810/10000  training loss: 0.5833530426025391 Validation loss 0.5817018151283264\n",
      "Epoch 1820/10000  training loss: 0.5828192830085754 Validation loss 0.5811548233032227\n",
      "Epoch 1830/10000  training loss: 0.582303524017334 Validation loss 0.5806236863136292\n",
      "Epoch 1840/10000  training loss: 0.5818039178848267 Validation loss 0.5801085233688354\n",
      "Epoch 1850/10000  training loss: 0.5813179016113281 Validation loss 0.5796058773994446\n",
      "Epoch 1860/10000  training loss: 0.580845296382904 Validation loss 0.5791141390800476\n",
      "Epoch 1870/10000  training loss: 0.580385148525238 Validation loss 0.5786356925964355\n",
      "Epoch 1880/10000  training loss: 0.5799355506896973 Validation loss 0.5781687498092651\n",
      "Epoch 1890/10000  training loss: 0.5794942378997803 Validation loss 0.5777291059494019\n",
      "Epoch 1900/10000  training loss: 0.5790583491325378 Validation loss 0.5773251056671143\n",
      "Epoch 1910/10000  training loss: 0.5786301493644714 Validation loss 0.57691890001297\n",
      "Epoch 1920/10000  training loss: 0.5782113075256348 Validation loss 0.5765138864517212\n",
      "Epoch 1930/10000  training loss: 0.5778034925460815 Validation loss 0.576119065284729\n",
      "Epoch 1940/10000  training loss: 0.5774067044258118 Validation loss 0.5757360458374023\n",
      "Epoch 1950/10000  training loss: 0.5770182609558105 Validation loss 0.5753674507141113\n",
      "Epoch 1960/10000  training loss: 0.5766389966011047 Validation loss 0.5750068426132202\n",
      "Epoch 1970/10000  training loss: 0.5762685537338257 Validation loss 0.5746538043022156\n",
      "Epoch 1980/10000  training loss: 0.5759065747261047 Validation loss 0.574301540851593\n",
      "Epoch 1990/10000  training loss: 0.5755480527877808 Validation loss 0.5739542841911316\n",
      "Epoch 2000/10000  training loss: 0.5751938819885254 Validation loss 0.573636531829834\n",
      "Epoch 2010/10000  training loss: 0.5748379826545715 Validation loss 0.5733438730239868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2020/10000  training loss: 0.5744890570640564 Validation loss 0.5730522274971008\n",
      "Epoch 2030/10000  training loss: 0.5741474628448486 Validation loss 0.572760283946991\n",
      "Epoch 2040/10000  training loss: 0.5738109946250916 Validation loss 0.5724648237228394\n",
      "Epoch 2050/10000  training loss: 0.5734818577766418 Validation loss 0.5721743106842041\n",
      "Epoch 2060/10000  training loss: 0.5731623768806458 Validation loss 0.5718897581100464\n",
      "Epoch 2070/10000  training loss: 0.5728490948677063 Validation loss 0.5716043710708618\n",
      "Epoch 2080/10000  training loss: 0.5725417137145996 Validation loss 0.5713304281234741\n",
      "Epoch 2090/10000  training loss: 0.5722435712814331 Validation loss 0.5710548162460327\n",
      "Epoch 2100/10000  training loss: 0.5719587206840515 Validation loss 0.5707247257232666\n",
      "Epoch 2110/10000  training loss: 0.5716820955276489 Validation loss 0.5704124569892883\n",
      "Epoch 2120/10000  training loss: 0.5714117288589478 Validation loss 0.5701201558113098\n",
      "Epoch 2130/10000  training loss: 0.5711464881896973 Validation loss 0.5698281526565552\n",
      "Epoch 2140/10000  training loss: 0.5708874464035034 Validation loss 0.5695459246635437\n",
      "Epoch 2150/10000  training loss: 0.5706344246864319 Validation loss 0.5692650079727173\n",
      "Epoch 2160/10000  training loss: 0.5703868269920349 Validation loss 0.5689933896064758\n",
      "Epoch 2170/10000  training loss: 0.5701441168785095 Validation loss 0.5687176585197449\n",
      "Epoch 2180/10000  training loss: 0.5699061155319214 Validation loss 0.5684525370597839\n",
      "Epoch 2190/10000  training loss: 0.5696725845336914 Validation loss 0.5681924819946289\n",
      "Epoch 2200/10000  training loss: 0.5694433450698853 Validation loss 0.5679388642311096\n",
      "Epoch 2210/10000  training loss: 0.5692183375358582 Validation loss 0.5676866769790649\n",
      "Epoch 2220/10000  training loss: 0.5690000057220459 Validation loss 0.5674358606338501\n",
      "Epoch 2230/10000  training loss: 0.5687853097915649 Validation loss 0.5671979188919067\n",
      "Epoch 2240/10000  training loss: 0.5685755014419556 Validation loss 0.5669728517532349\n",
      "Epoch 2250/10000  training loss: 0.5683698058128357 Validation loss 0.5667532682418823\n",
      "Epoch 2260/10000  training loss: 0.5681687593460083 Validation loss 0.5665416121482849\n",
      "Epoch 2270/10000  training loss: 0.5679730176925659 Validation loss 0.5663372874259949\n",
      "Epoch 2280/10000  training loss: 0.5677803158760071 Validation loss 0.5661281943321228\n",
      "Epoch 2290/10000  training loss: 0.5675902962684631 Validation loss 0.5659189820289612\n",
      "Epoch 2300/10000  training loss: 0.567403256893158 Validation loss 0.5657156705856323\n",
      "Epoch 2310/10000  training loss: 0.567219078540802 Validation loss 0.5655156970024109\n",
      "Epoch 2320/10000  training loss: 0.5670377612113953 Validation loss 0.5653194189071655\n",
      "Epoch 2330/10000  training loss: 0.5668594241142273 Validation loss 0.5651271343231201\n",
      "Epoch 2340/10000  training loss: 0.5666835308074951 Validation loss 0.5649411082267761\n",
      "Epoch 2350/10000  training loss: 0.5665103793144226 Validation loss 0.564754843711853\n",
      "Epoch 2360/10000  training loss: 0.566339373588562 Validation loss 0.5645712614059448\n",
      "Epoch 2370/10000  training loss: 0.5661706328392029 Validation loss 0.5643884539604187\n",
      "Epoch 2380/10000  training loss: 0.5660034418106079 Validation loss 0.5642081499099731\n",
      "Epoch 2390/10000  training loss: 0.5658385753631592 Validation loss 0.5640318989753723\n",
      "Epoch 2400/10000  training loss: 0.5656740069389343 Validation loss 0.563858151435852\n",
      "Epoch 2410/10000  training loss: 0.5655109286308289 Validation loss 0.5636884570121765\n",
      "Epoch 2420/10000  training loss: 0.5653498768806458 Validation loss 0.5635180473327637\n",
      "Epoch 2430/10000  training loss: 0.5651905536651611 Validation loss 0.5633481740951538\n",
      "Epoch 2440/10000  training loss: 0.5650336742401123 Validation loss 0.5631758570671082\n",
      "Epoch 2450/10000  training loss: 0.5648789405822754 Validation loss 0.5630109310150146\n",
      "Epoch 2460/10000  training loss: 0.5647265911102295 Validation loss 0.5628442168235779\n",
      "Epoch 2470/10000  training loss: 0.564576268196106 Validation loss 0.5626757740974426\n",
      "Epoch 2480/10000  training loss: 0.5644276142120361 Validation loss 0.562515914440155\n",
      "Epoch 2490/10000  training loss: 0.5642810463905334 Validation loss 0.5623569488525391\n",
      "Epoch 2500/10000  training loss: 0.5641366243362427 Validation loss 0.5622018575668335\n",
      "Epoch 2510/10000  training loss: 0.5639932155609131 Validation loss 0.5620541572570801\n",
      "Epoch 2520/10000  training loss: 0.5638507008552551 Validation loss 0.5619053840637207\n",
      "Epoch 2530/10000  training loss: 0.5637091994285583 Validation loss 0.5617560148239136\n",
      "Epoch 2540/10000  training loss: 0.5635694861412048 Validation loss 0.5616030693054199\n",
      "Epoch 2550/10000  training loss: 0.5634310841560364 Validation loss 0.5614553689956665\n",
      "Epoch 2560/10000  training loss: 0.563294529914856 Validation loss 0.5613086819648743\n",
      "Epoch 2570/10000  training loss: 0.5631588101387024 Validation loss 0.5611580610275269\n",
      "Epoch 2580/10000  training loss: 0.5630240440368652 Validation loss 0.5610120296478271\n",
      "Epoch 2590/10000  training loss: 0.5628896355628967 Validation loss 0.560860812664032\n",
      "Epoch 2600/10000  training loss: 0.5627568960189819 Validation loss 0.5607131719589233\n",
      "Epoch 2610/10000  training loss: 0.562625527381897 Validation loss 0.5605717897415161\n",
      "Epoch 2620/10000  training loss: 0.5624951124191284 Validation loss 0.5604370832443237\n",
      "Epoch 2630/10000  training loss: 0.5623602271080017 Validation loss 0.5603410005569458\n",
      "Epoch 2640/10000  training loss: 0.5622188448905945 Validation loss 0.5602681636810303\n",
      "Epoch 2650/10000  training loss: 0.5620744228363037 Validation loss 0.5601934790611267\n",
      "Epoch 2660/10000  training loss: 0.5619312524795532 Validation loss 0.5600653290748596\n",
      "Epoch 2670/10000  training loss: 0.5617906451225281 Validation loss 0.5599331259727478\n",
      "Epoch 2680/10000  training loss: 0.5616528391838074 Validation loss 0.5598196983337402\n",
      "Epoch 2690/10000  training loss: 0.5615158081054688 Validation loss 0.5597184896469116\n",
      "Epoch 2700/10000  training loss: 0.5613794922828674 Validation loss 0.5596221089363098\n",
      "Epoch 2710/10000  training loss: 0.5612460970878601 Validation loss 0.5595253705978394\n",
      "Epoch 2720/10000  training loss: 0.5611159801483154 Validation loss 0.5594125986099243\n",
      "Epoch 2730/10000  training loss: 0.5609883666038513 Validation loss 0.5592889785766602\n",
      "Epoch 2740/10000  training loss: 0.5608627796173096 Validation loss 0.5591691732406616\n",
      "Epoch 2750/10000  training loss: 0.5607389807701111 Validation loss 0.5590506792068481\n",
      "Epoch 2760/10000  training loss: 0.5606159567832947 Validation loss 0.5589359402656555\n",
      "Epoch 2770/10000  training loss: 0.5604941844940186 Validation loss 0.5588197112083435\n",
      "Epoch 2780/10000  training loss: 0.5603737235069275 Validation loss 0.5586962699890137\n",
      "Epoch 2790/10000  training loss: 0.5602536797523499 Validation loss 0.5585553646087646\n",
      "Epoch 2800/10000  training loss: 0.5601350665092468 Validation loss 0.5584276914596558\n",
      "Epoch 2810/10000  training loss: 0.5600175261497498 Validation loss 0.5583145022392273\n",
      "Epoch 2820/10000  training loss: 0.5599009990692139 Validation loss 0.5581997632980347\n",
      "Epoch 2830/10000  training loss: 0.5597856640815735 Validation loss 0.5580801367759705\n",
      "Epoch 2840/10000  training loss: 0.55967116355896 Validation loss 0.557960033416748\n",
      "Epoch 2850/10000  training loss: 0.5595575571060181 Validation loss 0.5578416585922241\n",
      "Epoch 2860/10000  training loss: 0.5594450235366821 Validation loss 0.5577243566513062\n",
      "Epoch 2870/10000  training loss: 0.559334933757782 Validation loss 0.5576067566871643\n",
      "Epoch 2880/10000  training loss: 0.5592260360717773 Validation loss 0.5574963688850403\n",
      "Epoch 2890/10000  training loss: 0.5591180920600891 Validation loss 0.5573925971984863\n",
      "Epoch 2900/10000  training loss: 0.5590112209320068 Validation loss 0.5572891235351562\n",
      "Epoch 2910/10000  training loss: 0.5589051246643066 Validation loss 0.5571836829185486\n",
      "Epoch 2920/10000  training loss: 0.5587999224662781 Validation loss 0.5570774674415588\n",
      "Epoch 2930/10000  training loss: 0.5586954355239868 Validation loss 0.5569714307785034\n",
      "Epoch 2940/10000  training loss: 0.5585917830467224 Validation loss 0.556865394115448\n",
      "Epoch 2950/10000  training loss: 0.5584889054298401 Validation loss 0.5567589998245239\n",
      "Epoch 2960/10000  training loss: 0.5583867430686951 Validation loss 0.556652307510376\n",
      "Epoch 2970/10000  training loss: 0.5582855939865112 Validation loss 0.5565453767776489\n",
      "Epoch 2980/10000  training loss: 0.5581883788108826 Validation loss 0.5564401745796204\n",
      "Epoch 2990/10000  training loss: 0.5580923557281494 Validation loss 0.5563438534736633\n",
      "Epoch 3000/10000  training loss: 0.5579972267150879 Validation loss 0.5562490820884705\n",
      "Epoch 3010/10000  training loss: 0.5579029321670532 Validation loss 0.5561695098876953\n",
      "Epoch 3020/10000  training loss: 0.5578086376190186 Validation loss 0.5561053156852722\n",
      "Epoch 3030/10000  training loss: 0.5577155947685242 Validation loss 0.5560376048088074\n",
      "Epoch 3040/10000  training loss: 0.5576238036155701 Validation loss 0.5559700727462769\n",
      "Epoch 3050/10000  training loss: 0.5575330257415771 Validation loss 0.5559138655662537\n",
      "Epoch 3060/10000  training loss: 0.5574432015419006 Validation loss 0.5558642148971558\n",
      "Epoch 3070/10000  training loss: 0.5573549270629883 Validation loss 0.5557982325553894\n",
      "Epoch 3080/10000  training loss: 0.5572676062583923 Validation loss 0.5557166337966919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3090/10000  training loss: 0.5571812987327576 Validation loss 0.5556356310844421\n",
      "Epoch 3100/10000  training loss: 0.5570957064628601 Validation loss 0.555559515953064\n",
      "Epoch 3110/10000  training loss: 0.557009220123291 Validation loss 0.5555058717727661\n",
      "Epoch 3120/10000  training loss: 0.5569234490394592 Validation loss 0.5554431676864624\n",
      "Epoch 3130/10000  training loss: 0.55683833360672 Validation loss 0.5553659200668335\n",
      "Epoch 3140/10000  training loss: 0.5567541718482971 Validation loss 0.5552896857261658\n",
      "Epoch 3150/10000  training loss: 0.5566707253456116 Validation loss 0.5552160143852234\n",
      "Epoch 3160/10000  training loss: 0.5565881729125977 Validation loss 0.5551388263702393\n",
      "Epoch 3170/10000  training loss: 0.5565053224563599 Validation loss 0.5550637245178223\n",
      "Epoch 3180/10000  training loss: 0.5564176440238953 Validation loss 0.5550419688224792\n",
      "Epoch 3190/10000  training loss: 0.5563303828239441 Validation loss 0.5549942851066589\n",
      "Epoch 3200/10000  training loss: 0.5562436580657959 Validation loss 0.5549215078353882\n",
      "Epoch 3210/10000  training loss: 0.5561579465866089 Validation loss 0.5548567771911621\n",
      "Epoch 3220/10000  training loss: 0.556073784828186 Validation loss 0.5548018217086792\n",
      "Epoch 3230/10000  training loss: 0.5559907555580139 Validation loss 0.5547515749931335\n",
      "Epoch 3240/10000  training loss: 0.555909276008606 Validation loss 0.5547057390213013\n",
      "Epoch 3250/10000  training loss: 0.5558291077613831 Validation loss 0.554658055305481\n",
      "Epoch 3260/10000  training loss: 0.5557502508163452 Validation loss 0.5546050071716309\n",
      "Epoch 3270/10000  training loss: 0.5556724071502686 Validation loss 0.5545495748519897\n",
      "Epoch 3280/10000  training loss: 0.5555954575538635 Validation loss 0.5544916391372681\n",
      "Epoch 3290/10000  training loss: 0.5555194020271301 Validation loss 0.5544292330741882\n",
      "Epoch 3300/10000  training loss: 0.555443286895752 Validation loss 0.554362416267395\n",
      "Epoch 3310/10000  training loss: 0.5553669929504395 Validation loss 0.5542957186698914\n",
      "Epoch 3320/10000  training loss: 0.5552911162376404 Validation loss 0.5542332530021667\n",
      "Epoch 3330/10000  training loss: 0.5552158951759338 Validation loss 0.5541709661483765\n",
      "Epoch 3340/10000  training loss: 0.555141270160675 Validation loss 0.5541054010391235\n",
      "Epoch 3350/10000  training loss: 0.5550652742385864 Validation loss 0.5540329217910767\n",
      "Epoch 3360/10000  training loss: 0.5549879670143127 Validation loss 0.5539442896842957\n",
      "Epoch 3370/10000  training loss: 0.5549110174179077 Validation loss 0.5538428425788879\n",
      "Epoch 3380/10000  training loss: 0.5548346042633057 Validation loss 0.5537492036819458\n",
      "Epoch 3390/10000  training loss: 0.5547585487365723 Validation loss 0.5536654591560364\n",
      "Epoch 3400/10000  training loss: 0.5546826124191284 Validation loss 0.5535876750946045\n",
      "Epoch 3410/10000  training loss: 0.5546062588691711 Validation loss 0.553521990776062\n",
      "Epoch 3420/10000  training loss: 0.5545302033424377 Validation loss 0.5534418225288391\n",
      "Epoch 3430/10000  training loss: 0.5544546246528625 Validation loss 0.5533567667007446\n",
      "Epoch 3440/10000  training loss: 0.5543795824050903 Validation loss 0.5532712936401367\n",
      "Epoch 3450/10000  training loss: 0.5543049573898315 Validation loss 0.5531871914863586\n",
      "Epoch 3460/10000  training loss: 0.5542306900024414 Validation loss 0.5531030297279358\n",
      "Epoch 3470/10000  training loss: 0.5541569590568542 Validation loss 0.553017795085907\n",
      "Epoch 3480/10000  training loss: 0.5540836453437805 Validation loss 0.5529332160949707\n",
      "Epoch 3490/10000  training loss: 0.5540106892585754 Validation loss 0.5528477430343628\n",
      "Epoch 3500/10000  training loss: 0.5539381504058838 Validation loss 0.5527615547180176\n",
      "Epoch 3510/10000  training loss: 0.5538625717163086 Validation loss 0.5526659488677979\n",
      "Epoch 3520/10000  training loss: 0.5537857413291931 Validation loss 0.5525528192520142\n",
      "Epoch 3530/10000  training loss: 0.5537088513374329 Validation loss 0.5524314641952515\n",
      "Epoch 3540/10000  training loss: 0.5536330342292786 Validation loss 0.5523139238357544\n",
      "Epoch 3550/10000  training loss: 0.553565263748169 Validation loss 0.5522130131721497\n",
      "Epoch 3560/10000  training loss: 0.5534988641738892 Validation loss 0.5521509051322937\n",
      "Epoch 3570/10000  training loss: 0.5534325838088989 Validation loss 0.5521181225776672\n",
      "Epoch 3580/10000  training loss: 0.5533642172813416 Validation loss 0.5521273612976074\n",
      "Epoch 3590/10000  training loss: 0.5532967448234558 Validation loss 0.5520772933959961\n",
      "Epoch 3600/10000  training loss: 0.5532304644584656 Validation loss 0.5520306825637817\n",
      "Epoch 3610/10000  training loss: 0.5531647801399231 Validation loss 0.5520031452178955\n",
      "Epoch 3620/10000  training loss: 0.5530998706817627 Validation loss 0.5519646406173706\n",
      "Epoch 3630/10000  training loss: 0.5530357360839844 Validation loss 0.5519102811813354\n",
      "Epoch 3640/10000  training loss: 0.5529720783233643 Validation loss 0.5518426299095154\n",
      "Epoch 3650/10000  training loss: 0.552909791469574 Validation loss 0.5517805218696594\n",
      "Epoch 3660/10000  training loss: 0.5528485178947449 Validation loss 0.5517377257347107\n",
      "Epoch 3670/10000  training loss: 0.5527876615524292 Validation loss 0.5517052412033081\n",
      "Epoch 3680/10000  training loss: 0.5527268648147583 Validation loss 0.5516787767410278\n",
      "Epoch 3690/10000  training loss: 0.5526666045188904 Validation loss 0.5516335368156433\n",
      "Epoch 3700/10000  training loss: 0.5526068210601807 Validation loss 0.5515819191932678\n",
      "Epoch 3710/10000  training loss: 0.5525476932525635 Validation loss 0.5515323877334595\n",
      "Epoch 3720/10000  training loss: 0.5524890422821045 Validation loss 0.5514801740646362\n",
      "Epoch 3730/10000  training loss: 0.5524308681488037 Validation loss 0.5514172315597534\n",
      "Epoch 3740/10000  training loss: 0.5523731708526611 Validation loss 0.5513439178466797\n",
      "Epoch 3750/10000  training loss: 0.5523158311843872 Validation loss 0.5512749552726746\n",
      "Epoch 3760/10000  training loss: 0.5522588491439819 Validation loss 0.5512083172798157\n",
      "Epoch 3770/10000  training loss: 0.552202045917511 Validation loss 0.551143229007721\n",
      "Epoch 3780/10000  training loss: 0.5521456599235535 Validation loss 0.5510789155960083\n",
      "Epoch 3790/10000  training loss: 0.552089512348175 Validation loss 0.5510123372077942\n",
      "Epoch 3800/10000  training loss: 0.5520337224006653 Validation loss 0.5509470701217651\n",
      "Epoch 3810/10000  training loss: 0.5519781112670898 Validation loss 0.5508809089660645\n",
      "Epoch 3820/10000  training loss: 0.5519228577613831 Validation loss 0.5508147478103638\n",
      "Epoch 3830/10000  training loss: 0.5518679618835449 Validation loss 0.5507471561431885\n",
      "Epoch 3840/10000  training loss: 0.5518132448196411 Validation loss 0.5506796836853027\n",
      "Epoch 3850/10000  training loss: 0.5517575740814209 Validation loss 0.5506272912025452\n",
      "Epoch 3860/10000  training loss: 0.551697850227356 Validation loss 0.5506208539009094\n",
      "Epoch 3870/10000  training loss: 0.551636815071106 Validation loss 0.5505877733230591\n",
      "Epoch 3880/10000  training loss: 0.5515757203102112 Validation loss 0.5505498051643372\n",
      "Epoch 3890/10000  training loss: 0.5515154600143433 Validation loss 0.5505162477493286\n",
      "Epoch 3900/10000  training loss: 0.5514513850212097 Validation loss 0.5505338907241821\n",
      "Epoch 3910/10000  training loss: 0.5513874888420105 Validation loss 0.5505382418632507\n",
      "Epoch 3920/10000  training loss: 0.5513245463371277 Validation loss 0.5504986047744751\n",
      "Epoch 3930/10000  training loss: 0.5512630343437195 Validation loss 0.5504668354988098\n",
      "Epoch 3940/10000  training loss: 0.5512027144432068 Validation loss 0.5504409074783325\n",
      "Epoch 3950/10000  training loss: 0.5511434078216553 Validation loss 0.5504123568534851\n",
      "Epoch 3960/10000  training loss: 0.5510843396186829 Validation loss 0.5503824949264526\n",
      "Epoch 3970/10000  training loss: 0.5510249137878418 Validation loss 0.5503634214401245\n",
      "Epoch 3980/10000  training loss: 0.5509666204452515 Validation loss 0.5503144860267639\n",
      "Epoch 3990/10000  training loss: 0.5509088635444641 Validation loss 0.5502820611000061\n",
      "Epoch 4000/10000  training loss: 0.5508520007133484 Validation loss 0.5502517819404602\n",
      "Epoch 4010/10000  training loss: 0.5507961511611938 Validation loss 0.5502080321311951\n",
      "Epoch 4020/10000  training loss: 0.5507413148880005 Validation loss 0.5501632690429688\n",
      "Epoch 4030/10000  training loss: 0.550687313079834 Validation loss 0.5501213669776917\n",
      "Epoch 4040/10000  training loss: 0.5506341457366943 Validation loss 0.5500714182853699\n",
      "Epoch 4050/10000  training loss: 0.5505817532539368 Validation loss 0.5500192046165466\n",
      "Epoch 4060/10000  training loss: 0.5505301356315613 Validation loss 0.549973726272583\n",
      "Epoch 4070/10000  training loss: 0.5504791140556335 Validation loss 0.5499305129051208\n",
      "Epoch 4080/10000  training loss: 0.5504286289215088 Validation loss 0.5498865842819214\n",
      "Epoch 4090/10000  training loss: 0.550378680229187 Validation loss 0.5498367547988892\n",
      "Epoch 4100/10000  training loss: 0.5503293871879578 Validation loss 0.549787163734436\n",
      "Epoch 4110/10000  training loss: 0.5502803921699524 Validation loss 0.5497381687164307\n",
      "Epoch 4120/10000  training loss: 0.5502319931983948 Validation loss 0.5496875643730164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4130/10000  training loss: 0.55018550157547 Validation loss 0.5496405959129333\n",
      "Epoch 4140/10000  training loss: 0.5501399636268616 Validation loss 0.5496176481246948\n",
      "Epoch 4150/10000  training loss: 0.5500951409339905 Validation loss 0.5496047735214233\n",
      "Epoch 4160/10000  training loss: 0.5500510931015015 Validation loss 0.5495849251747131\n",
      "Epoch 4170/10000  training loss: 0.5500075817108154 Validation loss 0.549564003944397\n",
      "Epoch 4180/10000  training loss: 0.5499646067619324 Validation loss 0.5495409965515137\n",
      "Epoch 4190/10000  training loss: 0.549922525882721 Validation loss 0.5495113134384155\n",
      "Epoch 4200/10000  training loss: 0.5498810410499573 Validation loss 0.54950350522995\n",
      "Epoch 4210/10000  training loss: 0.5498402118682861 Validation loss 0.5495039224624634\n",
      "Epoch 4220/10000  training loss: 0.549799919128418 Validation loss 0.5494945049285889\n",
      "Epoch 4230/10000  training loss: 0.5497600436210632 Validation loss 0.5494845509529114\n",
      "Epoch 4240/10000  training loss: 0.5497206449508667 Validation loss 0.549472451210022\n",
      "Epoch 4250/10000  training loss: 0.5496818423271179 Validation loss 0.5494534969329834\n",
      "Epoch 4260/10000  training loss: 0.5496433973312378 Validation loss 0.5494383573532104\n",
      "Epoch 4270/10000  training loss: 0.5496052503585815 Validation loss 0.5494276881217957\n",
      "Epoch 4280/10000  training loss: 0.5495673418045044 Validation loss 0.5494148135185242\n",
      "Epoch 4290/10000  training loss: 0.5495297908782959 Validation loss 0.5493948459625244\n",
      "Epoch 4300/10000  training loss: 0.5494927763938904 Validation loss 0.5493695139884949\n",
      "Epoch 4310/10000  training loss: 0.5494569540023804 Validation loss 0.549349308013916\n",
      "Epoch 4320/10000  training loss: 0.549421489238739 Validation loss 0.5493340492248535\n",
      "Epoch 4330/10000  training loss: 0.5493862628936768 Validation loss 0.5493184328079224\n",
      "Epoch 4340/10000  training loss: 0.549351155757904 Validation loss 0.5493001937866211\n",
      "Epoch 4350/10000  training loss: 0.5493162870407104 Validation loss 0.5492808222770691\n",
      "Epoch 4360/10000  training loss: 0.5492813587188721 Validation loss 0.5492600202560425\n",
      "Epoch 4370/10000  training loss: 0.549246609210968 Validation loss 0.5492376089096069\n",
      "Epoch 4380/10000  training loss: 0.5492119789123535 Validation loss 0.5492140650749207\n",
      "Epoch 4390/10000  training loss: 0.5491774678230286 Validation loss 0.5491896867752075\n",
      "Epoch 4400/10000  training loss: 0.5491430163383484 Validation loss 0.5491642355918884\n",
      "Epoch 4410/10000  training loss: 0.5491085052490234 Validation loss 0.549139142036438\n",
      "Epoch 4420/10000  training loss: 0.5490742325782776 Validation loss 0.549119770526886\n",
      "Epoch 4430/10000  training loss: 0.549039900302887 Validation loss 0.5491009950637817\n",
      "Epoch 4440/10000  training loss: 0.5490057468414307 Validation loss 0.5490798950195312\n",
      "Epoch 4450/10000  training loss: 0.5489716529846191 Validation loss 0.5490580201148987\n",
      "Epoch 4460/10000  training loss: 0.5489376187324524 Validation loss 0.5490338206291199\n",
      "Epoch 4470/10000  training loss: 0.5489036440849304 Validation loss 0.5490149259567261\n",
      "Epoch 4480/10000  training loss: 0.5488697290420532 Validation loss 0.5489957332611084\n",
      "Epoch 4490/10000  training loss: 0.548835813999176 Validation loss 0.5489746332168579\n",
      "Epoch 4500/10000  training loss: 0.5488019585609436 Validation loss 0.5489538908004761\n",
      "Epoch 4510/10000  training loss: 0.5487682819366455 Validation loss 0.5489328503608704\n",
      "Epoch 4520/10000  training loss: 0.5487344861030579 Validation loss 0.5489135980606079\n",
      "Epoch 4530/10000  training loss: 0.5487008094787598 Validation loss 0.5488936305046082\n",
      "Epoch 4540/10000  training loss: 0.5486671328544617 Validation loss 0.5488739013671875\n",
      "Epoch 4550/10000  training loss: 0.5486334562301636 Validation loss 0.5488544702529907\n",
      "Epoch 4560/10000  training loss: 0.5485997796058655 Validation loss 0.5488351583480835\n",
      "Epoch 4570/10000  training loss: 0.5485661625862122 Validation loss 0.5488161444664001\n",
      "Epoch 4580/10000  training loss: 0.5485324263572693 Validation loss 0.5487975478172302\n",
      "Epoch 4590/10000  training loss: 0.5484987497329712 Validation loss 0.5487788319587708\n",
      "Epoch 4600/10000  training loss: 0.5484651327133179 Validation loss 0.548760712146759\n",
      "Epoch 4610/10000  training loss: 0.5484313368797302 Validation loss 0.5487420558929443\n",
      "Epoch 4620/10000  training loss: 0.5483976602554321 Validation loss 0.5487164258956909\n",
      "Epoch 4630/10000  training loss: 0.5483642816543579 Validation loss 0.5486911535263062\n",
      "Epoch 4640/10000  training loss: 0.5483313202857971 Validation loss 0.5487027168273926\n",
      "Epoch 4650/10000  training loss: 0.5482987761497498 Validation loss 0.5486816763877869\n",
      "Epoch 4660/10000  training loss: 0.5482662916183472 Validation loss 0.5486536622047424\n",
      "Epoch 4670/10000  training loss: 0.5482339262962341 Validation loss 0.5486499667167664\n",
      "Epoch 4680/10000  training loss: 0.5482020378112793 Validation loss 0.5486518144607544\n",
      "Epoch 4690/10000  training loss: 0.5481704473495483 Validation loss 0.5486451983451843\n",
      "Epoch 4700/10000  training loss: 0.5481389164924622 Validation loss 0.5486333966255188\n",
      "Epoch 4710/10000  training loss: 0.5481075644493103 Validation loss 0.548615038394928\n",
      "Epoch 4720/10000  training loss: 0.5480761528015137 Validation loss 0.5486012697219849\n",
      "Epoch 4730/10000  training loss: 0.5480448603630066 Validation loss 0.5485880374908447\n",
      "Epoch 4740/10000  training loss: 0.5480135083198547 Validation loss 0.5485740900039673\n",
      "Epoch 4750/10000  training loss: 0.5479828715324402 Validation loss 0.5485536456108093\n",
      "Epoch 4760/10000  training loss: 0.5479529500007629 Validation loss 0.5485531091690063\n",
      "Epoch 4770/10000  training loss: 0.5479232668876648 Validation loss 0.5485581159591675\n",
      "Epoch 4780/10000  training loss: 0.5478938221931458 Validation loss 0.548563539981842\n",
      "Epoch 4790/10000  training loss: 0.547864556312561 Validation loss 0.5485613346099854\n",
      "Epoch 4800/10000  training loss: 0.5478352904319763 Validation loss 0.5485503673553467\n",
      "Epoch 4810/10000  training loss: 0.5478062033653259 Validation loss 0.5485379099845886\n",
      "Epoch 4820/10000  training loss: 0.5477771162986755 Validation loss 0.5485249757766724\n",
      "Epoch 4830/10000  training loss: 0.5477480292320251 Validation loss 0.5485067963600159\n",
      "Epoch 4840/10000  training loss: 0.54771888256073 Validation loss 0.5484882593154907\n",
      "Epoch 4850/10000  training loss: 0.5476898550987244 Validation loss 0.5484668016433716\n",
      "Epoch 4860/10000  training loss: 0.5476608276367188 Validation loss 0.5484534502029419\n",
      "Epoch 4870/10000  training loss: 0.5476289391517639 Validation loss 0.5484980344772339\n",
      "Epoch 4880/10000  training loss: 0.5475954413414001 Validation loss 0.5485357046127319\n",
      "Epoch 4890/10000  training loss: 0.5475618839263916 Validation loss 0.5485575795173645\n",
      "Epoch 4900/10000  training loss: 0.547528862953186 Validation loss 0.548590898513794\n",
      "Epoch 4910/10000  training loss: 0.547496497631073 Validation loss 0.5486041307449341\n",
      "Epoch 4920/10000  training loss: 0.5474645495414734 Validation loss 0.5486088395118713\n",
      "Epoch 4930/10000  training loss: 0.5474329590797424 Validation loss 0.5486101508140564\n",
      "Epoch 4940/10000  training loss: 0.5474015474319458 Validation loss 0.5486016273498535\n",
      "Epoch 4950/10000  training loss: 0.5473703145980835 Validation loss 0.5485907793045044\n",
      "Epoch 4960/10000  training loss: 0.5473392009735107 Validation loss 0.5485771894454956\n",
      "Epoch 4970/10000  training loss: 0.5473080277442932 Validation loss 0.5485607981681824\n",
      "Epoch 4980/10000  training loss: 0.5472771525382996 Validation loss 0.5485438108444214\n",
      "Epoch 4990/10000  training loss: 0.5472461581230164 Validation loss 0.5485255122184753\n",
      "Epoch 5000/10000  training loss: 0.5472151637077332 Validation loss 0.5485066175460815\n",
      "Epoch 5010/10000  training loss: 0.5471842885017395 Validation loss 0.5484878420829773\n",
      "Epoch 5020/10000  training loss: 0.5471534132957458 Validation loss 0.5484693646430969\n",
      "Epoch 5030/10000  training loss: 0.5471224784851074 Validation loss 0.5484505891799927\n",
      "Epoch 5040/10000  training loss: 0.5470916628837585 Validation loss 0.5484329462051392\n",
      "Epoch 5050/10000  training loss: 0.5470608472824097 Validation loss 0.5484146475791931\n",
      "Epoch 5060/10000  training loss: 0.547029972076416 Validation loss 0.5483967065811157\n",
      "Epoch 5070/10000  training loss: 0.5469990968704224 Validation loss 0.5483806133270264\n",
      "Epoch 5080/10000  training loss: 0.5469682216644287 Validation loss 0.5483638644218445\n",
      "Epoch 5090/10000  training loss: 0.5469373464584351 Validation loss 0.5483473539352417\n",
      "Epoch 5100/10000  training loss: 0.5469064712524414 Validation loss 0.5483314990997314\n",
      "Epoch 5110/10000  training loss: 0.546876072883606 Validation loss 0.5483348965644836\n",
      "Epoch 5120/10000  training loss: 0.5468459725379944 Validation loss 0.5483593940734863\n",
      "Epoch 5130/10000  training loss: 0.5468159914016724 Validation loss 0.5483560562133789\n",
      "Epoch 5140/10000  training loss: 0.546786367893219 Validation loss 0.5483707189559937\n",
      "Epoch 5150/10000  training loss: 0.5467569231987 Validation loss 0.5483800768852234\n",
      "Epoch 5160/10000  training loss: 0.5467275381088257 Validation loss 0.5483881831169128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5170/10000  training loss: 0.5466981530189514 Validation loss 0.5483942627906799\n",
      "Epoch 5180/10000  training loss: 0.5466688275337219 Validation loss 0.5483989715576172\n",
      "Epoch 5190/10000  training loss: 0.5466393828392029 Validation loss 0.548401951789856\n",
      "Epoch 5200/10000  training loss: 0.5466099381446838 Validation loss 0.5484023094177246\n",
      "Epoch 5210/10000  training loss: 0.5465804934501648 Validation loss 0.5484012365341187\n",
      "Epoch 5220/10000  training loss: 0.5465509295463562 Validation loss 0.5483973622322083\n",
      "Epoch 5230/10000  training loss: 0.5465213060379028 Validation loss 0.5483939051628113\n",
      "Epoch 5240/10000  training loss: 0.5464916229248047 Validation loss 0.5483915209770203\n",
      "Epoch 5250/10000  training loss: 0.546461820602417 Validation loss 0.5483881235122681\n",
      "Epoch 5260/10000  training loss: 0.5464320778846741 Validation loss 0.5483843684196472\n",
      "Epoch 5270/10000  training loss: 0.546402096748352 Validation loss 0.548378586769104\n",
      "Epoch 5280/10000  training loss: 0.54637211561203 Validation loss 0.5483720898628235\n",
      "Epoch 5290/10000  training loss: 0.5463419556617737 Validation loss 0.5483654737472534\n",
      "Epoch 5300/10000  training loss: 0.5463116765022278 Validation loss 0.5483590960502625\n",
      "Epoch 5310/10000  training loss: 0.5462813377380371 Validation loss 0.5483545064926147\n",
      "Epoch 5320/10000  training loss: 0.5462508797645569 Validation loss 0.5483509302139282\n",
      "Epoch 5330/10000  training loss: 0.5462205410003662 Validation loss 0.5483531951904297\n",
      "Epoch 5340/10000  training loss: 0.5461901426315308 Validation loss 0.5483536124229431\n",
      "Epoch 5350/10000  training loss: 0.5461596250534058 Validation loss 0.5483556389808655\n",
      "Epoch 5360/10000  training loss: 0.5461289286613464 Validation loss 0.5483565926551819\n",
      "Epoch 5370/10000  training loss: 0.5460981726646423 Validation loss 0.5483556389808655\n",
      "Epoch 5380/10000  training loss: 0.5460672974586487 Validation loss 0.5483559370040894\n",
      "Epoch 5390/10000  training loss: 0.5460362434387207 Validation loss 0.5483554005622864\n",
      "Epoch 5400/10000  training loss: 0.5460050702095032 Validation loss 0.5483552813529968\n",
      "Epoch 5410/10000  training loss: 0.5459737777709961 Validation loss 0.5483547449111938\n",
      "Epoch 5420/10000  training loss: 0.5459423661231995 Validation loss 0.5483545064926147\n",
      "Epoch 5430/10000  training loss: 0.5459107160568237 Validation loss 0.5483547449111938\n",
      "Epoch 5440/10000  training loss: 0.5458788871765137 Validation loss 0.548355758190155\n",
      "Epoch 5450/10000  training loss: 0.5458468794822693 Validation loss 0.548357367515564\n",
      "Epoch 5460/10000  training loss: 0.5458147525787354 Validation loss 0.5483596920967102\n",
      "Epoch 5470/10000  training loss: 0.5457823872566223 Validation loss 0.5483626127243042\n",
      "Epoch 5480/10000  training loss: 0.5457499027252197 Validation loss 0.5483659505844116\n",
      "Epoch 5490/10000  training loss: 0.5457172393798828 Validation loss 0.5483697056770325\n",
      "Epoch 5500/10000  training loss: 0.5456843376159668 Validation loss 0.5483736395835876\n",
      "Epoch 5510/10000  training loss: 0.5456512570381165 Validation loss 0.5483778715133667\n",
      "Epoch 5520/10000  training loss: 0.5456179976463318 Validation loss 0.5483792424201965\n",
      "Epoch 5530/10000  training loss: 0.5455845594406128 Validation loss 0.5483804941177368\n",
      "Epoch 5540/10000  training loss: 0.5455510020256042 Validation loss 0.5483895540237427\n",
      "Epoch 5550/10000  training loss: 0.5455173850059509 Validation loss 0.5484080910682678\n",
      "Epoch 5560/10000  training loss: 0.5454836487770081 Validation loss 0.5484277606010437\n",
      "Epoch 5570/10000  training loss: 0.5454497933387756 Validation loss 0.5484479069709778\n",
      "Epoch 5580/10000  training loss: 0.5454159379005432 Validation loss 0.5484630465507507\n",
      "Epoch 5590/10000  training loss: 0.5453819036483765 Validation loss 0.5484782457351685\n",
      "Epoch 5600/10000  training loss: 0.5453477501869202 Validation loss 0.5484917163848877\n",
      "Epoch 5610/10000  training loss: 0.5453133583068848 Validation loss 0.5485039949417114\n",
      "Epoch 5620/10000  training loss: 0.5452790260314941 Validation loss 0.5485140085220337\n",
      "Epoch 5630/10000  training loss: 0.5452445149421692 Validation loss 0.5485262870788574\n",
      "Epoch 5640/10000  training loss: 0.5452099442481995 Validation loss 0.5485330820083618\n",
      "Epoch 5650/10000  training loss: 0.5451752543449402 Validation loss 0.5485330820083618\n",
      "Epoch 5660/10000  training loss: 0.5451409816741943 Validation loss 0.5485206842422485\n",
      "Epoch 5670/10000  training loss: 0.5451065897941589 Validation loss 0.5485047101974487\n",
      "Epoch 5680/10000  training loss: 0.545072078704834 Validation loss 0.548488974571228\n",
      "Epoch 5690/10000  training loss: 0.5450376272201538 Validation loss 0.5484753847122192\n",
      "Epoch 5700/10000  training loss: 0.5450029969215393 Validation loss 0.5484665632247925\n",
      "Epoch 5710/10000  training loss: 0.5449682474136353 Validation loss 0.5484628081321716\n",
      "Epoch 5720/10000  training loss: 0.5449334383010864 Validation loss 0.5484613180160522\n",
      "Epoch 5730/10000  training loss: 0.544898509979248 Validation loss 0.5484596490859985\n",
      "Epoch 5740/10000  training loss: 0.5448635816574097 Validation loss 0.5484566688537598\n",
      "Epoch 5750/10000  training loss: 0.5448284149169922 Validation loss 0.5484520196914673\n",
      "Epoch 5760/10000  training loss: 0.5447932481765747 Validation loss 0.5484458208084106\n",
      "Epoch 5770/10000  training loss: 0.5447579622268677 Validation loss 0.5484388470649719\n",
      "Epoch 5780/10000  training loss: 0.5447225570678711 Validation loss 0.5484312772750854\n",
      "Epoch 5790/10000  training loss: 0.5446869730949402 Validation loss 0.5484197735786438\n",
      "Epoch 5800/10000  training loss: 0.5446515083312988 Validation loss 0.5484007000923157\n",
      "Epoch 5810/10000  training loss: 0.5446159243583679 Validation loss 0.5483813285827637\n",
      "Epoch 5820/10000  training loss: 0.5445740818977356 Validation loss 0.5482954978942871\n",
      "Epoch 5830/10000  training loss: 0.5445305109024048 Validation loss 0.5481941103935242\n",
      "Epoch 5840/10000  training loss: 0.5444888472557068 Validation loss 0.5481158494949341\n",
      "Epoch 5850/10000  training loss: 0.5444480180740356 Validation loss 0.5480818748474121\n",
      "Epoch 5860/10000  training loss: 0.5444073677062988 Validation loss 0.5480693578720093\n",
      "Epoch 5870/10000  training loss: 0.544366717338562 Validation loss 0.548067569732666\n",
      "Epoch 5880/10000  training loss: 0.5443260073661804 Validation loss 0.548067033290863\n",
      "Epoch 5890/10000  training loss: 0.5442855358123779 Validation loss 0.5480644106864929\n",
      "Epoch 5900/10000  training loss: 0.5442450642585754 Validation loss 0.5480586886405945\n",
      "Epoch 5910/10000  training loss: 0.544204592704773 Validation loss 0.5480504035949707\n",
      "Epoch 5920/10000  training loss: 0.5441641211509705 Validation loss 0.548040509223938\n",
      "Epoch 5930/10000  training loss: 0.5441237092018127 Validation loss 0.548029899597168\n",
      "Epoch 5940/10000  training loss: 0.5440833568572998 Validation loss 0.5480190515518188\n",
      "Epoch 5950/10000  training loss: 0.5440429449081421 Validation loss 0.5480079054832458\n",
      "Epoch 5960/10000  training loss: 0.5440026521682739 Validation loss 0.5479965209960938\n",
      "Epoch 5970/10000  training loss: 0.5439624190330505 Validation loss 0.5479850769042969\n",
      "Epoch 5980/10000  training loss: 0.5439221858978271 Validation loss 0.5479733943939209\n",
      "Epoch 5990/10000  training loss: 0.5438819527626038 Validation loss 0.5479605793952942\n",
      "Epoch 6000/10000  training loss: 0.5438418388366699 Validation loss 0.5479442477226257\n",
      "Epoch 6010/10000  training loss: 0.5438017249107361 Validation loss 0.5479279160499573\n",
      "Epoch 6020/10000  training loss: 0.543761670589447 Validation loss 0.5479130148887634\n",
      "Epoch 6030/10000  training loss: 0.5437216758728027 Validation loss 0.5478993654251099\n",
      "Epoch 6040/10000  training loss: 0.5436817407608032 Validation loss 0.5478866696357727\n",
      "Epoch 6050/10000  training loss: 0.5436419248580933 Validation loss 0.5478744506835938\n",
      "Epoch 6060/10000  training loss: 0.5436021685600281 Validation loss 0.5478622317314148\n",
      "Epoch 6070/10000  training loss: 0.5435624718666077 Validation loss 0.5478496551513672\n",
      "Epoch 6080/10000  training loss: 0.5435227751731873 Validation loss 0.547829270362854\n",
      "Epoch 6090/10000  training loss: 0.5434834361076355 Validation loss 0.547808051109314\n",
      "Epoch 6100/10000  training loss: 0.5434445738792419 Validation loss 0.5477994680404663\n",
      "Epoch 6110/10000  training loss: 0.5434058308601379 Validation loss 0.5477951765060425\n",
      "Epoch 6120/10000  training loss: 0.5433672070503235 Validation loss 0.5477874875068665\n",
      "Epoch 6130/10000  training loss: 0.5433288216590881 Validation loss 0.5477762222290039\n",
      "Epoch 6140/10000  training loss: 0.5432904362678528 Validation loss 0.5477638244628906\n",
      "Epoch 6150/10000  training loss: 0.5432522296905518 Validation loss 0.5477536916732788\n",
      "Epoch 6160/10000  training loss: 0.5432141423225403 Validation loss 0.5477423667907715\n",
      "Epoch 6170/10000  training loss: 0.5431760549545288 Validation loss 0.5477299690246582\n",
      "Epoch 6180/10000  training loss: 0.5431380271911621 Validation loss 0.5477169752120972\n",
      "Epoch 6190/10000  training loss: 0.543100118637085 Validation loss 0.5477030873298645\n",
      "Epoch 6200/10000  training loss: 0.5430622100830078 Validation loss 0.5476886034011841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6210/10000  training loss: 0.5430244207382202 Validation loss 0.5476739406585693\n",
      "Epoch 6220/10000  training loss: 0.5429865717887878 Validation loss 0.547659158706665\n",
      "Epoch 6230/10000  training loss: 0.542948842048645 Validation loss 0.547644317150116\n",
      "Epoch 6240/10000  training loss: 0.542911171913147 Validation loss 0.5476295351982117\n",
      "Epoch 6250/10000  training loss: 0.5428736209869385 Validation loss 0.5476148128509521\n",
      "Epoch 6260/10000  training loss: 0.5428360104560852 Validation loss 0.5475999712944031\n",
      "Epoch 6270/10000  training loss: 0.5427985787391663 Validation loss 0.547585129737854\n",
      "Epoch 6280/10000  training loss: 0.5427610874176025 Validation loss 0.5475702285766602\n",
      "Epoch 6290/10000  training loss: 0.5427235960960388 Validation loss 0.5475552678108215\n",
      "Epoch 6300/10000  training loss: 0.5426862835884094 Validation loss 0.5475402474403381\n",
      "Epoch 6310/10000  training loss: 0.5426489114761353 Validation loss 0.54752516746521\n",
      "Epoch 6320/10000  training loss: 0.5426117181777954 Validation loss 0.547510027885437\n",
      "Epoch 6330/10000  training loss: 0.5425744652748108 Validation loss 0.5474957227706909\n",
      "Epoch 6340/10000  training loss: 0.5425373911857605 Validation loss 0.5474839210510254\n",
      "Epoch 6350/10000  training loss: 0.5425003170967102 Validation loss 0.5474724173545837\n",
      "Epoch 6360/10000  training loss: 0.5424633622169495 Validation loss 0.5474547147750854\n",
      "Epoch 6370/10000  training loss: 0.5424264669418335 Validation loss 0.5474259257316589\n",
      "Epoch 6380/10000  training loss: 0.5423898100852966 Validation loss 0.5474022626876831\n",
      "Epoch 6390/10000  training loss: 0.5423531532287598 Validation loss 0.5473824739456177\n",
      "Epoch 6400/10000  training loss: 0.5423165559768677 Validation loss 0.547362744808197\n",
      "Epoch 6410/10000  training loss: 0.5422800183296204 Validation loss 0.5473424196243286\n",
      "Epoch 6420/10000  training loss: 0.5422435998916626 Validation loss 0.5473240613937378\n",
      "Epoch 6430/10000  training loss: 0.5422072410583496 Validation loss 0.5473067164421082\n",
      "Epoch 6440/10000  training loss: 0.54217129945755 Validation loss 0.5472973585128784\n",
      "Epoch 6450/10000  training loss: 0.5421355962753296 Validation loss 0.5472992062568665\n",
      "Epoch 6460/10000  training loss: 0.5421000123023987 Validation loss 0.5472911596298218\n",
      "Epoch 6470/10000  training loss: 0.5420646667480469 Validation loss 0.547279417514801\n",
      "Epoch 6480/10000  training loss: 0.5420293211936951 Validation loss 0.5472694039344788\n",
      "Epoch 6490/10000  training loss: 0.5419940948486328 Validation loss 0.5472601652145386\n",
      "Epoch 6500/10000  training loss: 0.5419588088989258 Validation loss 0.5472502708435059\n",
      "Epoch 6510/10000  training loss: 0.5419239401817322 Validation loss 0.5472396612167358\n",
      "Epoch 6520/10000  training loss: 0.5418888926506042 Validation loss 0.5472275018692017\n",
      "Epoch 6530/10000  training loss: 0.5418539643287659 Validation loss 0.5472143888473511\n",
      "Epoch 6540/10000  training loss: 0.5418192148208618 Validation loss 0.547204852104187\n",
      "Epoch 6550/10000  training loss: 0.5417842864990234 Validation loss 0.5471915006637573\n",
      "Epoch 6560/10000  training loss: 0.5417495965957642 Validation loss 0.5471807718276978\n",
      "Epoch 6570/10000  training loss: 0.5417150259017944 Validation loss 0.5471664667129517\n",
      "Epoch 6580/10000  training loss: 0.5416803956031799 Validation loss 0.5471537709236145\n",
      "Epoch 6590/10000  training loss: 0.5416460037231445 Validation loss 0.5471398830413818\n",
      "Epoch 6600/10000  training loss: 0.5416114926338196 Validation loss 0.5471211671829224\n",
      "Epoch 6610/10000  training loss: 0.5415772199630737 Validation loss 0.5471112132072449\n",
      "Epoch 6620/10000  training loss: 0.5415427088737488 Validation loss 0.5470986366271973\n",
      "Epoch 6630/10000  training loss: 0.5415085554122925 Validation loss 0.547081470489502\n",
      "Epoch 6640/10000  training loss: 0.5414743423461914 Validation loss 0.5470659136772156\n",
      "Epoch 6650/10000  training loss: 0.5414401292800903 Validation loss 0.5470506548881531\n",
      "Epoch 6660/10000  training loss: 0.5414060950279236 Validation loss 0.5470350980758667\n",
      "Epoch 6670/10000  training loss: 0.5413718819618225 Validation loss 0.5470185875892639\n",
      "Epoch 6680/10000  training loss: 0.5413378477096558 Validation loss 0.5470061302185059\n",
      "Epoch 6690/10000  training loss: 0.5413038730621338 Validation loss 0.5469877123832703\n",
      "Epoch 6700/10000  training loss: 0.541269838809967 Validation loss 0.5469666719436646\n",
      "Epoch 6710/10000  training loss: 0.5412358045578003 Validation loss 0.5469551086425781\n",
      "Epoch 6720/10000  training loss: 0.5412019491195679 Validation loss 0.5469487309455872\n",
      "Epoch 6730/10000  training loss: 0.5411680340766907 Validation loss 0.5469273924827576\n",
      "Epoch 6740/10000  training loss: 0.5411342978477478 Validation loss 0.5469110608100891\n",
      "Epoch 6750/10000  training loss: 0.5411005616188049 Validation loss 0.5468971729278564\n",
      "Epoch 6760/10000  training loss: 0.5410670042037964 Validation loss 0.5468868017196655\n",
      "Epoch 6770/10000  training loss: 0.5410336256027222 Validation loss 0.5468744039535522\n",
      "Epoch 6780/10000  training loss: 0.5410001873970032 Validation loss 0.5468633770942688\n",
      "Epoch 6790/10000  training loss: 0.5409669280052185 Validation loss 0.5468491315841675\n",
      "Epoch 6800/10000  training loss: 0.5409336686134338 Validation loss 0.546833872795105\n",
      "Epoch 6810/10000  training loss: 0.5409004092216492 Validation loss 0.546818196773529\n",
      "Epoch 6820/10000  training loss: 0.5408673286437988 Validation loss 0.5468016266822815\n",
      "Epoch 6830/10000  training loss: 0.5408343076705933 Validation loss 0.5467890501022339\n",
      "Epoch 6840/10000  training loss: 0.5408013463020325 Validation loss 0.5467778444290161\n",
      "Epoch 6850/10000  training loss: 0.5407684445381165 Validation loss 0.5467683672904968\n",
      "Epoch 6860/10000  training loss: 0.5407355427742004 Validation loss 0.546739399433136\n",
      "Epoch 6870/10000  training loss: 0.540702760219574 Validation loss 0.5467307567596436\n",
      "Epoch 6880/10000  training loss: 0.5406700968742371 Validation loss 0.5467174649238586\n",
      "Epoch 6890/10000  training loss: 0.5406374335289001 Validation loss 0.5467039346694946\n",
      "Epoch 6900/10000  training loss: 0.5406047701835632 Validation loss 0.5466869473457336\n",
      "Epoch 6910/10000  training loss: 0.5405724048614502 Validation loss 0.5466732382774353\n",
      "Epoch 6920/10000  training loss: 0.5405399203300476 Validation loss 0.5466574430465698\n",
      "Epoch 6930/10000  training loss: 0.5405076146125793 Validation loss 0.5466405749320984\n",
      "Epoch 6940/10000  training loss: 0.5404753088951111 Validation loss 0.5466240048408508\n",
      "Epoch 6950/10000  training loss: 0.5404430627822876 Validation loss 0.5466073155403137\n",
      "Epoch 6960/10000  training loss: 0.5404109954833984 Validation loss 0.546591579914093\n",
      "Epoch 6970/10000  training loss: 0.5403790473937988 Validation loss 0.5465760827064514\n",
      "Epoch 6980/10000  training loss: 0.5403470993041992 Validation loss 0.5465599298477173\n",
      "Epoch 6990/10000  training loss: 0.5403152704238892 Validation loss 0.546543300151825\n",
      "Epoch 7000/10000  training loss: 0.5402834415435791 Validation loss 0.5465268492698669\n",
      "Epoch 7010/10000  training loss: 0.5402517318725586 Validation loss 0.5465105772018433\n",
      "Epoch 7020/10000  training loss: 0.5402201414108276 Validation loss 0.5464950799942017\n",
      "Epoch 7030/10000  training loss: 0.5401886105537415 Validation loss 0.5464797019958496\n",
      "Epoch 7040/10000  training loss: 0.5401571989059448 Validation loss 0.5464637875556946\n",
      "Epoch 7050/10000  training loss: 0.5401257872581482 Validation loss 0.546448826789856\n",
      "Epoch 7060/10000  training loss: 0.5400946140289307 Validation loss 0.5464351177215576\n",
      "Epoch 7070/10000  training loss: 0.5400634407997131 Validation loss 0.5464202165603638\n",
      "Epoch 7080/10000  training loss: 0.5400325059890747 Validation loss 0.546440601348877\n",
      "Epoch 7090/10000  training loss: 0.5400018692016602 Validation loss 0.5464521646499634\n",
      "Epoch 7100/10000  training loss: 0.5399712920188904 Validation loss 0.5464627742767334\n",
      "Epoch 7110/10000  training loss: 0.5399408340454102 Validation loss 0.5464701056480408\n",
      "Epoch 7120/10000  training loss: 0.5399106740951538 Validation loss 0.546474039554596\n",
      "Epoch 7130/10000  training loss: 0.5398805737495422 Validation loss 0.5464755296707153\n",
      "Epoch 7140/10000  training loss: 0.5398505926132202 Validation loss 0.5464805364608765\n",
      "Epoch 7150/10000  training loss: 0.539820671081543 Validation loss 0.5464784502983093\n",
      "Epoch 7160/10000  training loss: 0.5397909879684448 Validation loss 0.5464771389961243\n",
      "Epoch 7170/10000  training loss: 0.5397613048553467 Validation loss 0.5464740991592407\n",
      "Epoch 7180/10000  training loss: 0.5397318005561829 Validation loss 0.5464717149734497\n",
      "Epoch 7190/10000  training loss: 0.5397024154663086 Validation loss 0.5464677214622498\n",
      "Epoch 7200/10000  training loss: 0.5396730303764343 Validation loss 0.546457827091217\n",
      "Epoch 7210/10000  training loss: 0.5396438837051392 Validation loss 0.5464544892311096\n",
      "Epoch 7220/10000  training loss: 0.539614737033844 Validation loss 0.5464574098587036\n",
      "Epoch 7230/10000  training loss: 0.5395857691764832 Validation loss 0.5464568138122559\n",
      "Epoch 7240/10000  training loss: 0.5395569801330566 Validation loss 0.5464522838592529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7250/10000  training loss: 0.5395282506942749 Validation loss 0.5464370846748352\n",
      "Epoch 7260/10000  training loss: 0.5394997000694275 Validation loss 0.5464171767234802\n",
      "Epoch 7270/10000  training loss: 0.5394713282585144 Validation loss 0.5463987588882446\n",
      "Epoch 7280/10000  training loss: 0.5394431352615356 Validation loss 0.5463831424713135\n",
      "Epoch 7290/10000  training loss: 0.5394150614738464 Validation loss 0.5463746190071106\n",
      "Epoch 7300/10000  training loss: 0.5393869876861572 Validation loss 0.5463594198226929\n",
      "Epoch 7310/10000  training loss: 0.5393591523170471 Validation loss 0.5463436841964722\n",
      "Epoch 7320/10000  training loss: 0.5393314361572266 Validation loss 0.5463281869888306\n",
      "Epoch 7330/10000  training loss: 0.539303183555603 Validation loss 0.5463050603866577\n",
      "Epoch 7340/10000  training loss: 0.5392748117446899 Validation loss 0.5462913513183594\n",
      "Epoch 7350/10000  training loss: 0.5392465591430664 Validation loss 0.5462794899940491\n",
      "Epoch 7360/10000  training loss: 0.5392184853553772 Validation loss 0.5462692379951477\n",
      "Epoch 7370/10000  training loss: 0.539190948009491 Validation loss 0.546257495880127\n",
      "Epoch 7380/10000  training loss: 0.5391634702682495 Validation loss 0.5462482571601868\n",
      "Epoch 7390/10000  training loss: 0.5391362905502319 Validation loss 0.5462391972541809\n",
      "Epoch 7400/10000  training loss: 0.5391092896461487 Validation loss 0.5462268590927124\n",
      "Epoch 7410/10000  training loss: 0.5390824675559998 Validation loss 0.5462212562561035\n",
      "Epoch 7420/10000  training loss: 0.5390558242797852 Validation loss 0.5462039709091187\n",
      "Epoch 7430/10000  training loss: 0.5390291810035706 Validation loss 0.5462111830711365\n",
      "Epoch 7440/10000  training loss: 0.5390028953552246 Validation loss 0.5461832284927368\n",
      "Epoch 7450/10000  training loss: 0.5389763712882996 Validation loss 0.5461739301681519\n",
      "Epoch 7460/10000  training loss: 0.5389500856399536 Validation loss 0.5461641550064087\n",
      "Epoch 7470/10000  training loss: 0.5389242768287659 Validation loss 0.5461570024490356\n",
      "Epoch 7480/10000  training loss: 0.5388984680175781 Validation loss 0.546151340007782\n",
      "Epoch 7490/10000  training loss: 0.5388728380203247 Validation loss 0.5461446642875671\n",
      "Epoch 7500/10000  training loss: 0.5388474464416504 Validation loss 0.5461391806602478\n",
      "Epoch 7510/10000  training loss: 0.5388220548629761 Validation loss 0.5461214780807495\n",
      "Epoch 7520/10000  training loss: 0.5387967824935913 Validation loss 0.5461083650588989\n",
      "Epoch 7530/10000  training loss: 0.5387715697288513 Validation loss 0.5460949540138245\n",
      "Epoch 7540/10000  training loss: 0.5387465357780457 Validation loss 0.5460792183876038\n",
      "Epoch 7550/10000  training loss: 0.5387216806411743 Validation loss 0.5460621118545532\n",
      "Epoch 7560/10000  training loss: 0.5386969447135925 Validation loss 0.5460489392280579\n",
      "Epoch 7570/10000  training loss: 0.538672149181366 Validation loss 0.5460337996482849\n",
      "Epoch 7580/10000  training loss: 0.5386473536491394 Validation loss 0.5460165143013\n",
      "Epoch 7590/10000  training loss: 0.5386228561401367 Validation loss 0.5459979176521301\n",
      "Epoch 7600/10000  training loss: 0.5385985374450684 Validation loss 0.5459871292114258\n",
      "Epoch 7610/10000  training loss: 0.5385743975639343 Validation loss 0.5459746718406677\n",
      "Epoch 7620/10000  training loss: 0.5385503768920898 Validation loss 0.5459583401679993\n",
      "Epoch 7630/10000  training loss: 0.5385264158248901 Validation loss 0.5459437966346741\n",
      "Epoch 7640/10000  training loss: 0.5385027527809143 Validation loss 0.5459426641464233\n",
      "Epoch 7650/10000  training loss: 0.5384791493415833 Validation loss 0.5459319949150085\n",
      "Epoch 7660/10000  training loss: 0.5384556651115417 Validation loss 0.5459240078926086\n",
      "Epoch 7670/10000  training loss: 0.5384321808815002 Validation loss 0.5459246039390564\n",
      "Epoch 7680/10000  training loss: 0.5384090542793274 Validation loss 0.5458951592445374\n",
      "Epoch 7690/10000  training loss: 0.5383861064910889 Validation loss 0.5458753705024719\n",
      "Epoch 7700/10000  training loss: 0.5383632183074951 Validation loss 0.5458545684814453\n",
      "Epoch 7710/10000  training loss: 0.5383404493331909 Validation loss 0.5458430051803589\n",
      "Epoch 7720/10000  training loss: 0.5383177399635315 Validation loss 0.5458129644393921\n",
      "Epoch 7730/10000  training loss: 0.5382950305938721 Validation loss 0.5457894206047058\n",
      "Epoch 7740/10000  training loss: 0.5382726192474365 Validation loss 0.5457585453987122\n",
      "Epoch 7750/10000  training loss: 0.5382500886917114 Validation loss 0.5457285046577454\n",
      "Epoch 7760/10000  training loss: 0.538227915763855 Validation loss 0.5457168221473694\n",
      "Epoch 7770/10000  training loss: 0.5382055044174194 Validation loss 0.5457015633583069\n",
      "Epoch 7780/10000  training loss: 0.5381833910942078 Validation loss 0.5456717610359192\n",
      "Epoch 7790/10000  training loss: 0.5381612777709961 Validation loss 0.5456358790397644\n",
      "Epoch 7800/10000  training loss: 0.5381391048431396 Validation loss 0.5456205606460571\n",
      "Epoch 7810/10000  training loss: 0.5381172895431519 Validation loss 0.5455911755561829\n",
      "Epoch 7820/10000  training loss: 0.5380954742431641 Validation loss 0.5455752611160278\n",
      "Epoch 7830/10000  training loss: 0.5380735993385315 Validation loss 0.5455572605133057\n",
      "Epoch 7840/10000  training loss: 0.5380518436431885 Validation loss 0.5455327033996582\n",
      "Epoch 7850/10000  training loss: 0.5380303859710693 Validation loss 0.5454999804496765\n",
      "Epoch 7860/10000  training loss: 0.5380087494850159 Validation loss 0.5454840660095215\n",
      "Epoch 7870/10000  training loss: 0.5379872918128967 Validation loss 0.5454672574996948\n",
      "Epoch 7880/10000  training loss: 0.5379660129547119 Validation loss 0.5454441905021667\n",
      "Epoch 7890/10000  training loss: 0.5379446744918823 Validation loss 0.5454143285751343\n",
      "Epoch 7900/10000  training loss: 0.5379235148429871 Validation loss 0.5454009771347046\n",
      "Epoch 7910/10000  training loss: 0.5379022359848022 Validation loss 0.5453828573226929\n",
      "Epoch 7920/10000  training loss: 0.5378811955451965 Validation loss 0.5453547835350037\n",
      "Epoch 7930/10000  training loss: 0.5378602743148804 Validation loss 0.5453332662582397\n",
      "Epoch 7940/10000  training loss: 0.5378392934799194 Validation loss 0.5453116297721863\n",
      "Epoch 7950/10000  training loss: 0.5378184914588928 Validation loss 0.5452975034713745\n",
      "Epoch 7960/10000  training loss: 0.5377976298332214 Validation loss 0.5452754497528076\n",
      "Epoch 7970/10000  training loss: 0.5377770066261292 Validation loss 0.5452589988708496\n",
      "Epoch 7980/10000  training loss: 0.5377564430236816 Validation loss 0.5452449917793274\n",
      "Epoch 7990/10000  training loss: 0.5377358794212341 Validation loss 0.5452250242233276\n",
      "Epoch 8000/10000  training loss: 0.5377154350280762 Validation loss 0.5452067255973816\n",
      "Epoch 8010/10000  training loss: 0.5376949310302734 Validation loss 0.5451948046684265\n",
      "Epoch 8020/10000  training loss: 0.5376747250556946 Validation loss 0.5451812744140625\n",
      "Epoch 8030/10000  training loss: 0.5376543998718262 Validation loss 0.5451592206954956\n",
      "Epoch 8040/10000  training loss: 0.5376341938972473 Validation loss 0.5451433062553406\n",
      "Epoch 8050/10000  training loss: 0.5376140475273132 Validation loss 0.5451281070709229\n",
      "Epoch 8060/10000  training loss: 0.5375941395759583 Validation loss 0.5451092720031738\n",
      "Epoch 8070/10000  training loss: 0.5375741720199585 Validation loss 0.54509437084198\n",
      "Epoch 8080/10000  training loss: 0.5375544428825378 Validation loss 0.545071005821228\n",
      "Epoch 8090/10000  training loss: 0.5375344157218933 Validation loss 0.5450692176818848\n",
      "Epoch 8100/10000  training loss: 0.5375146269798279 Validation loss 0.5450478792190552\n",
      "Epoch 8110/10000  training loss: 0.5374948382377625 Validation loss 0.5450313687324524\n",
      "Epoch 8120/10000  training loss: 0.5374754071235657 Validation loss 0.5450177788734436\n",
      "Epoch 8130/10000  training loss: 0.5374557971954346 Validation loss 0.5449976325035095\n",
      "Epoch 8140/10000  training loss: 0.537436306476593 Validation loss 0.5449889302253723\n",
      "Epoch 8150/10000  training loss: 0.5374168753623962 Validation loss 0.5449692010879517\n",
      "Epoch 8160/10000  training loss: 0.5373975038528442 Validation loss 0.5449476838111877\n",
      "Epoch 8170/10000  training loss: 0.5373782515525818 Validation loss 0.5449315309524536\n",
      "Epoch 8180/10000  training loss: 0.5373582243919373 Validation loss 0.5449249148368835\n",
      "Epoch 8190/10000  training loss: 0.5373335480690002 Validation loss 0.5447636842727661\n",
      "Epoch 8200/10000  training loss: 0.5373097062110901 Validation loss 0.5447681546211243\n",
      "Epoch 8210/10000  training loss: 0.537286639213562 Validation loss 0.544807493686676\n",
      "Epoch 8220/10000  training loss: 0.5372638702392578 Validation loss 0.5447712540626526\n",
      "Epoch 8230/10000  training loss: 0.5372419357299805 Validation loss 0.5447394251823425\n",
      "Epoch 8240/10000  training loss: 0.537220299243927 Validation loss 0.5447177886962891\n",
      "Epoch 8250/10000  training loss: 0.5371989607810974 Validation loss 0.5447102785110474\n",
      "Epoch 8260/10000  training loss: 0.5371776819229126 Validation loss 0.5447103381156921\n",
      "Epoch 8270/10000  training loss: 0.5371567010879517 Validation loss 0.5446956157684326\n",
      "Epoch 8280/10000  training loss: 0.537135899066925 Validation loss 0.544689953327179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8290/10000  training loss: 0.5371150970458984 Validation loss 0.5446707010269165\n",
      "Epoch 8300/10000  training loss: 0.5370945930480957 Validation loss 0.5446664690971375\n",
      "Epoch 8310/10000  training loss: 0.537074089050293 Validation loss 0.5446497797966003\n",
      "Epoch 8320/10000  training loss: 0.5370537042617798 Validation loss 0.5446407198905945\n",
      "Epoch 8330/10000  training loss: 0.5370333790779114 Validation loss 0.5446399450302124\n",
      "Epoch 8340/10000  training loss: 0.5370131731033325 Validation loss 0.5446265339851379\n",
      "Epoch 8350/10000  training loss: 0.5369929671287537 Validation loss 0.5446115732192993\n",
      "Epoch 8360/10000  training loss: 0.5369729399681091 Validation loss 0.5445926785469055\n",
      "Epoch 8370/10000  training loss: 0.5369529128074646 Validation loss 0.5445756912231445\n",
      "Epoch 8380/10000  training loss: 0.5369328856468201 Validation loss 0.5445584654808044\n",
      "Epoch 8390/10000  training loss: 0.5369130373001099 Validation loss 0.5445460081100464\n",
      "Epoch 8400/10000  training loss: 0.5368931889533997 Validation loss 0.5445327758789062\n",
      "Epoch 8410/10000  training loss: 0.5368733406066895 Validation loss 0.5445325970649719\n",
      "Epoch 8420/10000  training loss: 0.536853551864624 Validation loss 0.5445111989974976\n",
      "Epoch 8430/10000  training loss: 0.5368337631225586 Validation loss 0.5444912314414978\n",
      "Epoch 8440/10000  training loss: 0.5368140935897827 Validation loss 0.5444823503494263\n",
      "Epoch 8450/10000  training loss: 0.5367944240570068 Validation loss 0.5444628000259399\n",
      "Epoch 8460/10000  training loss: 0.536774754524231 Validation loss 0.5444499850273132\n",
      "Epoch 8470/10000  training loss: 0.5367551445960999 Validation loss 0.5444397926330566\n",
      "Epoch 8480/10000  training loss: 0.5367355346679688 Validation loss 0.5444233417510986\n",
      "Epoch 8490/10000  training loss: 0.5367158651351929 Validation loss 0.5444046854972839\n",
      "Epoch 8500/10000  training loss: 0.5366962552070618 Validation loss 0.5443838238716125\n",
      "Epoch 8510/10000  training loss: 0.5366767644882202 Validation loss 0.5443679690361023\n",
      "Epoch 8520/10000  training loss: 0.5366571545600891 Validation loss 0.5443581938743591\n",
      "Epoch 8530/10000  training loss: 0.536637544631958 Validation loss 0.5443364381790161\n",
      "Epoch 8540/10000  training loss: 0.5366180539131165 Validation loss 0.5443196892738342\n",
      "Epoch 8550/10000  training loss: 0.5365985035896301 Validation loss 0.5443028211593628\n",
      "Epoch 8560/10000  training loss: 0.5365788340568542 Validation loss 0.5442840456962585\n",
      "Epoch 8570/10000  training loss: 0.5365593433380127 Validation loss 0.5442684292793274\n",
      "Epoch 8580/10000  training loss: 0.5365398526191711 Validation loss 0.5442495346069336\n",
      "Epoch 8590/10000  training loss: 0.53652024269104 Validation loss 0.5442335605621338\n",
      "Epoch 8600/10000  training loss: 0.5365007519721985 Validation loss 0.544216513633728\n",
      "Epoch 8610/10000  training loss: 0.5364810824394226 Validation loss 0.5441941022872925\n",
      "Epoch 8620/10000  training loss: 0.5364614725112915 Validation loss 0.5441710352897644\n",
      "Epoch 8630/10000  training loss: 0.5364418625831604 Validation loss 0.5441516637802124\n",
      "Epoch 8640/10000  training loss: 0.536422073841095 Validation loss 0.5441330671310425\n",
      "Epoch 8650/10000  training loss: 0.5364024639129639 Validation loss 0.5441147089004517\n",
      "Epoch 8660/10000  training loss: 0.5363827347755432 Validation loss 0.5440961122512817\n",
      "Epoch 8670/10000  training loss: 0.5363629460334778 Validation loss 0.5440801382064819\n",
      "Epoch 8680/10000  training loss: 0.5363430976867676 Validation loss 0.544054388999939\n",
      "Epoch 8690/10000  training loss: 0.5363231897354126 Validation loss 0.5440405607223511\n",
      "Epoch 8700/10000  training loss: 0.5363033413887024 Validation loss 0.544021487236023\n",
      "Epoch 8710/10000  training loss: 0.5362833738327026 Validation loss 0.5439987182617188\n",
      "Epoch 8720/10000  training loss: 0.5362633466720581 Validation loss 0.5439687967300415\n",
      "Epoch 8730/10000  training loss: 0.5362433195114136 Validation loss 0.5439624786376953\n",
      "Epoch 8740/10000  training loss: 0.5362231731414795 Validation loss 0.5439382791519165\n",
      "Epoch 8750/10000  training loss: 0.5362030863761902 Validation loss 0.5439066886901855\n",
      "Epoch 8760/10000  training loss: 0.5361828207969666 Validation loss 0.5438892245292664\n",
      "Epoch 8770/10000  training loss: 0.5361626148223877 Validation loss 0.5438701510429382\n",
      "Epoch 8780/10000  training loss: 0.5361424088478088 Validation loss 0.5438467264175415\n",
      "Epoch 8790/10000  training loss: 0.5361220836639404 Validation loss 0.543825089931488\n",
      "Epoch 8800/10000  training loss: 0.5361018180847168 Validation loss 0.5438046455383301\n",
      "Epoch 8810/10000  training loss: 0.5360816717147827 Validation loss 0.5437488555908203\n",
      "Epoch 8820/10000  training loss: 0.5360615253448486 Validation loss 0.5437408089637756\n",
      "Epoch 8830/10000  training loss: 0.5360415577888489 Validation loss 0.5437194108963013\n",
      "Epoch 8840/10000  training loss: 0.5360217690467834 Validation loss 0.5436913967132568\n",
      "Epoch 8850/10000  training loss: 0.536001980304718 Validation loss 0.5436976552009583\n",
      "Epoch 8860/10000  training loss: 0.5359822511672974 Validation loss 0.543675422668457\n",
      "Epoch 8870/10000  training loss: 0.5359626412391663 Validation loss 0.5436772108078003\n",
      "Epoch 8880/10000  training loss: 0.5359426140785217 Validation loss 0.543666660785675\n",
      "Epoch 8890/10000  training loss: 0.5359228253364563 Validation loss 0.5436586141586304\n",
      "Epoch 8900/10000  training loss: 0.5359031558036804 Validation loss 0.5436595678329468\n",
      "Epoch 8910/10000  training loss: 0.5358840823173523 Validation loss 0.5436461567878723\n",
      "Epoch 8920/10000  training loss: 0.5358643531799316 Validation loss 0.5436400771141052\n",
      "Epoch 8930/10000  training loss: 0.5358449816703796 Validation loss 0.5436536073684692\n",
      "Epoch 8940/10000  training loss: 0.5358259081840515 Validation loss 0.5436420440673828\n",
      "Epoch 8950/10000  training loss: 0.5358064770698547 Validation loss 0.5436477661132812\n",
      "Epoch 8960/10000  training loss: 0.5357877016067505 Validation loss 0.5436339378356934\n",
      "Epoch 8970/10000  training loss: 0.5357683897018433 Validation loss 0.5436370968818665\n",
      "Epoch 8980/10000  training loss: 0.5357494950294495 Validation loss 0.5436433553695679\n",
      "Epoch 8990/10000  training loss: 0.5357300639152527 Validation loss 0.5436381101608276\n",
      "Epoch 9000/10000  training loss: 0.5357109904289246 Validation loss 0.543624997138977\n",
      "Epoch 9010/10000  training loss: 0.5356922149658203 Validation loss 0.5436321496963501\n",
      "Epoch 9020/10000  training loss: 0.5356733798980713 Validation loss 0.5436357259750366\n",
      "Epoch 9030/10000  training loss: 0.5356547236442566 Validation loss 0.5436314344406128\n",
      "Epoch 9040/10000  training loss: 0.5356360077857971 Validation loss 0.5436222553253174\n",
      "Epoch 9050/10000  training loss: 0.535617470741272 Validation loss 0.5436156988143921\n",
      "Epoch 9060/10000  training loss: 0.5355989336967468 Validation loss 0.5436080694198608\n",
      "Epoch 9070/10000  training loss: 0.5355803370475769 Validation loss 0.5436072945594788\n",
      "Epoch 9080/10000  training loss: 0.5355618596076965 Validation loss 0.5435982942581177\n",
      "Epoch 9090/10000  training loss: 0.5355436205863953 Validation loss 0.5435851812362671\n",
      "Epoch 9100/10000  training loss: 0.5355252027511597 Validation loss 0.5435813665390015\n",
      "Epoch 9110/10000  training loss: 0.5355070233345032 Validation loss 0.5435789823532104\n",
      "Epoch 9120/10000  training loss: 0.5354888439178467 Validation loss 0.5435706377029419\n",
      "Epoch 9130/10000  training loss: 0.5354706645011902 Validation loss 0.5435652732849121\n",
      "Epoch 9140/10000  training loss: 0.5354529023170471 Validation loss 0.5435672998428345\n",
      "Epoch 9150/10000  training loss: 0.5354347825050354 Validation loss 0.543545126914978\n",
      "Epoch 9160/10000  training loss: 0.535416841506958 Validation loss 0.5435425043106079\n",
      "Epoch 9170/10000  training loss: 0.5353995561599731 Validation loss 0.5435485243797302\n",
      "Epoch 9180/10000  training loss: 0.5353816151618958 Validation loss 0.5435279607772827\n",
      "Epoch 9190/10000  training loss: 0.5353637337684631 Validation loss 0.5435243248939514\n",
      "Epoch 9200/10000  training loss: 0.535346508026123 Validation loss 0.5435354113578796\n",
      "Epoch 9210/10000  training loss: 0.5353289842605591 Validation loss 0.5435010194778442\n",
      "Epoch 9220/10000  training loss: 0.5353114008903503 Validation loss 0.543516218662262\n",
      "Epoch 9230/10000  training loss: 0.5352941751480103 Validation loss 0.5435017347335815\n",
      "Epoch 9240/10000  training loss: 0.5352770686149597 Validation loss 0.543491780757904\n",
      "Epoch 9250/10000  training loss: 0.5352597832679749 Validation loss 0.5435087084770203\n",
      "Epoch 9260/10000  training loss: 0.535243034362793 Validation loss 0.5434838533401489\n",
      "Epoch 9270/10000  training loss: 0.5352258086204529 Validation loss 0.5434742569923401\n",
      "Epoch 9280/10000  training loss: 0.535208523273468 Validation loss 0.5434766411781311\n",
      "Epoch 9290/10000  training loss: 0.5351914763450623 Validation loss 0.5434638261795044\n",
      "Epoch 9300/10000  training loss: 0.5351746082305908 Validation loss 0.5434659719467163\n",
      "Epoch 9310/10000  training loss: 0.5351579785346985 Validation loss 0.54347163438797\n",
      "Epoch 9320/10000  training loss: 0.5351411700248718 Validation loss 0.5434897541999817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9330/10000  training loss: 0.5351248979568481 Validation loss 0.5434848070144653\n",
      "Epoch 9340/10000  training loss: 0.535108208656311 Validation loss 0.5434865355491638\n",
      "Epoch 9350/10000  training loss: 0.5350916981697083 Validation loss 0.5434718132019043\n",
      "Epoch 9360/10000  training loss: 0.5350756049156189 Validation loss 0.5434803366661072\n",
      "Epoch 9370/10000  training loss: 0.5350591540336609 Validation loss 0.543465793132782\n",
      "Epoch 9380/10000  training loss: 0.5350427627563477 Validation loss 0.5434536337852478\n",
      "Epoch 9390/10000  training loss: 0.5350268483161926 Validation loss 0.5434459447860718\n",
      "Epoch 9400/10000  training loss: 0.5350110530853271 Validation loss 0.5434499979019165\n",
      "Epoch 9410/10000  training loss: 0.5349947810173035 Validation loss 0.5434502959251404\n",
      "Epoch 9420/10000  training loss: 0.5349787473678589 Validation loss 0.5434502959251404\n",
      "Epoch 9430/10000  training loss: 0.5349631905555725 Validation loss 0.5434414744377136\n",
      "Epoch 9440/10000  training loss: 0.5349411964416504 Validation loss 0.5432117581367493\n",
      "Epoch 9450/10000  training loss: 0.534917950630188 Validation loss 0.543261706829071\n",
      "Epoch 9460/10000  training loss: 0.5348952412605286 Validation loss 0.5433284044265747\n",
      "Epoch 9470/10000  training loss: 0.5348734259605408 Validation loss 0.5433276891708374\n",
      "Epoch 9480/10000  training loss: 0.5348527431488037 Validation loss 0.5433077812194824\n",
      "Epoch 9490/10000  training loss: 0.5348329544067383 Validation loss 0.5432485342025757\n",
      "Epoch 9500/10000  training loss: 0.5348135828971863 Validation loss 0.5432276129722595\n",
      "Epoch 9510/10000  training loss: 0.5347945690155029 Validation loss 0.54317706823349\n",
      "Epoch 9520/10000  training loss: 0.5347753167152405 Validation loss 0.543166995048523\n",
      "Epoch 9530/10000  training loss: 0.5347566604614258 Validation loss 0.5431456565856934\n",
      "Epoch 9540/10000  training loss: 0.534737765789032 Validation loss 0.543127179145813\n",
      "Epoch 9550/10000  training loss: 0.5347192287445068 Validation loss 0.5431019067764282\n",
      "Epoch 9560/10000  training loss: 0.5347006320953369 Validation loss 0.5430672764778137\n",
      "Epoch 9570/10000  training loss: 0.5346820950508118 Validation loss 0.5430880784988403\n",
      "Epoch 9580/10000  training loss: 0.5346638560295105 Validation loss 0.5430622100830078\n",
      "Epoch 9590/10000  training loss: 0.5346459150314331 Validation loss 0.5430540442466736\n",
      "Epoch 9600/10000  training loss: 0.5346241593360901 Validation loss 0.5428680181503296\n",
      "Epoch 9610/10000  training loss: 0.5346024036407471 Validation loss 0.5428379774093628\n",
      "Epoch 9620/10000  training loss: 0.5345805287361145 Validation loss 0.5428644418716431\n",
      "Epoch 9630/10000  training loss: 0.5345475077629089 Validation loss 0.5426269769668579\n",
      "Epoch 9640/10000  training loss: 0.5345144271850586 Validation loss 0.5426725149154663\n",
      "Epoch 9650/10000  training loss: 0.534483015537262 Validation loss 0.5427266359329224\n",
      "Epoch 9660/10000  training loss: 0.5344529747962952 Validation loss 0.5426589250564575\n",
      "Epoch 9670/10000  training loss: 0.5344244241714478 Validation loss 0.5426371097564697\n",
      "Epoch 9680/10000  training loss: 0.5343971252441406 Validation loss 0.5426232218742371\n",
      "Epoch 9690/10000  training loss: 0.5343708395957947 Validation loss 0.5425898432731628\n",
      "Epoch 9700/10000  training loss: 0.5343453884124756 Validation loss 0.542568564414978\n",
      "Epoch 9710/10000  training loss: 0.5343207716941833 Validation loss 0.5425496101379395\n",
      "Epoch 9720/10000  training loss: 0.5342965722084045 Validation loss 0.5425255298614502\n",
      "Epoch 9730/10000  training loss: 0.5342729091644287 Validation loss 0.5425032377243042\n",
      "Epoch 9740/10000  training loss: 0.5342497229576111 Validation loss 0.5424811244010925\n",
      "Epoch 9750/10000  training loss: 0.5342267155647278 Validation loss 0.542458176612854\n",
      "Epoch 9760/10000  training loss: 0.5342041254043579 Validation loss 0.5424360036849976\n",
      "Epoch 9770/10000  training loss: 0.5341817140579224 Validation loss 0.5424140691757202\n",
      "Epoch 9780/10000  training loss: 0.5341596007347107 Validation loss 0.5423985123634338\n",
      "Epoch 9790/10000  training loss: 0.5341376662254333 Validation loss 0.5423763990402222\n",
      "Epoch 9800/10000  training loss: 0.5341159105300903 Validation loss 0.5423538684844971\n",
      "Epoch 9810/10000  training loss: 0.5340943336486816 Validation loss 0.5423337817192078\n",
      "Epoch 9820/10000  training loss: 0.5340728163719177 Validation loss 0.542312502861023\n",
      "Epoch 9830/10000  training loss: 0.5340515971183777 Validation loss 0.5423039197921753\n",
      "Epoch 9840/10000  training loss: 0.5340304374694824 Validation loss 0.5422809720039368\n",
      "Epoch 9850/10000  training loss: 0.5340093970298767 Validation loss 0.5422588586807251\n",
      "Epoch 9860/10000  training loss: 0.5339885354042053 Validation loss 0.542242705821991\n",
      "Epoch 9870/10000  training loss: 0.5339676737785339 Validation loss 0.5422225594520569\n",
      "Epoch 9880/10000  training loss: 0.5339469313621521 Validation loss 0.5421978831291199\n",
      "Epoch 9890/10000  training loss: 0.5339263081550598 Validation loss 0.5421744585037231\n",
      "Epoch 9900/10000  training loss: 0.5339056849479675 Validation loss 0.5421503186225891\n",
      "Epoch 9910/10000  training loss: 0.5338852405548096 Validation loss 0.5421308279037476\n",
      "Epoch 9920/10000  training loss: 0.5338649749755859 Validation loss 0.5421403646469116\n",
      "Epoch 9930/10000  training loss: 0.5338448286056519 Validation loss 0.5421309471130371\n",
      "Epoch 9940/10000  training loss: 0.5338248610496521 Validation loss 0.5421156287193298\n",
      "Epoch 9950/10000  training loss: 0.5338049530982971 Validation loss 0.5421100854873657\n",
      "Epoch 9960/10000  training loss: 0.5337852239608765 Validation loss 0.5420910120010376\n",
      "Epoch 9970/10000  training loss: 0.5337655544281006 Validation loss 0.5420719981193542\n",
      "Epoch 9980/10000  training loss: 0.5337459444999695 Validation loss 0.542055070400238\n",
      "Epoch 9990/10000  training loss: 0.5337263941764832 Validation loss 0.5420383214950562\n"
     ]
    }
   ],
   "source": [
    "loss, loss_val = model.train(X_train, Y_train, X_val, y_val, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Cross Entropy Loss')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f97e4dd16d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss, label=\"Taining Loss\")\n",
    "plt.plot(loss_val, label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(np.float32(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.778125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "accuracy = sum(np.argmax(Y_pred,1) == np.argmax(np.array(y_val),1))/len(y_val)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f97e40fbcf8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f97e44d9828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "cm = confusion_matrix(np.argmax(np.array(y_val),1), np.argmax(Y_pred,1))\n",
    "sns.heatmap(cm,annot=True,fmt='2.0f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    855\n",
       "2    744\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
