{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/am/anaconda3/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self,n_input=2,n_hidden=4, n_output=1, act_func=[tf.nn.elu, tf.sigmoid], learning_rate= 0.001):\n",
    "        self.n_input = n_input # Number of inputs to the neuron\n",
    "        self.act_fn = act_func\n",
    "        seed = 123\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, name='X', shape=[None,n_input])\n",
    "        self.y = tf.placeholder(tf.float32, name='Y')\n",
    "                \n",
    "        # Build the graph for a single neuron\n",
    "        # Hidden layer\n",
    "        self.W1 = tf.Variable(tf.random_normal([n_input,n_hidden], stddev=2, seed = seed), name = \"weights\")\n",
    "        self.b1 = tf.Variable(tf.random_normal([1, n_hidden], seed = seed), name=\"bias\")\n",
    "        tf.summary.histogram(\"Weights_Layer_1\",self.W1)\n",
    "        tf.summary.histogram(\"Bias_Layer_1\", self.b1)\n",
    "        \n",
    "        \n",
    "        # Output Layer\n",
    "        self.W2 = tf.Variable(tf.random_normal([n_hidden,n_output], stddev=2, seed = 0), name = \"weights\")\n",
    "        self.b2 = tf.Variable(tf.random_normal([1, n_output], seed = seed), name=\"bias\")\n",
    "        tf.summary.histogram(\"Weights_Layer_2\",self.W2)\n",
    "        tf.summary.histogram(\"Bias_Layer_2\", self.b2)\n",
    "               \n",
    "        \n",
    "        activity = tf.matmul(self.X, self.W1) + self.b1\n",
    "        h1 = self.act_fn[0](activity)\n",
    "        \n",
    "        activity = tf.matmul(h1, self.W2) + self.b2\n",
    "        self.y_hat = self.act_fn[1](activity)\n",
    "        \n",
    "        \n",
    "        error = self.y - self.y_hat\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.square(error)) + 0.6*tf.nn.l2_loss(self.W1) #+ 0.6*tf.nn.l2_loss(self.W2)\n",
    "        self.opt =  tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(self.loss)\n",
    "        \n",
    "        \n",
    "        tf.summary.scalar(\"loss\",self.loss)\n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "        self.merge = tf.summary.merge_all()\n",
    "        self.writer = tf.summary.FileWriter(\"logs/\", graph=tf.get_default_graph())\n",
    "        \n",
    "        \n",
    "        \n",
    "    def train(self, X, Y, X_val, Y_val, epochs=100):\n",
    "        epoch = 0\n",
    "        X, Y = shuffle(X,Y)\n",
    "        loss = []\n",
    "        loss_val = []\n",
    "        while epoch < epochs:\n",
    "            # Run the optimizer for the whole training set batch wise (Stochastic Gradient Descent)    \n",
    "            merge, _, l = self.sess.run([self.merge,self.opt,self.loss], feed_dict={self.X: X, self.y: Y})\n",
    "            l_val = self.sess.run(self.loss, feed_dict={self.X: X_val, self.y: Y_val})\n",
    "            \n",
    "            loss.append(l)\n",
    "            loss_val.append(l_val)\n",
    "            self.writer.add_summary(merge, epoch)\n",
    "                \n",
    "            if epoch % 10 == 0:\n",
    "                print(\"Epoch {}/{}  training loss: {} Validation loss {}\".\\\n",
    "                      format(epoch,epochs,l, l_val ))\n",
    "                \n",
    "               \n",
    "            epoch += 1\n",
    "        return loss, loss_val\n",
    "            \n",
    "    def predict(self, X):\n",
    "        return self.sess.run(self.y_hat, feed_dict={self.X: X})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'Folds5x2_pp.xlsx'\n",
    "df = pd.read_excel(filename, sheet_name='Sheet1')\n",
    "X, Y = df[['AT', 'V','AP','RH']], df['PE']\n",
    "scaler = MinMaxScaler()\n",
    "X_new = scaler.fit_transform(X)\n",
    "target_scaler = MinMaxScaler()\n",
    "Y_new = target_scaler.fit_transform(Y.values.reshape(-1,1))\n",
    "X_train, X_val, Y_train, y_val = \\\n",
    "  train_test_split(X_new, Y_new, test_size=0.4, random_state=333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, d = X_train.shape\n",
    "_, n = Y_train.shape\n",
    "model = MLP(n_input=d, n_hidden=15, n_output=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/6000  training loss: 108.84590911865234 Validation loss 108.71923828125\n",
      "Epoch 10/6000  training loss: 107.54991912841797 Validation loss 107.42478942871094\n",
      "Epoch 20/6000  training loss: 106.2693862915039 Validation loss 106.14578247070312\n",
      "Epoch 30/6000  training loss: 105.00411224365234 Validation loss 104.88204193115234\n",
      "Epoch 40/6000  training loss: 103.75394439697266 Validation loss 103.63336181640625\n",
      "Epoch 50/6000  training loss: 102.51868438720703 Validation loss 102.39956665039062\n",
      "Epoch 60/6000  training loss: 101.29817199707031 Validation loss 101.18049621582031\n",
      "Epoch 70/6000  training loss: 100.09220886230469 Validation loss 99.97598266601562\n",
      "Epoch 80/6000  training loss: 98.90062713623047 Validation loss 98.78582763671875\n",
      "Epoch 90/6000  training loss: 97.72327423095703 Validation loss 97.60987091064453\n",
      "Epoch 100/6000  training loss: 96.55992889404297 Validation loss 96.44790649414062\n",
      "Epoch 110/6000  training loss: 95.41049194335938 Validation loss 95.29981994628906\n",
      "Epoch 120/6000  training loss: 94.2747573852539 Validation loss 94.16545104980469\n",
      "Epoch 130/6000  training loss: 93.1525650024414 Validation loss 93.04459381103516\n",
      "Epoch 140/6000  training loss: 92.04374694824219 Validation loss 91.9371109008789\n",
      "Epoch 150/6000  training loss: 90.94818115234375 Validation loss 90.84282684326172\n",
      "Epoch 160/6000  training loss: 89.86566162109375 Validation loss 89.76158905029297\n",
      "Epoch 170/6000  training loss: 88.79605865478516 Validation loss 88.6932601928711\n",
      "Epoch 180/6000  training loss: 87.73921203613281 Validation loss 87.63766479492188\n",
      "Epoch 190/6000  training loss: 86.6949691772461 Validation loss 86.59466552734375\n",
      "Epoch 200/6000  training loss: 85.66317749023438 Validation loss 85.56410217285156\n",
      "Epoch 210/6000  training loss: 84.64368438720703 Validation loss 84.54582214355469\n",
      "Epoch 220/6000  training loss: 83.63636779785156 Validation loss 83.53968811035156\n",
      "Epoch 230/6000  training loss: 82.64104461669922 Validation loss 82.54554748535156\n",
      "Epoch 240/6000  training loss: 81.65760040283203 Validation loss 81.56327056884766\n",
      "Epoch 250/6000  training loss: 80.68589782714844 Validation loss 80.59271240234375\n",
      "Epoch 260/6000  training loss: 79.72577667236328 Validation loss 79.63370513916016\n",
      "Epoch 270/6000  training loss: 78.7770767211914 Validation loss 78.68614959716797\n",
      "Epoch 280/6000  training loss: 77.83971405029297 Validation loss 77.7498779296875\n",
      "Epoch 290/6000  training loss: 76.91352844238281 Validation loss 76.82479858398438\n",
      "Epoch 300/6000  training loss: 75.99838256835938 Validation loss 75.91072082519531\n",
      "Epoch 310/6000  training loss: 75.09415435791016 Validation loss 75.00755310058594\n",
      "Epoch 320/6000  training loss: 74.2007064819336 Validation loss 74.11516571044922\n",
      "Epoch 330/6000  training loss: 73.31790924072266 Validation loss 73.23341369628906\n",
      "Epoch 340/6000  training loss: 72.44564819335938 Validation loss 72.36217498779297\n",
      "Epoch 350/6000  training loss: 71.58378601074219 Validation loss 71.50131225585938\n",
      "Epoch 360/6000  training loss: 70.73219299316406 Validation loss 70.65072631835938\n",
      "Epoch 370/6000  training loss: 69.89075469970703 Validation loss 69.81027221679688\n",
      "Epoch 380/6000  training loss: 69.05934143066406 Validation loss 68.9798355102539\n",
      "Epoch 390/6000  training loss: 68.23784637451172 Validation loss 68.15930938720703\n",
      "Epoch 400/6000  training loss: 67.4261474609375 Validation loss 67.34855651855469\n",
      "Epoch 410/6000  training loss: 66.6241226196289 Validation loss 66.54747772216797\n",
      "Epoch 420/6000  training loss: 65.8316650390625 Validation loss 65.75593566894531\n",
      "Epoch 430/6000  training loss: 65.04865264892578 Validation loss 64.97383880615234\n",
      "Epoch 440/6000  training loss: 64.27497100830078 Validation loss 64.20105743408203\n",
      "Epoch 450/6000  training loss: 63.51051330566406 Validation loss 63.4375\n",
      "Epoch 460/6000  training loss: 62.75516128540039 Validation loss 62.68302917480469\n",
      "Epoch 470/6000  training loss: 62.00881576538086 Validation loss 61.93754959106445\n",
      "Epoch 480/6000  training loss: 61.2713623046875 Validation loss 61.200965881347656\n",
      "Epoch 490/6000  training loss: 60.5427131652832 Validation loss 60.4731559753418\n",
      "Epoch 500/6000  training loss: 59.82273864746094 Validation loss 59.754024505615234\n",
      "Epoch 510/6000  training loss: 59.11135482788086 Validation loss 59.043460845947266\n",
      "Epoch 520/6000  training loss: 58.40843963623047 Validation loss 58.34135818481445\n",
      "Epoch 530/6000  training loss: 57.71390151977539 Validation loss 57.64762878417969\n",
      "Epoch 540/6000  training loss: 57.02764129638672 Validation loss 56.9621696472168\n",
      "Epoch 550/6000  training loss: 56.34955596923828 Validation loss 56.284873962402344\n",
      "Epoch 560/6000  training loss: 55.67955780029297 Validation loss 55.61564636230469\n",
      "Epoch 570/6000  training loss: 55.01754379272461 Validation loss 54.95439147949219\n",
      "Epoch 580/6000  training loss: 54.363407135009766 Validation loss 54.3010139465332\n",
      "Epoch 590/6000  training loss: 53.717071533203125 Validation loss 53.655433654785156\n",
      "Epoch 600/6000  training loss: 53.078433990478516 Validation loss 53.01753234863281\n",
      "Epoch 610/6000  training loss: 52.4473991394043 Validation loss 52.387229919433594\n",
      "Epoch 620/6000  training loss: 51.823890686035156 Validation loss 51.76443099975586\n",
      "Epoch 630/6000  training loss: 51.20780944824219 Validation loss 51.14906692504883\n",
      "Epoch 640/6000  training loss: 50.599063873291016 Validation loss 50.541015625\n",
      "Epoch 650/6000  training loss: 49.9975700378418 Validation loss 49.94021224975586\n",
      "Epoch 660/6000  training loss: 49.403228759765625 Validation loss 49.3465576171875\n",
      "Epoch 670/6000  training loss: 48.815982818603516 Validation loss 48.75998306274414\n",
      "Epoch 680/6000  training loss: 48.2357177734375 Validation loss 48.18038558959961\n",
      "Epoch 690/6000  training loss: 47.6623649597168 Validation loss 47.607696533203125\n",
      "Epoch 700/6000  training loss: 47.0958366394043 Validation loss 47.04181671142578\n",
      "Epoch 710/6000  training loss: 46.53605651855469 Validation loss 46.48267364501953\n",
      "Epoch 720/6000  training loss: 45.982933044433594 Validation loss 45.9301872253418\n",
      "Epoch 730/6000  training loss: 45.436397552490234 Validation loss 45.384281158447266\n",
      "Epoch 740/6000  training loss: 44.89636993408203 Validation loss 44.84486770629883\n",
      "Epoch 750/6000  training loss: 44.36276626586914 Validation loss 44.31187057495117\n",
      "Epoch 760/6000  training loss: 43.83551025390625 Validation loss 43.78522491455078\n",
      "Epoch 770/6000  training loss: 43.31452941894531 Validation loss 43.264835357666016\n",
      "Epoch 780/6000  training loss: 42.79975128173828 Validation loss 42.75063705444336\n",
      "Epoch 790/6000  training loss: 42.29108428955078 Validation loss 42.24256134033203\n",
      "Epoch 800/6000  training loss: 41.78847122192383 Validation loss 41.74052047729492\n",
      "Epoch 810/6000  training loss: 41.291839599609375 Validation loss 41.24445343017578\n",
      "Epoch 820/6000  training loss: 40.801109313964844 Validation loss 40.75428009033203\n",
      "Epoch 830/6000  training loss: 40.31620788574219 Validation loss 40.26993942260742\n",
      "Epoch 840/6000  training loss: 39.83707809448242 Validation loss 39.791351318359375\n",
      "Epoch 850/6000  training loss: 39.36363983154297 Validation loss 39.318450927734375\n",
      "Epoch 860/6000  training loss: 38.89583206176758 Validation loss 38.851173400878906\n",
      "Epoch 870/6000  training loss: 38.43357467651367 Validation loss 38.389442443847656\n",
      "Epoch 880/6000  training loss: 37.97681427001953 Validation loss 37.93320083618164\n",
      "Epoch 890/6000  training loss: 37.525474548339844 Validation loss 37.48237609863281\n",
      "Epoch 900/6000  training loss: 37.07949447631836 Validation loss 37.036903381347656\n",
      "Epoch 910/6000  training loss: 36.63880920410156 Validation loss 36.596717834472656\n",
      "Epoch 920/6000  training loss: 36.20335388183594 Validation loss 36.1617546081543\n",
      "Epoch 930/6000  training loss: 35.773067474365234 Validation loss 35.731956481933594\n",
      "Epoch 940/6000  training loss: 35.347877502441406 Validation loss 35.307247161865234\n",
      "Epoch 950/6000  training loss: 34.927738189697266 Validation loss 34.88758850097656\n",
      "Epoch 960/6000  training loss: 34.512577056884766 Validation loss 34.47289276123047\n",
      "Epoch 970/6000  training loss: 34.10233688354492 Validation loss 34.063114166259766\n",
      "Epoch 980/6000  training loss: 33.696956634521484 Validation loss 33.6581916809082\n",
      "Epoch 990/6000  training loss: 33.296382904052734 Validation loss 33.2580680847168\n",
      "Epoch 1000/6000  training loss: 32.90054702758789 Validation loss 32.8626823425293\n",
      "Epoch 1010/6000  training loss: 32.509395599365234 Validation loss 32.47197341918945\n",
      "Epoch 1020/6000  training loss: 32.12287521362305 Validation loss 32.08588790893555\n",
      "Epoch 1030/6000  training loss: 31.740928649902344 Validation loss 31.704368591308594\n",
      "Epoch 1040/6000  training loss: 31.363494873046875 Validation loss 31.327360153198242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1050/6000  training loss: 30.990524291992188 Validation loss 30.954809188842773\n",
      "Epoch 1060/6000  training loss: 30.6219539642334 Validation loss 30.586654663085938\n",
      "Epoch 1070/6000  training loss: 30.257740020751953 Validation loss 30.22284698486328\n",
      "Epoch 1080/6000  training loss: 29.8978271484375 Validation loss 29.863338470458984\n",
      "Epoch 1090/6000  training loss: 29.542156219482422 Validation loss 29.50806427001953\n",
      "Epoch 1100/6000  training loss: 29.19068145751953 Validation loss 29.156984329223633\n",
      "Epoch 1110/6000  training loss: 28.843345642089844 Validation loss 28.810035705566406\n",
      "Epoch 1120/6000  training loss: 28.500099182128906 Validation loss 28.467172622680664\n",
      "Epoch 1130/6000  training loss: 28.160898208618164 Validation loss 28.12834930419922\n",
      "Epoch 1140/6000  training loss: 27.82568359375 Validation loss 27.79351043701172\n",
      "Epoch 1150/6000  training loss: 27.49441146850586 Validation loss 27.462610244750977\n",
      "Epoch 1160/6000  training loss: 27.16703224182129 Validation loss 27.13559341430664\n",
      "Epoch 1170/6000  training loss: 26.84349250793457 Validation loss 26.812416076660156\n",
      "Epoch 1180/6000  training loss: 26.523754119873047 Validation loss 26.493026733398438\n",
      "Epoch 1190/6000  training loss: 26.207763671875 Validation loss 26.17738914489746\n",
      "Epoch 1200/6000  training loss: 25.895475387573242 Validation loss 25.86544418334961\n",
      "Epoch 1210/6000  training loss: 25.58683967590332 Validation loss 25.557157516479492\n",
      "Epoch 1220/6000  training loss: 25.281824111938477 Validation loss 25.252477645874023\n",
      "Epoch 1230/6000  training loss: 24.980371475219727 Validation loss 24.951358795166016\n",
      "Epoch 1240/6000  training loss: 24.682443618774414 Validation loss 24.65375518798828\n",
      "Epoch 1250/6000  training loss: 24.387990951538086 Validation loss 24.35963249206543\n",
      "Epoch 1260/6000  training loss: 24.09697723388672 Validation loss 24.06894302368164\n",
      "Epoch 1270/6000  training loss: 23.809362411499023 Validation loss 23.781641006469727\n",
      "Epoch 1280/6000  training loss: 23.52509880065918 Validation loss 23.49768829345703\n",
      "Epoch 1290/6000  training loss: 23.244144439697266 Validation loss 23.2170467376709\n",
      "Epoch 1300/6000  training loss: 22.966463088989258 Validation loss 22.939672470092773\n",
      "Epoch 1310/6000  training loss: 22.692018508911133 Validation loss 22.665529251098633\n",
      "Epoch 1320/6000  training loss: 22.420766830444336 Validation loss 22.394582748413086\n",
      "Epoch 1330/6000  training loss: 22.152677536010742 Validation loss 22.126787185668945\n",
      "Epoch 1340/6000  training loss: 21.88770294189453 Validation loss 21.86210823059082\n",
      "Epoch 1350/6000  training loss: 21.625810623168945 Validation loss 21.600507736206055\n",
      "Epoch 1360/6000  training loss: 21.36697006225586 Validation loss 21.341955184936523\n",
      "Epoch 1370/6000  training loss: 21.11113929748535 Validation loss 21.086416244506836\n",
      "Epoch 1380/6000  training loss: 20.858291625976562 Validation loss 20.833850860595703\n",
      "Epoch 1390/6000  training loss: 20.6083927154541 Validation loss 20.5842342376709\n",
      "Epoch 1400/6000  training loss: 20.361404418945312 Validation loss 20.337535858154297\n",
      "Epoch 1410/6000  training loss: 20.117305755615234 Validation loss 20.093717575073242\n",
      "Epoch 1420/6000  training loss: 19.876054763793945 Validation loss 19.85275650024414\n",
      "Epoch 1430/6000  training loss: 19.637632369995117 Validation loss 19.614612579345703\n",
      "Epoch 1440/6000  training loss: 19.402008056640625 Validation loss 19.379270553588867\n",
      "Epoch 1450/6000  training loss: 19.169147491455078 Validation loss 19.146696090698242\n",
      "Epoch 1460/6000  training loss: 18.93903160095215 Validation loss 18.9168643951416\n",
      "Epoch 1470/6000  training loss: 18.711626052856445 Validation loss 18.689746856689453\n",
      "Epoch 1480/6000  training loss: 18.486913681030273 Validation loss 18.46531867980957\n",
      "Epoch 1490/6000  training loss: 18.264862060546875 Validation loss 18.243553161621094\n",
      "Epoch 1500/6000  training loss: 18.04545021057129 Validation loss 18.024431228637695\n",
      "Epoch 1510/6000  training loss: 17.828649520874023 Validation loss 17.807918548583984\n",
      "Epoch 1520/6000  training loss: 17.61444091796875 Validation loss 17.593996047973633\n",
      "Epoch 1530/6000  training loss: 17.40279769897461 Validation loss 17.38264274597168\n",
      "Epoch 1540/6000  training loss: 17.193695068359375 Validation loss 17.173828125\n",
      "Epoch 1550/6000  training loss: 16.98710823059082 Validation loss 16.967531204223633\n",
      "Epoch 1560/6000  training loss: 16.783018112182617 Validation loss 16.76372528076172\n",
      "Epoch 1570/6000  training loss: 16.581392288208008 Validation loss 16.562387466430664\n",
      "Epoch 1580/6000  training loss: 16.38221549987793 Validation loss 16.363492965698242\n",
      "Epoch 1590/6000  training loss: 16.185457229614258 Validation loss 16.167015075683594\n",
      "Epoch 1600/6000  training loss: 15.99109172821045 Validation loss 15.972928047180176\n",
      "Epoch 1610/6000  training loss: 15.79909896850586 Validation loss 15.781208038330078\n",
      "Epoch 1620/6000  training loss: 15.60944938659668 Validation loss 15.591830253601074\n",
      "Epoch 1630/6000  training loss: 15.422119140625 Validation loss 15.404766082763672\n",
      "Epoch 1640/6000  training loss: 15.237079620361328 Validation loss 15.219989776611328\n",
      "Epoch 1650/6000  training loss: 15.05430793762207 Validation loss 15.037474632263184\n",
      "Epoch 1660/6000  training loss: 14.873775482177734 Validation loss 14.857195854187012\n",
      "Epoch 1670/6000  training loss: 14.695454597473145 Validation loss 14.679122924804688\n",
      "Epoch 1680/6000  training loss: 14.519320487976074 Validation loss 14.503232955932617\n",
      "Epoch 1690/6000  training loss: 14.345345497131348 Validation loss 14.329493522644043\n",
      "Epoch 1700/6000  training loss: 14.173500061035156 Validation loss 14.157882690429688\n",
      "Epoch 1710/6000  training loss: 14.00376033782959 Validation loss 13.988369941711426\n",
      "Epoch 1720/6000  training loss: 13.836097717285156 Validation loss 13.820930480957031\n",
      "Epoch 1730/6000  training loss: 13.670487403869629 Validation loss 13.655534744262695\n",
      "Epoch 1740/6000  training loss: 13.506898880004883 Validation loss 13.492159843444824\n",
      "Epoch 1750/6000  training loss: 13.345308303833008 Validation loss 13.330777168273926\n",
      "Epoch 1760/6000  training loss: 13.185688018798828 Validation loss 13.171361923217773\n",
      "Epoch 1770/6000  training loss: 13.0280122756958 Validation loss 13.013886451721191\n",
      "Epoch 1780/6000  training loss: 12.872254371643066 Validation loss 12.85832405090332\n",
      "Epoch 1790/6000  training loss: 12.718391418457031 Validation loss 12.704655647277832\n",
      "Epoch 1800/6000  training loss: 12.566399574279785 Validation loss 12.552848815917969\n",
      "Epoch 1810/6000  training loss: 12.416251182556152 Validation loss 12.402884483337402\n",
      "Epoch 1820/6000  training loss: 12.267922401428223 Validation loss 12.25473403930664\n",
      "Epoch 1830/6000  training loss: 12.12138843536377 Validation loss 12.108378410339355\n",
      "Epoch 1840/6000  training loss: 11.976625442504883 Validation loss 11.963788032531738\n",
      "Epoch 1850/6000  training loss: 11.833610534667969 Validation loss 11.820945739746094\n",
      "Epoch 1860/6000  training loss: 11.692323684692383 Validation loss 11.679827690124512\n",
      "Epoch 1870/6000  training loss: 11.552738189697266 Validation loss 11.540407180786133\n",
      "Epoch 1880/6000  training loss: 11.414835929870605 Validation loss 11.402666091918945\n",
      "Epoch 1890/6000  training loss: 11.278593063354492 Validation loss 11.266584396362305\n",
      "Epoch 1900/6000  training loss: 11.143988609313965 Validation loss 11.132136344909668\n",
      "Epoch 1910/6000  training loss: 11.01099967956543 Validation loss 10.99930191040039\n",
      "Epoch 1920/6000  training loss: 10.879609107971191 Validation loss 10.868064880371094\n",
      "Epoch 1930/6000  training loss: 10.74979305267334 Validation loss 10.738399505615234\n",
      "Epoch 1940/6000  training loss: 10.621535301208496 Validation loss 10.610288619995117\n",
      "Epoch 1950/6000  training loss: 10.494813919067383 Validation loss 10.483713150024414\n",
      "Epoch 1960/6000  training loss: 10.369611740112305 Validation loss 10.358654975891113\n",
      "Epoch 1970/6000  training loss: 10.245906829833984 Validation loss 10.235093116760254\n",
      "Epoch 1980/6000  training loss: 10.123682975769043 Validation loss 10.113008499145508\n",
      "Epoch 1990/6000  training loss: 10.002920150756836 Validation loss 9.992384910583496\n",
      "Epoch 2000/6000  training loss: 9.883602142333984 Validation loss 9.873202323913574\n",
      "Epoch 2010/6000  training loss: 9.76570987701416 Validation loss 9.755443572998047\n",
      "Epoch 2020/6000  training loss: 9.649226188659668 Validation loss 9.639091491699219\n",
      "Epoch 2030/6000  training loss: 9.534131050109863 Validation loss 9.524128913879395\n",
      "Epoch 2040/6000  training loss: 9.420413970947266 Validation loss 9.410539627075195\n",
      "Epoch 2050/6000  training loss: 9.308052062988281 Validation loss 9.29830551147461\n",
      "Epoch 2060/6000  training loss: 9.19703197479248 Validation loss 9.187411308288574\n",
      "Epoch 2070/6000  training loss: 9.087336540222168 Validation loss 9.077839851379395\n",
      "Epoch 2080/6000  training loss: 8.978949546813965 Validation loss 8.969574928283691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2090/6000  training loss: 8.871855735778809 Validation loss 8.862602233886719\n",
      "Epoch 2100/6000  training loss: 8.76603889465332 Validation loss 8.756905555725098\n",
      "Epoch 2110/6000  training loss: 8.66148567199707 Validation loss 8.652469635009766\n",
      "Epoch 2120/6000  training loss: 8.558177947998047 Validation loss 8.549278259277344\n",
      "Epoch 2130/6000  training loss: 8.456101417541504 Validation loss 8.447317123413086\n",
      "Epoch 2140/6000  training loss: 8.355242729187012 Validation loss 8.346571922302246\n",
      "Epoch 2150/6000  training loss: 8.255585670471191 Validation loss 8.247027397155762\n",
      "Epoch 2160/6000  training loss: 8.157116889953613 Validation loss 8.148667335510254\n",
      "Epoch 2170/6000  training loss: 8.059823036193848 Validation loss 8.051482200622559\n",
      "Epoch 2180/6000  training loss: 7.963688850402832 Validation loss 7.955456256866455\n",
      "Epoch 2190/6000  training loss: 7.8687005043029785 Validation loss 7.860574245452881\n",
      "Epoch 2200/6000  training loss: 7.774844169616699 Validation loss 7.766822814941406\n",
      "Epoch 2210/6000  training loss: 7.6821064949035645 Validation loss 7.67418909072876\n",
      "Epoch 2220/6000  training loss: 7.590475082397461 Validation loss 7.582659721374512\n",
      "Epoch 2230/6000  training loss: 7.499936103820801 Validation loss 7.492220878601074\n",
      "Epoch 2240/6000  training loss: 7.410475254058838 Validation loss 7.402860641479492\n",
      "Epoch 2250/6000  training loss: 7.322082042694092 Validation loss 7.314566612243652\n",
      "Epoch 2260/6000  training loss: 7.234742641448975 Validation loss 7.227323532104492\n",
      "Epoch 2270/6000  training loss: 7.148444175720215 Validation loss 7.141120910644531\n",
      "Epoch 2280/6000  training loss: 7.063173770904541 Validation loss 7.055944919586182\n",
      "Epoch 2290/6000  training loss: 6.978920936584473 Validation loss 6.971786022186279\n",
      "Epoch 2300/6000  training loss: 6.895672798156738 Validation loss 6.888630390167236\n",
      "Epoch 2310/6000  training loss: 6.813417434692383 Validation loss 6.806465148925781\n",
      "Epoch 2320/6000  training loss: 6.732141971588135 Validation loss 6.72528076171875\n",
      "Epoch 2330/6000  training loss: 6.651835918426514 Validation loss 6.645063877105713\n",
      "Epoch 2340/6000  training loss: 6.5724873542785645 Validation loss 6.565802097320557\n",
      "Epoch 2350/6000  training loss: 6.494085311889648 Validation loss 6.487486362457275\n",
      "Epoch 2360/6000  training loss: 6.416617393493652 Validation loss 6.410104751586914\n",
      "Epoch 2370/6000  training loss: 6.340074062347412 Validation loss 6.333645820617676\n",
      "Epoch 2380/6000  training loss: 6.264444351196289 Validation loss 6.258099555969238\n",
      "Epoch 2390/6000  training loss: 6.189715385437012 Validation loss 6.183452129364014\n",
      "Epoch 2400/6000  training loss: 6.115878105163574 Validation loss 6.109696388244629\n",
      "Epoch 2410/6000  training loss: 6.04292106628418 Validation loss 6.036820411682129\n",
      "Epoch 2420/6000  training loss: 5.970835208892822 Validation loss 5.964813709259033\n",
      "Epoch 2430/6000  training loss: 5.899608612060547 Validation loss 5.893664836883545\n",
      "Epoch 2440/6000  training loss: 5.829232692718506 Validation loss 5.823366165161133\n",
      "Epoch 2450/6000  training loss: 5.759695053100586 Validation loss 5.753904819488525\n",
      "Epoch 2460/6000  training loss: 5.690987586975098 Validation loss 5.685272216796875\n",
      "Epoch 2470/6000  training loss: 5.6230998039245605 Validation loss 5.617458820343018\n",
      "Epoch 2480/6000  training loss: 5.5560221672058105 Validation loss 5.550454616546631\n",
      "Epoch 2490/6000  training loss: 5.489744663238525 Validation loss 5.484250068664551\n",
      "Epoch 2500/6000  training loss: 5.424258232116699 Validation loss 5.418835639953613\n",
      "Epoch 2510/6000  training loss: 5.359553337097168 Validation loss 5.35420036315918\n",
      "Epoch 2520/6000  training loss: 5.295620918273926 Validation loss 5.290338039398193\n",
      "Epoch 2530/6000  training loss: 5.23245096206665 Validation loss 5.227236747741699\n",
      "Epoch 2540/6000  training loss: 5.170035362243652 Validation loss 5.164889335632324\n",
      "Epoch 2550/6000  training loss: 5.108364105224609 Validation loss 5.103285312652588\n",
      "Epoch 2560/6000  training loss: 5.047429084777832 Validation loss 5.042417526245117\n",
      "Epoch 2570/6000  training loss: 4.987222194671631 Validation loss 4.982275485992432\n",
      "Epoch 2580/6000  training loss: 4.927733898162842 Validation loss 4.922852039337158\n",
      "Epoch 2590/6000  training loss: 4.868955612182617 Validation loss 4.864138126373291\n",
      "Epoch 2600/6000  training loss: 4.810879707336426 Validation loss 4.806124687194824\n",
      "Epoch 2610/6000  training loss: 4.7534966468811035 Validation loss 4.748803615570068\n",
      "Epoch 2620/6000  training loss: 4.696799278259277 Validation loss 4.69216775894165\n",
      "Epoch 2630/6000  training loss: 4.640778064727783 Validation loss 4.636207103729248\n",
      "Epoch 2640/6000  training loss: 4.5854268074035645 Validation loss 4.580915927886963\n",
      "Epoch 2650/6000  training loss: 4.530736923217773 Validation loss 4.526284694671631\n",
      "Epoch 2660/6000  training loss: 4.476699352264404 Validation loss 4.472306728363037\n",
      "Epoch 2670/6000  training loss: 4.4233078956604 Validation loss 4.418972015380859\n",
      "Epoch 2680/6000  training loss: 4.370554447174072 Validation loss 4.366275787353516\n",
      "Epoch 2690/6000  training loss: 4.318431377410889 Validation loss 4.314208507537842\n",
      "Epoch 2700/6000  training loss: 4.26693058013916 Validation loss 4.262763500213623\n",
      "Epoch 2710/6000  training loss: 4.216045379638672 Validation loss 4.211933612823486\n",
      "Epoch 2720/6000  training loss: 4.165768146514893 Validation loss 4.161710739135742\n",
      "Epoch 2730/6000  training loss: 4.116091728210449 Validation loss 4.112087726593018\n",
      "Epoch 2740/6000  training loss: 4.067009449005127 Validation loss 4.063057899475098\n",
      "Epoch 2750/6000  training loss: 4.0185136795043945 Validation loss 4.014614105224609\n",
      "Epoch 2760/6000  training loss: 3.970597267150879 Validation loss 3.966749429702759\n",
      "Epoch 2770/6000  training loss: 3.9232535362243652 Validation loss 3.919456720352173\n",
      "Epoch 2780/6000  training loss: 3.876476526260376 Validation loss 3.872730016708374\n",
      "Epoch 2790/6000  training loss: 3.830258369445801 Validation loss 3.826561450958252\n",
      "Epoch 2800/6000  training loss: 3.784592390060425 Validation loss 3.780944585800171\n",
      "Epoch 2810/6000  training loss: 3.7394728660583496 Validation loss 3.7358734607696533\n",
      "Epoch 2820/6000  training loss: 3.6948931217193604 Validation loss 3.6913414001464844\n",
      "Epoch 2830/6000  training loss: 3.650846242904663 Validation loss 3.6473419666290283\n",
      "Epoch 2840/6000  training loss: 3.6073267459869385 Validation loss 3.6038689613342285\n",
      "Epoch 2850/6000  training loss: 3.5643277168273926 Validation loss 3.56091570854187\n",
      "Epoch 2860/6000  training loss: 3.521843433380127 Validation loss 3.518476963043213\n",
      "Epoch 2870/6000  training loss: 3.4798667430877686 Validation loss 3.4765453338623047\n",
      "Epoch 2880/6000  training loss: 3.4383931159973145 Validation loss 3.4351155757904053\n",
      "Epoch 2890/6000  training loss: 3.3974151611328125 Validation loss 3.394181728363037\n",
      "Epoch 2900/6000  training loss: 3.3569278717041016 Validation loss 3.3537375926971436\n",
      "Epoch 2910/6000  training loss: 3.316925048828125 Validation loss 3.313777208328247\n",
      "Epoch 2920/6000  training loss: 3.2774012088775635 Validation loss 3.2742953300476074\n",
      "Epoch 2930/6000  training loss: 3.2383503913879395 Validation loss 3.235285997390747\n",
      "Epoch 2940/6000  training loss: 3.1997666358947754 Validation loss 3.1967437267303467\n",
      "Epoch 2950/6000  training loss: 3.1616451740264893 Validation loss 3.1586625576019287\n",
      "Epoch 2960/6000  training loss: 3.1239805221557617 Validation loss 3.1210379600524902\n",
      "Epoch 2970/6000  training loss: 3.086766004562378 Validation loss 3.0838632583618164\n",
      "Epoch 2980/6000  training loss: 3.0499978065490723 Validation loss 3.0471339225769043\n",
      "Epoch 2990/6000  training loss: 3.0136702060699463 Validation loss 3.0108447074890137\n",
      "Epoch 3000/6000  training loss: 2.9777770042419434 Validation loss 2.974989414215088\n",
      "Epoch 3010/6000  training loss: 2.9423136711120605 Validation loss 2.9395639896392822\n",
      "Epoch 3020/6000  training loss: 2.907275915145874 Validation loss 2.9045629501342773\n",
      "Epoch 3030/6000  training loss: 2.872657299041748 Validation loss 2.869981288909912\n",
      "Epoch 3040/6000  training loss: 2.838453769683838 Validation loss 2.8358137607574463\n",
      "Epoch 3050/6000  training loss: 2.8046600818634033 Validation loss 2.802055597305298\n",
      "Epoch 3060/6000  training loss: 2.771271228790283 Validation loss 2.7687020301818848\n",
      "Epoch 3070/6000  training loss: 2.7382822036743164 Validation loss 2.735748052597046\n",
      "Epoch 3080/6000  training loss: 2.705688953399658 Validation loss 2.7031893730163574\n",
      "Epoch 3090/6000  training loss: 2.6734864711761475 Validation loss 2.671020269393921\n",
      "Epoch 3100/6000  training loss: 2.641669750213623 Validation loss 2.63923716545105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3110/6000  training loss: 2.610234022140503 Validation loss 2.607834577560425\n",
      "Epoch 3120/6000  training loss: 2.5791757106781006 Validation loss 2.5768089294433594\n",
      "Epoch 3130/6000  training loss: 2.5484893321990967 Validation loss 2.5461552143096924\n",
      "Epoch 3140/6000  training loss: 2.5181713104248047 Validation loss 2.515868902206421\n",
      "Epoch 3150/6000  training loss: 2.4882168769836426 Validation loss 2.485945463180542\n",
      "Epoch 3160/6000  training loss: 2.4586215019226074 Validation loss 2.4563815593719482\n",
      "Epoch 3170/6000  training loss: 2.4293811321258545 Validation loss 2.427171468734741\n",
      "Epoch 3180/6000  training loss: 2.400491237640381 Validation loss 2.3983120918273926\n",
      "Epoch 3190/6000  training loss: 2.371948003768921 Validation loss 2.3697986602783203\n",
      "Epoch 3200/6000  training loss: 2.34374737739563 Validation loss 2.341627359390259\n",
      "Epoch 3210/6000  training loss: 2.315884590148926 Validation loss 2.3137943744659424\n",
      "Epoch 3220/6000  training loss: 2.2883570194244385 Validation loss 2.286294937133789\n",
      "Epoch 3230/6000  training loss: 2.2611591815948486 Validation loss 2.2591257095336914\n",
      "Epoch 3240/6000  training loss: 2.234287738800049 Validation loss 2.2322821617126465\n",
      "Epoch 3250/6000  training loss: 2.2077386379241943 Validation loss 2.205760955810547\n",
      "Epoch 3260/6000  training loss: 2.1815080642700195 Validation loss 2.1795578002929688\n",
      "Epoch 3270/6000  training loss: 2.155592918395996 Validation loss 2.1536693572998047\n",
      "Epoch 3280/6000  training loss: 2.129988193511963 Validation loss 2.128091335296631\n",
      "Epoch 3290/6000  training loss: 2.10469126701355 Validation loss 2.102820873260498\n",
      "Epoch 3300/6000  training loss: 2.079697847366333 Validation loss 2.0778536796569824\n",
      "Epoch 3310/6000  training loss: 2.055004358291626 Validation loss 2.0531859397888184\n",
      "Epoch 3320/6000  training loss: 2.0306077003479004 Validation loss 2.0288145542144775\n",
      "Epoch 3330/6000  training loss: 2.0065040588378906 Validation loss 2.0047359466552734\n",
      "Epoch 3340/6000  training loss: 1.982689380645752 Validation loss 1.9809458255767822\n",
      "Epoch 3350/6000  training loss: 1.9591609239578247 Validation loss 1.957441806793213\n",
      "Epoch 3360/6000  training loss: 1.9359151124954224 Validation loss 1.9342201948165894\n",
      "Epoch 3370/6000  training loss: 1.9129483699798584 Validation loss 1.911277174949646\n",
      "Epoch 3380/6000  training loss: 1.8902575969696045 Validation loss 1.8886098861694336\n",
      "Epoch 3390/6000  training loss: 1.8678390979766846 Validation loss 1.8662148714065552\n",
      "Epoch 3400/6000  training loss: 1.845690369606018 Validation loss 1.8440890312194824\n",
      "Epoch 3410/6000  training loss: 1.8238078355789185 Validation loss 1.8222289085388184\n",
      "Epoch 3420/6000  training loss: 1.8021880388259888 Validation loss 1.8006316423416138\n",
      "Epoch 3430/6000  training loss: 1.7808278799057007 Validation loss 1.7792937755584717\n",
      "Epoch 3440/6000  training loss: 1.759724497795105 Validation loss 1.7582122087478638\n",
      "Epoch 3450/6000  training loss: 1.7388747930526733 Validation loss 1.7373838424682617\n",
      "Epoch 3460/6000  training loss: 1.7182754278182983 Validation loss 1.7168060541152954\n",
      "Epoch 3470/6000  training loss: 1.697924017906189 Validation loss 1.6964752674102783\n",
      "Epoch 3480/6000  training loss: 1.6778168678283691 Validation loss 1.6763890981674194\n",
      "Epoch 3490/6000  training loss: 1.6579514741897583 Validation loss 1.6565443277359009\n",
      "Epoch 3500/6000  training loss: 1.6383250951766968 Validation loss 1.636938214302063\n",
      "Epoch 3510/6000  training loss: 1.6189345121383667 Validation loss 1.617567539215088\n",
      "Epoch 3520/6000  training loss: 1.599777340888977 Validation loss 1.5984300374984741\n",
      "Epoch 3530/6000  training loss: 1.58085036277771 Validation loss 1.5795223712921143\n",
      "Epoch 3540/6000  training loss: 1.5621505975723267 Validation loss 1.5608422756195068\n",
      "Epoch 3550/6000  training loss: 1.5436760187149048 Validation loss 1.542386770248413\n",
      "Epoch 3560/6000  training loss: 1.5254234075546265 Validation loss 1.5241529941558838\n",
      "Epoch 3570/6000  training loss: 1.5073906183242798 Validation loss 1.5061386823654175\n",
      "Epoch 3580/6000  training loss: 1.4895745515823364 Validation loss 1.4883408546447754\n",
      "Epoch 3590/6000  training loss: 1.4719728231430054 Validation loss 1.470757246017456\n",
      "Epoch 3600/6000  training loss: 1.4545824527740479 Validation loss 1.4533851146697998\n",
      "Epoch 3610/6000  training loss: 1.4374016523361206 Validation loss 1.436221957206726\n",
      "Epoch 3620/6000  training loss: 1.4204275608062744 Validation loss 1.4192651510238647\n",
      "Epoch 3630/6000  training loss: 1.4036575555801392 Validation loss 1.4025124311447144\n",
      "Epoch 3640/6000  training loss: 1.387089490890503 Validation loss 1.3859612941741943\n",
      "Epoch 3650/6000  training loss: 1.370720386505127 Validation loss 1.3696093559265137\n",
      "Epoch 3660/6000  training loss: 1.354548454284668 Validation loss 1.3534537553787231\n",
      "Epoch 3670/6000  training loss: 1.3385710716247559 Validation loss 1.3374930620193481\n",
      "Epoch 3680/6000  training loss: 1.3227858543395996 Validation loss 1.3217239379882812\n",
      "Epoch 3690/6000  training loss: 1.3071908950805664 Validation loss 1.3061449527740479\n",
      "Epoch 3700/6000  training loss: 1.291783332824707 Validation loss 1.2907534837722778\n",
      "Epoch 3710/6000  training loss: 1.2765614986419678 Validation loss 1.275546908378601\n",
      "Epoch 3720/6000  training loss: 1.2615227699279785 Validation loss 1.2605235576629639\n",
      "Epoch 3730/6000  training loss: 1.2466648817062378 Validation loss 1.2456811666488647\n",
      "Epoch 3740/6000  training loss: 1.2319855690002441 Validation loss 1.2310171127319336\n",
      "Epoch 3750/6000  training loss: 1.2174835205078125 Validation loss 1.2165297269821167\n",
      "Epoch 3760/6000  training loss: 1.2031558752059937 Validation loss 1.2022167444229126\n",
      "Epoch 3770/6000  training loss: 1.1890007257461548 Validation loss 1.1880760192871094\n",
      "Epoch 3780/6000  training loss: 1.175015926361084 Validation loss 1.1741056442260742\n",
      "Epoch 3790/6000  training loss: 1.1611994504928589 Validation loss 1.1603031158447266\n",
      "Epoch 3800/6000  training loss: 1.147549033164978 Validation loss 1.1466670036315918\n",
      "Epoch 3810/6000  training loss: 1.1340632438659668 Validation loss 1.1331950426101685\n",
      "Epoch 3820/6000  training loss: 1.1207398176193237 Validation loss 1.1198852062225342\n",
      "Epoch 3830/6000  training loss: 1.1075769662857056 Validation loss 1.1067357063293457\n",
      "Epoch 3840/6000  training loss: 1.0945724248886108 Validation loss 1.0937446355819702\n",
      "Epoch 3850/6000  training loss: 1.0817246437072754 Validation loss 1.080910086631775\n",
      "Epoch 3860/6000  training loss: 1.0690313577651978 Validation loss 1.0682296752929688\n",
      "Epoch 3870/6000  training loss: 1.0564912557601929 Validation loss 1.0557023286819458\n",
      "Epoch 3880/6000  training loss: 1.0441019535064697 Validation loss 1.0433259010314941\n",
      "Epoch 3890/6000  training loss: 1.0318617820739746 Validation loss 1.0310982465744019\n",
      "Epoch 3900/6000  training loss: 1.019769310951233 Validation loss 1.0190180540084839\n",
      "Epoch 3910/6000  training loss: 1.0078222751617432 Validation loss 1.0070831775665283\n",
      "Epoch 3920/6000  training loss: 0.9960192441940308 Validation loss 0.9952921867370605\n",
      "Epoch 3930/6000  training loss: 0.9843583703041077 Validation loss 0.9836432337760925\n",
      "Epoch 3940/6000  training loss: 0.9728378057479858 Validation loss 0.972134530544281\n",
      "Epoch 3950/6000  training loss: 0.9614560604095459 Validation loss 0.9607645273208618\n",
      "Epoch 3960/6000  training loss: 0.9502116441726685 Validation loss 0.9495314955711365\n",
      "Epoch 3970/6000  training loss: 0.9391024708747864 Validation loss 0.9384337663650513\n",
      "Epoch 3980/6000  training loss: 0.9281272292137146 Validation loss 0.9274696111679077\n",
      "Epoch 3990/6000  training loss: 0.9172842502593994 Validation loss 0.9166375994682312\n",
      "Epoch 4000/6000  training loss: 0.9065716862678528 Validation loss 0.905936062335968\n",
      "Epoch 4010/6000  training loss: 0.8959883451461792 Validation loss 0.8953635096549988\n",
      "Epoch 4020/6000  training loss: 0.885532557964325 Validation loss 0.8849182724952698\n",
      "Epoch 4030/6000  training loss: 0.8752025961875916 Validation loss 0.8745989203453064\n",
      "Epoch 4040/6000  training loss: 0.864997148513794 Validation loss 0.8644039034843445\n",
      "Epoch 4050/6000  training loss: 0.854914665222168 Validation loss 0.8543317914009094\n",
      "Epoch 4060/6000  training loss: 0.844953715801239 Validation loss 0.8443809151649475\n",
      "Epoch 4070/6000  training loss: 0.8351128101348877 Validation loss 0.8345500230789185\n",
      "Epoch 4080/6000  training loss: 0.8253904581069946 Validation loss 0.8248375058174133\n",
      "Epoch 4090/6000  training loss: 0.81578528881073 Validation loss 0.8152421116828918\n",
      "Epoch 4100/6000  training loss: 0.8062956929206848 Validation loss 0.8057621717453003\n",
      "Epoch 4110/6000  training loss: 0.796920657157898 Validation loss 0.7963966727256775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4120/6000  training loss: 0.7876584529876709 Validation loss 0.7871440052986145\n",
      "Epoch 4130/6000  training loss: 0.7785079479217529 Validation loss 0.7780027389526367\n",
      "Epoch 4140/6000  training loss: 0.7694676518440247 Validation loss 0.7689716815948486\n",
      "Epoch 4150/6000  training loss: 0.7605364322662354 Validation loss 0.7600495219230652\n",
      "Epoch 4160/6000  training loss: 0.7517127394676208 Validation loss 0.7512346506118774\n",
      "Epoch 4170/6000  training loss: 0.7429953217506409 Validation loss 0.7425262331962585\n",
      "Epoch 4180/6000  training loss: 0.7343831062316895 Validation loss 0.7339227199554443\n",
      "Epoch 4190/6000  training loss: 0.7258746027946472 Validation loss 0.7254228591918945\n",
      "Epoch 4200/6000  training loss: 0.7174686789512634 Validation loss 0.7170254588127136\n",
      "Epoch 4210/6000  training loss: 0.709164023399353 Validation loss 0.7087292671203613\n",
      "Epoch 4220/6000  training loss: 0.7009595632553101 Validation loss 0.7005330324172974\n",
      "Epoch 4230/6000  training loss: 0.6928539276123047 Validation loss 0.692435622215271\n",
      "Epoch 4240/6000  training loss: 0.6848458647727966 Validation loss 0.6844357848167419\n",
      "Epoch 4250/6000  training loss: 0.676934540271759 Validation loss 0.6765323877334595\n",
      "Epoch 4260/6000  training loss: 0.6691184639930725 Validation loss 0.6687242388725281\n",
      "Epoch 4270/6000  training loss: 0.6613966226577759 Validation loss 0.6610101461410522\n",
      "Epoch 4280/6000  training loss: 0.6537678241729736 Validation loss 0.6533891558647156\n",
      "Epoch 4290/6000  training loss: 0.646230936050415 Validation loss 0.6458598971366882\n",
      "Epoch 4300/6000  training loss: 0.6387850046157837 Validation loss 0.6384214758872986\n",
      "Epoch 4310/6000  training loss: 0.6314286589622498 Validation loss 0.6310725808143616\n",
      "Epoch 4320/6000  training loss: 0.6241610646247864 Validation loss 0.6238123774528503\n",
      "Epoch 4330/6000  training loss: 0.6169810891151428 Validation loss 0.6166396141052246\n",
      "Epoch 4340/6000  training loss: 0.6098876595497131 Validation loss 0.6095533967018127\n",
      "Epoch 4350/6000  training loss: 0.6028797030448914 Validation loss 0.6025525331497192\n",
      "Epoch 4360/6000  training loss: 0.5959562063217163 Validation loss 0.5956360697746277\n",
      "Epoch 4370/6000  training loss: 0.5891162157058716 Validation loss 0.5888029336929321\n",
      "Epoch 4380/6000  training loss: 0.582358717918396 Validation loss 0.5820521712303162\n",
      "Epoch 4390/6000  training loss: 0.5756825804710388 Validation loss 0.5753828883171082\n",
      "Epoch 4400/6000  training loss: 0.5690869092941284 Validation loss 0.5687938332557678\n",
      "Epoch 4410/6000  training loss: 0.5625707507133484 Validation loss 0.5622842311859131\n",
      "Epoch 4420/6000  training loss: 0.5561332106590271 Validation loss 0.5558531880378723\n",
      "Epoch 4430/6000  training loss: 0.5497732162475586 Validation loss 0.54949951171875\n",
      "Epoch 4440/6000  training loss: 0.5434898138046265 Validation loss 0.5432225465774536\n",
      "Epoch 4450/6000  training loss: 0.5372822284698486 Validation loss 0.5370211601257324\n",
      "Epoch 4460/6000  training loss: 0.5311495065689087 Validation loss 0.5308946371078491\n",
      "Epoch 4470/6000  training loss: 0.5250906348228455 Validation loss 0.524841845035553\n",
      "Epoch 4480/6000  training loss: 0.5191048383712769 Validation loss 0.5188620686531067\n",
      "Epoch 4490/6000  training loss: 0.5131911039352417 Validation loss 0.5129543542861938\n",
      "Epoch 4500/6000  training loss: 0.5073487162590027 Validation loss 0.5071178674697876\n",
      "Epoch 4510/6000  training loss: 0.5015767812728882 Validation loss 0.5013516545295715\n",
      "Epoch 4520/6000  training loss: 0.49587443470954895 Validation loss 0.49565500020980835\n",
      "Epoch 4530/6000  training loss: 0.49024078249931335 Validation loss 0.4900270402431488\n",
      "Epoch 4540/6000  training loss: 0.48467499017715454 Validation loss 0.4844668507575989\n",
      "Epoch 4550/6000  training loss: 0.4791763722896576 Validation loss 0.4789736866950989\n",
      "Epoch 4560/6000  training loss: 0.4737439453601837 Validation loss 0.47354674339294434\n",
      "Epoch 4570/6000  training loss: 0.468377023935318 Validation loss 0.46818527579307556\n",
      "Epoch 4580/6000  training loss: 0.46307486295700073 Validation loss 0.46288833022117615\n",
      "Epoch 4590/6000  training loss: 0.4578365385532379 Validation loss 0.45765528082847595\n",
      "Epoch 4600/6000  training loss: 0.45266133546829224 Validation loss 0.45248526334762573\n",
      "Epoch 4610/6000  training loss: 0.4475485384464264 Validation loss 0.44737762212753296\n",
      "Epoch 4620/6000  training loss: 0.4424973726272583 Validation loss 0.4423314929008484\n",
      "Epoch 4630/6000  training loss: 0.43750709295272827 Validation loss 0.4373462498188019\n",
      "Epoch 4640/6000  training loss: 0.4325769245624542 Validation loss 0.4324209988117218\n",
      "Epoch 4650/6000  training loss: 0.42770621180534363 Validation loss 0.4275551438331604\n",
      "Epoch 4660/6000  training loss: 0.4228942394256592 Validation loss 0.422747939825058\n",
      "Epoch 4670/6000  training loss: 0.4181402325630188 Validation loss 0.41799864172935486\n",
      "Epoch 4680/6000  training loss: 0.4134434759616852 Validation loss 0.4133066236972809\n",
      "Epoch 4690/6000  training loss: 0.4088033437728882 Validation loss 0.408671110868454\n",
      "Epoch 4700/6000  training loss: 0.4042191207408905 Validation loss 0.4040915071964264\n",
      "Epoch 4710/6000  training loss: 0.399690181016922 Validation loss 0.3995670676231384\n",
      "Epoch 4720/6000  training loss: 0.39521580934524536 Validation loss 0.39509716629981995\n",
      "Epoch 4730/6000  training loss: 0.39079537987709045 Validation loss 0.3906811475753784\n",
      "Epoch 4740/6000  training loss: 0.38642817735671997 Validation loss 0.38631829619407654\n",
      "Epoch 4750/6000  training loss: 0.38211366534233093 Validation loss 0.38200807571411133\n",
      "Epoch 4760/6000  training loss: 0.37785109877586365 Validation loss 0.3777497708797455\n",
      "Epoch 4770/6000  training loss: 0.3736399710178375 Validation loss 0.37354275584220886\n",
      "Epoch 4780/6000  training loss: 0.3694795072078705 Validation loss 0.3693865239620209\n",
      "Epoch 4790/6000  training loss: 0.3653692901134491 Validation loss 0.36528027057647705\n",
      "Epoch 4800/6000  training loss: 0.3613085150718689 Validation loss 0.36122357845306396\n",
      "Epoch 4810/6000  training loss: 0.3572966456413269 Validation loss 0.3572157621383667\n",
      "Epoch 4820/6000  training loss: 0.3533332049846649 Validation loss 0.3532562255859375\n",
      "Epoch 4830/6000  training loss: 0.349417507648468 Validation loss 0.3493443727493286\n",
      "Epoch 4840/6000  training loss: 0.34554898738861084 Validation loss 0.34547969698905945\n",
      "Epoch 4850/6000  training loss: 0.341727077960968 Validation loss 0.34166157245635986\n",
      "Epoch 4860/6000  training loss: 0.3379511833190918 Validation loss 0.3378894031047821\n",
      "Epoch 4870/6000  training loss: 0.3342207968235016 Validation loss 0.33416277170181274\n",
      "Epoch 4880/6000  training loss: 0.3305353820323944 Validation loss 0.3304809629917145\n",
      "Epoch 4890/6000  training loss: 0.3268943727016449 Validation loss 0.32684358954429626\n",
      "Epoch 4900/6000  training loss: 0.3232972323894501 Validation loss 0.3232499957084656\n",
      "Epoch 4910/6000  training loss: 0.31974345445632935 Validation loss 0.3196996748447418\n",
      "Epoch 4920/6000  training loss: 0.31623244285583496 Validation loss 0.3161921501159668\n",
      "Epoch 4930/6000  training loss: 0.3127637803554535 Validation loss 0.3127269148826599\n",
      "Epoch 4940/6000  training loss: 0.30933690071105957 Validation loss 0.3093034029006958\n",
      "Epoch 4950/6000  training loss: 0.30595123767852783 Validation loss 0.30592113733291626\n",
      "Epoch 4960/6000  training loss: 0.3026064336299896 Validation loss 0.3025795817375183\n",
      "Epoch 4970/6000  training loss: 0.2993019223213196 Validation loss 0.2992783188819885\n",
      "Epoch 4980/6000  training loss: 0.2960372269153595 Validation loss 0.2960168421268463\n",
      "Epoch 4990/6000  training loss: 0.2928118109703064 Validation loss 0.2927946448326111\n",
      "Epoch 5000/6000  training loss: 0.28962528705596924 Validation loss 0.28961122035980225\n",
      "Epoch 5010/6000  training loss: 0.28647711873054504 Validation loss 0.28646615147590637\n",
      "Epoch 5020/6000  training loss: 0.283366858959198 Validation loss 0.28335899114608765\n",
      "Epoch 5030/6000  training loss: 0.28029412031173706 Validation loss 0.2802892327308655\n",
      "Epoch 5040/6000  training loss: 0.27725839614868164 Validation loss 0.27725648880004883\n",
      "Epoch 5050/6000  training loss: 0.27425920963287354 Validation loss 0.2742602229118347\n",
      "Epoch 5060/6000  training loss: 0.2712961733341217 Validation loss 0.2713001072406769\n",
      "Epoch 5070/6000  training loss: 0.26836881041526794 Validation loss 0.2683756351470947\n",
      "Epoch 5080/6000  training loss: 0.2654767334461212 Validation loss 0.26548638939857483\n",
      "Epoch 5090/6000  training loss: 0.2626194953918457 Validation loss 0.262631893157959\n",
      "Epoch 5100/6000  training loss: 0.259796679019928 Validation loss 0.2598118782043457\n",
      "Epoch 5110/6000  training loss: 0.2570078372955322 Validation loss 0.25702574849128723\n",
      "Epoch 5120/6000  training loss: 0.2542525827884674 Validation loss 0.2542731761932373\n",
      "Epoch 5130/6000  training loss: 0.2515304982662201 Validation loss 0.2515537142753601\n",
      "Epoch 5140/6000  training loss: 0.24884119629859924 Validation loss 0.24886707961559296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5150/6000  training loss: 0.246184304356575 Validation loss 0.24621278047561646\n",
      "Epoch 5160/6000  training loss: 0.24355940520763397 Validation loss 0.24359044432640076\n",
      "Epoch 5170/6000  training loss: 0.2409660816192627 Validation loss 0.24099963903427124\n",
      "Epoch 5180/6000  training loss: 0.2384040355682373 Validation loss 0.23844009637832642\n",
      "Epoch 5190/6000  training loss: 0.2358728051185608 Validation loss 0.2359113246202469\n",
      "Epoch 5200/6000  training loss: 0.23337207734584808 Validation loss 0.23341301083564758\n",
      "Epoch 5210/6000  training loss: 0.23090146481990814 Validation loss 0.23094481229782104\n",
      "Epoch 5220/6000  training loss: 0.22846056520938873 Validation loss 0.22850629687309265\n",
      "Epoch 5230/6000  training loss: 0.22604911029338837 Validation loss 0.22609715163707733\n",
      "Epoch 5240/6000  training loss: 0.22366668283939362 Validation loss 0.22371704876422882\n",
      "Epoch 5250/6000  training loss: 0.22131291031837463 Validation loss 0.2213655561208725\n",
      "Epoch 5260/6000  training loss: 0.21898749470710754 Validation loss 0.21904239058494568\n",
      "Epoch 5270/6000  training loss: 0.2166900634765625 Validation loss 0.2167471945285797\n",
      "Epoch 5280/6000  training loss: 0.21442030370235443 Validation loss 0.21447962522506714\n",
      "Epoch 5290/6000  training loss: 0.21217787265777588 Validation loss 0.2122393548488617\n",
      "Epoch 5300/6000  training loss: 0.20996247231960297 Validation loss 0.2100260853767395\n",
      "Epoch 5310/6000  training loss: 0.20777368545532227 Validation loss 0.20783942937850952\n",
      "Epoch 5320/6000  training loss: 0.20561127364635468 Validation loss 0.20567910373210907\n",
      "Epoch 5330/6000  training loss: 0.20347490906715393 Validation loss 0.2035447657108307\n",
      "Epoch 5340/6000  training loss: 0.20136421918869019 Validation loss 0.20143616199493408\n",
      "Epoch 5350/6000  training loss: 0.19927898049354553 Validation loss 0.19935289025306702\n",
      "Epoch 5360/6000  training loss: 0.19721882045269012 Validation loss 0.19729472696781158\n",
      "Epoch 5370/6000  training loss: 0.19518348574638367 Validation loss 0.19526131451129913\n",
      "Epoch 5380/6000  training loss: 0.1931726336479187 Validation loss 0.19325242936611176\n",
      "Epoch 5390/6000  training loss: 0.19118598103523254 Validation loss 0.19126765429973602\n",
      "Epoch 5400/6000  training loss: 0.1892232447862625 Validation loss 0.1893068253993988\n",
      "Epoch 5410/6000  training loss: 0.18728414177894592 Validation loss 0.18736955523490906\n",
      "Epoch 5420/6000  training loss: 0.1853683590888977 Validation loss 0.18545565009117126\n",
      "Epoch 5430/6000  training loss: 0.18347564339637756 Validation loss 0.18356473743915558\n",
      "Epoch 5440/6000  training loss: 0.1816057562828064 Validation loss 0.1816965937614441\n",
      "Epoch 5450/6000  training loss: 0.17975829541683197 Validation loss 0.17985093593597412\n",
      "Epoch 5460/6000  training loss: 0.17793311178684235 Validation loss 0.17802749574184418\n",
      "Epoch 5470/6000  training loss: 0.17612987756729126 Validation loss 0.17622599005699158\n",
      "Epoch 5480/6000  training loss: 0.1743483692407608 Validation loss 0.17444616556167603\n",
      "Epoch 5490/6000  training loss: 0.1725882738828659 Validation loss 0.17268773913383484\n",
      "Epoch 5500/6000  training loss: 0.17084935307502747 Validation loss 0.1709505021572113\n",
      "Epoch 5510/6000  training loss: 0.16913138329982758 Validation loss 0.16923414170742035\n",
      "Epoch 5520/6000  training loss: 0.16743409633636475 Validation loss 0.16753846406936646\n",
      "Epoch 5530/6000  training loss: 0.1657572090625763 Validation loss 0.16586315631866455\n",
      "Epoch 5540/6000  training loss: 0.1641004979610443 Validation loss 0.1642080396413803\n",
      "Epoch 5550/6000  training loss: 0.1624637097120285 Validation loss 0.16257280111312866\n",
      "Epoch 5560/6000  training loss: 0.16084662079811096 Validation loss 0.1609572321176529\n",
      "Epoch 5570/6000  training loss: 0.15924900770187378 Validation loss 0.15936113893985748\n",
      "Epoch 5580/6000  training loss: 0.15767061710357666 Validation loss 0.15778423845767975\n",
      "Epoch 5590/6000  training loss: 0.1561111956834793 Validation loss 0.1562262773513794\n",
      "Epoch 5600/6000  training loss: 0.1545705646276474 Validation loss 0.1546870917081833\n",
      "Epoch 5610/6000  training loss: 0.15304842591285706 Validation loss 0.15316641330718994\n",
      "Epoch 5620/6000  training loss: 0.15154463052749634 Validation loss 0.15166400372982025\n",
      "Epoch 5630/6000  training loss: 0.15005889534950256 Validation loss 0.1501796990633011\n",
      "Epoch 5640/6000  training loss: 0.1485910713672638 Validation loss 0.14871323108673096\n",
      "Epoch 5650/6000  training loss: 0.14714089035987854 Validation loss 0.14726440608501434\n",
      "Epoch 5660/6000  training loss: 0.1457081437110901 Validation loss 0.14583303034305573\n",
      "Epoch 5670/6000  training loss: 0.1442926526069641 Validation loss 0.1444188505411148\n",
      "Epoch 5680/6000  training loss: 0.14289416372776031 Validation loss 0.14302168786525726\n",
      "Epoch 5690/6000  training loss: 0.14151252806186676 Validation loss 0.14164133369922638\n",
      "Epoch 5700/6000  training loss: 0.14014747738838196 Validation loss 0.14027756452560425\n",
      "Epoch 5710/6000  training loss: 0.13879884779453278 Validation loss 0.13893020153045654\n",
      "Epoch 5720/6000  training loss: 0.1374664604663849 Validation loss 0.13759905099868774\n",
      "Epoch 5730/6000  training loss: 0.1361500769853592 Validation loss 0.13628390431404114\n",
      "Epoch 5740/6000  training loss: 0.13484953343868256 Validation loss 0.1349845677614212\n",
      "Epoch 5750/6000  training loss: 0.13356465101242065 Validation loss 0.1337008774280548\n",
      "Epoch 5760/6000  training loss: 0.13229520618915558 Validation loss 0.13243260979652405\n",
      "Epoch 5770/6000  training loss: 0.1310410052537918 Validation loss 0.1311795860528946\n",
      "Epoch 5780/6000  training loss: 0.1298019140958786 Validation loss 0.1299416422843933\n",
      "Epoch 5790/6000  training loss: 0.12857770919799805 Validation loss 0.1287185698747635\n",
      "Epoch 5800/6000  training loss: 0.1273682415485382 Validation loss 0.1275102198123932\n",
      "Epoch 5810/6000  training loss: 0.12617330253124237 Validation loss 0.1263163834810257\n",
      "Epoch 5820/6000  training loss: 0.124992735683918 Validation loss 0.12513691186904907\n",
      "Epoch 5830/6000  training loss: 0.12382635474205017 Validation loss 0.1239716112613678\n",
      "Epoch 5840/6000  training loss: 0.12267403304576874 Validation loss 0.12282033264636993\n",
      "Epoch 5850/6000  training loss: 0.1215355396270752 Validation loss 0.12168289721012115\n",
      "Epoch 5860/6000  training loss: 0.1204107329249382 Validation loss 0.12055912613868713\n",
      "Epoch 5870/6000  training loss: 0.11929944902658463 Validation loss 0.11944887787103653\n",
      "Epoch 5880/6000  training loss: 0.11820153146982193 Validation loss 0.11835195124149323\n",
      "Epoch 5890/6000  training loss: 0.11711681634187698 Validation loss 0.11726823449134827\n",
      "Epoch 5900/6000  training loss: 0.11604513227939606 Validation loss 0.11619752645492554\n",
      "Epoch 5910/6000  training loss: 0.11498632282018661 Validation loss 0.11513969302177429\n",
      "Epoch 5920/6000  training loss: 0.11394023895263672 Validation loss 0.1140945702791214\n",
      "Epoch 5930/6000  training loss: 0.11290674656629562 Validation loss 0.11306202411651611\n",
      "Epoch 5940/6000  training loss: 0.1118856742978096 Validation loss 0.11204186826944351\n",
      "Epoch 5950/6000  training loss: 0.11087686568498611 Validation loss 0.11103399097919464\n",
      "Epoch 5960/6000  training loss: 0.10988020151853561 Validation loss 0.11003821343183517\n",
      "Epoch 5970/6000  training loss: 0.10889548808336258 Validation loss 0.10905441641807556\n",
      "Epoch 5980/6000  training loss: 0.10792264342308044 Validation loss 0.10808244347572327\n",
      "Epoch 5990/6000  training loss: 0.10696147382259369 Validation loss 0.10712215304374695\n"
     ]
    }
   ],
   "source": [
    "loss, loss_val = model.train(X_train, Y_train, X_val, y_val, 6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Mean Square Error')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f10fc68d4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss, label=\"Taining Loss\")\n",
    "plt.plot(loss_val, label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Mean Square Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(np.float32(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score is 0.6767650953865371 and MSE 0.016636404989961568\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "print(\"R2 Score is {} and MSE {}\".format(\\\n",
    "       r2_score(y_val, Y_pred),\\\n",
    "       mean_squared_error(y_val, Y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
