{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/am/anaconda3/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtificialNeuron:\n",
    "    def __init__(self,N=2, act_func=tf.nn.sigmoid, learning_rate= 0.001):\n",
    "        self.N = N # Number of inputs to the neuron\n",
    "        self.act_fn = act_func\n",
    "        \n",
    "        # Build the graph for a single neuron\n",
    "        self.W = tf.Variable(tf.random_normal([N,1], stddev=2, seed = 0))\n",
    "        self.bias = tf.Variable(0.0, dtype=tf.float32)\n",
    "        self.X = tf.placeholder(tf.float32, name='X', shape=[None,N])\n",
    "        self.y = tf.placeholder(tf.float32, name='Y')\n",
    "        \n",
    "        activity = tf.matmul(self.X, self.W) + self.bias\n",
    "        self.y_hat = self.act_fn(activity)\n",
    "        \n",
    "        error = self.y - self.y_hat\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.square(error))\n",
    "        self.opt =  tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(self.loss)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "    def train(self, X, Y, X_val, Y_val, epochs=100):\n",
    "        epoch = 0\n",
    "        X, Y = shuffle(X,Y)\n",
    "        loss = []\n",
    "        loss_val = []\n",
    "        while epoch < epochs:\n",
    "            # Run the optimizer for the whole training set batch wise (Stochastic Gradient Descent)    \n",
    "            _, l = self.sess.run([self.opt,self.loss], feed_dict={self.X: X, self.y: Y})\n",
    "            l_val = self.sess.run(self.loss, feed_dict={self.X: X_val, self.y: Y_val})\n",
    "            \n",
    "            loss.append(l)\n",
    "            loss_val.append(l_val)\n",
    "                \n",
    "            if epoch % 10 == 0:\n",
    "                print(\"Epoch {}/{}  training loss: {} Validation loss {}\".\\\n",
    "                      format(epoch,epochs,l, l_val ))\n",
    "               \n",
    "            epoch += 1\n",
    "        return loss, loss_val\n",
    "            \n",
    "    def predict(self, X):\n",
    "        return self.sess.run(self.y_hat, feed_dict={self.X: X})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Folds5x2_pp.xlsx'\n",
    "df = pd.read_excel(filename, sheet_name='Sheet1')\n",
    "X, Y = df[['AT', 'V','AP','RH']], df['PE']\n",
    "scaler = MinMaxScaler()\n",
    "X_new = scaler.fit_transform(X)\n",
    "target_scaler = MinMaxScaler()\n",
    "Y_new = target_scaler.fit_transform(Y.values.reshape(-1,1))\n",
    "X_train, X_val, Y_train, y_val = \\\n",
    "  train_test_split(X_new, Y_new, test_size=0.4, random_state=333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, d = X_train.shape\n",
    "model = ArtificialNeuron(N=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/30000  training loss: 0.28809911012649536 Validation loss 0.2842104136943817\n",
      "Epoch 10/30000  training loss: 0.28804391622543335 Validation loss 0.28415507078170776\n",
      "Epoch 20/30000  training loss: 0.28798866271972656 Validation loss 0.2840996980667114\n",
      "Epoch 30/30000  training loss: 0.2879333794116974 Validation loss 0.2840442359447479\n",
      "Epoch 40/30000  training loss: 0.28787800669670105 Validation loss 0.28398871421813965\n",
      "Epoch 50/30000  training loss: 0.28782257437705994 Validation loss 0.28393319249153137\n",
      "Epoch 60/30000  training loss: 0.28776705265045166 Validation loss 0.28387752175331116\n",
      "Epoch 70/30000  training loss: 0.2877114713191986 Validation loss 0.2838217616081238\n",
      "Epoch 80/30000  training loss: 0.28765586018562317 Validation loss 0.2837660014629364\n",
      "Epoch 90/30000  training loss: 0.2876001000404358 Validation loss 0.2837101221084595\n",
      "Epoch 100/30000  training loss: 0.2875443696975708 Validation loss 0.28365421295166016\n",
      "Epoch 110/30000  training loss: 0.28748849034309387 Validation loss 0.2835982143878937\n",
      "Epoch 120/30000  training loss: 0.28743258118629456 Validation loss 0.2835421562194824\n",
      "Epoch 130/30000  training loss: 0.28737664222717285 Validation loss 0.2834860682487488\n",
      "Epoch 140/30000  training loss: 0.2873205840587616 Validation loss 0.2834298610687256\n",
      "Epoch 150/30000  training loss: 0.2872644364833832 Validation loss 0.2833735942840576\n",
      "Epoch 160/30000  training loss: 0.2872082591056824 Validation loss 0.2833172678947449\n",
      "Epoch 170/30000  training loss: 0.2871520221233368 Validation loss 0.28326085209846497\n",
      "Epoch 180/30000  training loss: 0.28709572553634644 Validation loss 0.28320440649986267\n",
      "Epoch 190/30000  training loss: 0.28703930974006653 Validation loss 0.2831479012966156\n",
      "Epoch 200/30000  training loss: 0.2869828939437866 Validation loss 0.283091276884079\n",
      "Epoch 210/30000  training loss: 0.2869262993335724 Validation loss 0.2830345928668976\n",
      "Epoch 220/30000  training loss: 0.28686973452568054 Validation loss 0.2829778492450714\n",
      "Epoch 230/30000  training loss: 0.28681308031082153 Validation loss 0.28292104601860046\n",
      "Epoch 240/30000  training loss: 0.2867562174797058 Validation loss 0.2828640937805176\n",
      "Epoch 250/30000  training loss: 0.2866993248462677 Validation loss 0.28280699253082275\n",
      "Epoch 260/30000  training loss: 0.28664228320121765 Validation loss 0.28274986147880554\n",
      "Epoch 270/30000  training loss: 0.2865852415561676 Validation loss 0.28269264101982117\n",
      "Epoch 280/30000  training loss: 0.286528080701828 Validation loss 0.2826353907585144\n",
      "Epoch 290/30000  training loss: 0.2864709198474884 Validation loss 0.2825780212879181\n",
      "Epoch 300/30000  training loss: 0.28641363978385925 Validation loss 0.2825206518173218\n",
      "Epoch 310/30000  training loss: 0.2863563001155853 Validation loss 0.2824631631374359\n",
      "Epoch 320/30000  training loss: 0.28629887104034424 Validation loss 0.2824055850505829\n",
      "Epoch 330/30000  training loss: 0.28624141216278076 Validation loss 0.28234797716140747\n",
      "Epoch 340/30000  training loss: 0.28618383407592773 Validation loss 0.2822903096675873\n",
      "Epoch 350/30000  training loss: 0.28612619638442993 Validation loss 0.28223249316215515\n",
      "Epoch 360/30000  training loss: 0.28606855869293213 Validation loss 0.28217464685440063\n",
      "Epoch 370/30000  training loss: 0.2860107719898224 Validation loss 0.2821168005466461\n",
      "Epoch 380/30000  training loss: 0.28595292568206787 Validation loss 0.28205886483192444\n",
      "Epoch 390/30000  training loss: 0.2858950197696686 Validation loss 0.2820007801055908\n",
      "Epoch 400/30000  training loss: 0.2858370244503021 Validation loss 0.2819426655769348\n",
      "Epoch 410/30000  training loss: 0.28577902913093567 Validation loss 0.28188449144363403\n",
      "Epoch 420/30000  training loss: 0.2857208847999573 Validation loss 0.2818261981010437\n",
      "Epoch 430/30000  training loss: 0.2856627106666565 Validation loss 0.28176790475845337\n",
      "Epoch 440/30000  training loss: 0.28560447692871094 Validation loss 0.28170955181121826\n",
      "Epoch 450/30000  training loss: 0.2855461537837982 Validation loss 0.28165102005004883\n",
      "Epoch 460/30000  training loss: 0.28548768162727356 Validation loss 0.2815924882888794\n",
      "Epoch 470/30000  training loss: 0.28542909026145935 Validation loss 0.28153374791145325\n",
      "Epoch 480/30000  training loss: 0.285370409488678 Validation loss 0.28147491812705994\n",
      "Epoch 490/30000  training loss: 0.2853116989135742 Validation loss 0.28141605854034424\n",
      "Epoch 500/30000  training loss: 0.2852528989315033 Validation loss 0.2813571095466614\n",
      "Epoch 510/30000  training loss: 0.2851940095424652 Validation loss 0.28129813075065613\n",
      "Epoch 520/30000  training loss: 0.28513503074645996 Validation loss 0.28123900294303894\n",
      "Epoch 530/30000  training loss: 0.28507599234580994 Validation loss 0.28117984533309937\n",
      "Epoch 540/30000  training loss: 0.2850169241428375 Validation loss 0.2811205983161926\n",
      "Epoch 550/30000  training loss: 0.28495773673057556 Validation loss 0.2810612916946411\n",
      "Epoch 560/30000  training loss: 0.2848984897136688 Validation loss 0.28100189566612244\n",
      "Epoch 570/30000  training loss: 0.2848392426967621 Validation loss 0.28094249963760376\n",
      "Epoch 580/30000  training loss: 0.28477978706359863 Validation loss 0.28088295459747314\n",
      "Epoch 590/30000  training loss: 0.2847203314304352 Validation loss 0.28082337975502014\n",
      "Epoch 600/30000  training loss: 0.28466081619262695 Validation loss 0.2807637155056\n",
      "Epoch 610/30000  training loss: 0.2846011817455292 Validation loss 0.28070399165153503\n",
      "Epoch 620/30000  training loss: 0.284541517496109 Validation loss 0.28064417839050293\n",
      "Epoch 630/30000  training loss: 0.28448179364204407 Validation loss 0.28058427572250366\n",
      "Epoch 640/30000  training loss: 0.2844219505786896 Validation loss 0.280524343252182\n",
      "Epoch 650/30000  training loss: 0.2843620777130127 Validation loss 0.2804642915725708\n",
      "Epoch 660/30000  training loss: 0.28430208563804626 Validation loss 0.2804041802883148\n",
      "Epoch 670/30000  training loss: 0.28424206376075745 Validation loss 0.28034403920173645\n",
      "Epoch 680/30000  training loss: 0.2841818928718567 Validation loss 0.28028377890586853\n",
      "Epoch 690/30000  training loss: 0.2841216027736664 Validation loss 0.2802233397960663\n",
      "Epoch 700/30000  training loss: 0.2840612232685089 Validation loss 0.2801627814769745\n",
      "Epoch 710/30000  training loss: 0.2840007543563843 Validation loss 0.2801021933555603\n",
      "Epoch 720/30000  training loss: 0.2839401960372925 Validation loss 0.28004151582717896\n",
      "Epoch 730/30000  training loss: 0.2838795781135559 Validation loss 0.2799808084964752\n",
      "Epoch 740/30000  training loss: 0.28381893038749695 Validation loss 0.27991998195648193\n",
      "Epoch 750/30000  training loss: 0.28375813364982605 Validation loss 0.27985909581184387\n",
      "Epoch 760/30000  training loss: 0.28369730710983276 Validation loss 0.2797981798648834\n",
      "Epoch 770/30000  training loss: 0.2836364209651947 Validation loss 0.27973708510398865\n",
      "Epoch 780/30000  training loss: 0.2835754156112671 Validation loss 0.2796759605407715\n",
      "Epoch 790/30000  training loss: 0.2835143506526947 Validation loss 0.27961480617523193\n",
      "Epoch 800/30000  training loss: 0.2834532558917999 Validation loss 0.2795535624027252\n",
      "Epoch 810/30000  training loss: 0.283392071723938 Validation loss 0.27949222922325134\n",
      "Epoch 820/30000  training loss: 0.2833307683467865 Validation loss 0.2794308066368103\n",
      "Epoch 830/30000  training loss: 0.28326940536499023 Validation loss 0.2793693542480469\n",
      "Epoch 840/30000  training loss: 0.2832079529762268 Validation loss 0.2793077826499939\n",
      "Epoch 850/30000  training loss: 0.283146470785141 Validation loss 0.27924615144729614\n",
      "Epoch 860/30000  training loss: 0.2830848693847656 Validation loss 0.2791844606399536\n",
      "Epoch 870/30000  training loss: 0.28302323818206787 Validation loss 0.2791227102279663\n",
      "Epoch 880/30000  training loss: 0.28296151757240295 Validation loss 0.27906084060668945\n",
      "Epoch 890/30000  training loss: 0.2828997075557709 Validation loss 0.2789989113807678\n",
      "Epoch 900/30000  training loss: 0.28283771872520447 Validation loss 0.2789367735385895\n",
      "Epoch 910/30000  training loss: 0.2827756404876709 Validation loss 0.27887454628944397\n",
      "Epoch 920/30000  training loss: 0.28271347284317017 Validation loss 0.2788122892379761\n",
      "Epoch 930/30000  training loss: 0.28265124559402466 Validation loss 0.278749942779541\n",
      "Epoch 940/30000  training loss: 0.2825889587402344 Validation loss 0.2786875069141388\n",
      "Epoch 950/30000  training loss: 0.28252655267715454 Validation loss 0.2786250114440918\n",
      "Epoch 960/30000  training loss: 0.28246405720710754 Validation loss 0.2785623371601105\n",
      "Epoch 970/30000  training loss: 0.28240153193473816 Validation loss 0.2784997224807739\n",
      "Epoch 980/30000  training loss: 0.2823389172554016 Validation loss 0.2784369885921478\n",
      "Epoch 990/30000  training loss: 0.2822762131690979 Validation loss 0.27837419509887695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/30000  training loss: 0.2822134494781494 Validation loss 0.2783113121986389\n",
      "Epoch 1010/30000  training loss: 0.28215059638023376 Validation loss 0.27824831008911133\n",
      "Epoch 1020/30000  training loss: 0.28208768367767334 Validation loss 0.27818530797958374\n",
      "Epoch 1030/30000  training loss: 0.28202465176582336 Validation loss 0.2781221568584442\n",
      "Epoch 1040/30000  training loss: 0.281961590051651 Validation loss 0.2780589461326599\n",
      "Epoch 1050/30000  training loss: 0.2818984091281891 Validation loss 0.27799567580223083\n",
      "Epoch 1060/30000  training loss: 0.2818351984024048 Validation loss 0.277932345867157\n",
      "Epoch 1070/30000  training loss: 0.28177186846733093 Validation loss 0.27786892652511597\n",
      "Epoch 1080/30000  training loss: 0.2817084789276123 Validation loss 0.2778054177761078\n",
      "Epoch 1090/30000  training loss: 0.2816450297832489 Validation loss 0.27774184942245483\n",
      "Epoch 1100/30000  training loss: 0.28158146142959595 Validation loss 0.27767816185951233\n",
      "Epoch 1110/30000  training loss: 0.28151780366897583 Validation loss 0.2776143550872803\n",
      "Epoch 1120/30000  training loss: 0.2814539670944214 Validation loss 0.2775503993034363\n",
      "Epoch 1130/30000  training loss: 0.28139007091522217 Validation loss 0.2774864137172699\n",
      "Epoch 1140/30000  training loss: 0.2813260853290558 Validation loss 0.2774222791194916\n",
      "Epoch 1150/30000  training loss: 0.28126198053359985 Validation loss 0.2773580551147461\n",
      "Epoch 1160/30000  training loss: 0.28119784593582153 Validation loss 0.2772938311100006\n",
      "Epoch 1170/30000  training loss: 0.28113362193107605 Validation loss 0.27722951769828796\n",
      "Epoch 1180/30000  training loss: 0.2810693383216858 Validation loss 0.27716511487960815\n",
      "Epoch 1190/30000  training loss: 0.281004935503006 Validation loss 0.2771006226539612\n",
      "Epoch 1200/30000  training loss: 0.2809404730796814 Validation loss 0.27703604102134705\n",
      "Epoch 1210/30000  training loss: 0.28087595105171204 Validation loss 0.27697136998176575\n",
      "Epoch 1220/30000  training loss: 0.2808113694190979 Validation loss 0.27690666913986206\n",
      "Epoch 1230/30000  training loss: 0.2807466685771942 Validation loss 0.2768418490886688\n",
      "Epoch 1240/30000  training loss: 0.28068190813064575 Validation loss 0.2767769992351532\n",
      "Epoch 1250/30000  training loss: 0.28061702847480774 Validation loss 0.276712030172348\n",
      "Epoch 1260/30000  training loss: 0.28055208921432495 Validation loss 0.2766469717025757\n",
      "Epoch 1270/30000  training loss: 0.2804870903491974 Validation loss 0.2765818238258362\n",
      "Epoch 1280/30000  training loss: 0.2804219722747803 Validation loss 0.2765166461467743\n",
      "Epoch 1290/30000  training loss: 0.28035685420036316 Validation loss 0.2764514088630676\n",
      "Epoch 1300/30000  training loss: 0.2802915871143341 Validation loss 0.2763860523700714\n",
      "Epoch 1310/30000  training loss: 0.2802262604236603 Validation loss 0.2763206362724304\n",
      "Epoch 1320/30000  training loss: 0.28016069531440735 Validation loss 0.27625495195388794\n",
      "Epoch 1330/30000  training loss: 0.28009510040283203 Validation loss 0.2761892080307007\n",
      "Epoch 1340/30000  training loss: 0.28002938628196716 Validation loss 0.27612343430519104\n",
      "Epoch 1350/30000  training loss: 0.2799636125564575 Validation loss 0.27605751156806946\n",
      "Epoch 1360/30000  training loss: 0.2798977494239807 Validation loss 0.2759915888309479\n",
      "Epoch 1370/30000  training loss: 0.27983179688453674 Validation loss 0.27592551708221436\n",
      "Epoch 1380/30000  training loss: 0.2797657549381256 Validation loss 0.27585938572883606\n",
      "Epoch 1390/30000  training loss: 0.2796996533870697 Validation loss 0.275793194770813\n",
      "Epoch 1400/30000  training loss: 0.27963346242904663 Validation loss 0.275726854801178\n",
      "Epoch 1410/30000  training loss: 0.2795672118663788 Validation loss 0.2756604850292206\n",
      "Epoch 1420/30000  training loss: 0.2795008718967438 Validation loss 0.2755940854549408\n",
      "Epoch 1430/30000  training loss: 0.279434472322464 Validation loss 0.27552759647369385\n",
      "Epoch 1440/30000  training loss: 0.27936795353889465 Validation loss 0.2754609286785126\n",
      "Epoch 1450/30000  training loss: 0.27930134534835815 Validation loss 0.2753942310810089\n",
      "Epoch 1460/30000  training loss: 0.2792346477508545 Validation loss 0.2753274738788605\n",
      "Epoch 1470/30000  training loss: 0.27916792035102844 Validation loss 0.27526065707206726\n",
      "Epoch 1480/30000  training loss: 0.27910110354423523 Validation loss 0.2751937210559845\n",
      "Epoch 1490/30000  training loss: 0.27903419733047485 Validation loss 0.27512672543525696\n",
      "Epoch 1500/30000  training loss: 0.2789671719074249 Validation loss 0.27505961060523987\n",
      "Epoch 1510/30000  training loss: 0.2789001166820526 Validation loss 0.2749924063682556\n",
      "Epoch 1520/30000  training loss: 0.2788328230381012 Validation loss 0.27492502331733704\n",
      "Epoch 1530/30000  training loss: 0.2787654399871826 Validation loss 0.2748575508594513\n",
      "Epoch 1540/30000  training loss: 0.27869799733161926 Validation loss 0.2747899889945984\n",
      "Epoch 1550/30000  training loss: 0.27863043546676636 Validation loss 0.2747223675251007\n",
      "Epoch 1560/30000  training loss: 0.2785627841949463 Validation loss 0.2746546268463135\n",
      "Epoch 1570/30000  training loss: 0.27849510312080383 Validation loss 0.27458682656288147\n",
      "Epoch 1580/30000  training loss: 0.2784273028373718 Validation loss 0.2745189070701599\n",
      "Epoch 1590/30000  training loss: 0.27835941314697266 Validation loss 0.27445095777511597\n",
      "Epoch 1600/30000  training loss: 0.2782914936542511 Validation loss 0.27438294887542725\n",
      "Epoch 1610/30000  training loss: 0.2782234251499176 Validation loss 0.274314820766449\n",
      "Epoch 1620/30000  training loss: 0.27815529704093933 Validation loss 0.27424657344818115\n",
      "Epoch 1630/30000  training loss: 0.2780871093273163 Validation loss 0.27417826652526855\n",
      "Epoch 1640/30000  training loss: 0.2780188024044037 Validation loss 0.2741098999977112\n",
      "Epoch 1650/30000  training loss: 0.2779504358768463 Validation loss 0.27404144406318665\n",
      "Epoch 1660/30000  training loss: 0.27788200974464417 Validation loss 0.27397289872169495\n",
      "Epoch 1670/30000  training loss: 0.2778134346008301 Validation loss 0.2739042639732361\n",
      "Epoch 1680/30000  training loss: 0.2777448296546936 Validation loss 0.27383556962013245\n",
      "Epoch 1690/30000  training loss: 0.27767613530158997 Validation loss 0.27376675605773926\n",
      "Epoch 1700/30000  training loss: 0.2776073217391968 Validation loss 0.2736979126930237\n",
      "Epoch 1710/30000  training loss: 0.2775384485721588 Validation loss 0.2736288905143738\n",
      "Epoch 1720/30000  training loss: 0.27746936678886414 Validation loss 0.2735597491264343\n",
      "Epoch 1730/30000  training loss: 0.2774001359939575 Validation loss 0.27349042892456055\n",
      "Epoch 1740/30000  training loss: 0.2773309051990509 Validation loss 0.27342110872268677\n",
      "Epoch 1750/30000  training loss: 0.27726152539253235 Validation loss 0.27335166931152344\n",
      "Epoch 1760/30000  training loss: 0.2771921157836914 Validation loss 0.27328214049339294\n",
      "Epoch 1770/30000  training loss: 0.2771225869655609 Validation loss 0.2732125520706177\n",
      "Epoch 1780/30000  training loss: 0.27705302834510803 Validation loss 0.27314287424087524\n",
      "Epoch 1790/30000  training loss: 0.2769833207130432 Validation loss 0.27307310700416565\n",
      "Epoch 1800/30000  training loss: 0.27691352367401123 Validation loss 0.2730032503604889\n",
      "Epoch 1810/30000  training loss: 0.27684369683265686 Validation loss 0.27293330430984497\n",
      "Epoch 1820/30000  training loss: 0.27677372097969055 Validation loss 0.2728632688522339\n",
      "Epoch 1830/30000  training loss: 0.27670374512672424 Validation loss 0.272793173789978\n",
      "Epoch 1840/30000  training loss: 0.276633620262146 Validation loss 0.272722989320755\n",
      "Epoch 1850/30000  training loss: 0.276563435792923 Validation loss 0.2726527154445648\n",
      "Epoch 1860/30000  training loss: 0.2764931321144104 Validation loss 0.27258235216140747\n",
      "Epoch 1870/30000  training loss: 0.27642276883125305 Validation loss 0.27251189947128296\n",
      "Epoch 1880/30000  training loss: 0.27635231614112854 Validation loss 0.27244138717651367\n",
      "Epoch 1890/30000  training loss: 0.27628177404403687 Validation loss 0.2723707854747772\n",
      "Epoch 1900/30000  training loss: 0.2762112021446228 Validation loss 0.2723000943660736\n",
      "Epoch 1910/30000  training loss: 0.27614033222198486 Validation loss 0.2722291648387909\n",
      "Epoch 1920/30000  training loss: 0.27606940269470215 Validation loss 0.272158145904541\n",
      "Epoch 1930/30000  training loss: 0.2759983539581299 Validation loss 0.2720870077610016\n",
      "Epoch 1940/30000  training loss: 0.27592727541923523 Validation loss 0.27201586961746216\n",
      "Epoch 1950/30000  training loss: 0.275856077671051 Validation loss 0.2719445824623108\n",
      "Epoch 1960/30000  training loss: 0.27578479051589966 Validation loss 0.27187326550483704\n",
      "Epoch 1970/30000  training loss: 0.2757134437561035 Validation loss 0.27180179953575134\n",
      "Epoch 1980/30000  training loss: 0.27564194798469543 Validation loss 0.2717302739620209\n",
      "Epoch 1990/30000  training loss: 0.27557042241096497 Validation loss 0.27165865898132324\n",
      "Epoch 2000/30000  training loss: 0.27549877762794495 Validation loss 0.27158695459365845\n",
      "Epoch 2010/30000  training loss: 0.27542707324028015 Validation loss 0.2715151906013489\n",
      "Epoch 2020/30000  training loss: 0.2753552794456482 Validation loss 0.27144333720207214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2030/30000  training loss: 0.2752833664417267 Validation loss 0.27137133479118347\n",
      "Epoch 2040/30000  training loss: 0.2752113938331604 Validation loss 0.2712993025779724\n",
      "Epoch 2050/30000  training loss: 0.27513933181762695 Validation loss 0.2712271511554718\n",
      "Epoch 2060/30000  training loss: 0.27506718039512634 Validation loss 0.2711549401283264\n",
      "Epoch 2070/30000  training loss: 0.27499493956565857 Validation loss 0.27108266949653625\n",
      "Epoch 2080/30000  training loss: 0.274922639131546 Validation loss 0.27101027965545654\n",
      "Epoch 2090/30000  training loss: 0.27485018968582153 Validation loss 0.2709377706050873\n",
      "Epoch 2100/30000  training loss: 0.2747775614261627 Validation loss 0.2708650529384613\n",
      "Epoch 2110/30000  training loss: 0.27470487356185913 Validation loss 0.27079224586486816\n",
      "Epoch 2120/30000  training loss: 0.2746320068836212 Validation loss 0.27071940898895264\n",
      "Epoch 2130/30000  training loss: 0.2745591104030609 Validation loss 0.27064642310142517\n",
      "Epoch 2140/30000  training loss: 0.27448612451553345 Validation loss 0.27057334780693054\n",
      "Epoch 2150/30000  training loss: 0.27441298961639404 Validation loss 0.27050021290779114\n",
      "Epoch 2160/30000  training loss: 0.27433985471725464 Validation loss 0.27042698860168457\n",
      "Epoch 2170/30000  training loss: 0.2742665708065033 Validation loss 0.2703535854816437\n",
      "Epoch 2180/30000  training loss: 0.2741932272911072 Validation loss 0.27028024196624756\n",
      "Epoch 2190/30000  training loss: 0.2741197943687439 Validation loss 0.2702067494392395\n",
      "Epoch 2200/30000  training loss: 0.27404624223709106 Validation loss 0.27013319730758667\n",
      "Epoch 2210/30000  training loss: 0.27397263050079346 Validation loss 0.2700594663619995\n",
      "Epoch 2220/30000  training loss: 0.2738989293575287 Validation loss 0.26998570561408997\n",
      "Epoch 2230/30000  training loss: 0.27382513880729675 Validation loss 0.26991185545921326\n",
      "Epoch 2240/30000  training loss: 0.27375125885009766 Validation loss 0.2698379158973694\n",
      "Epoch 2250/30000  training loss: 0.2736772894859314 Validation loss 0.26976391673088074\n",
      "Epoch 2260/30000  training loss: 0.2736032009124756 Validation loss 0.26968979835510254\n",
      "Epoch 2270/30000  training loss: 0.2735290229320526 Validation loss 0.2696155607700348\n",
      "Epoch 2280/30000  training loss: 0.2734547555446625 Validation loss 0.2695412039756775\n",
      "Epoch 2290/30000  training loss: 0.2733802795410156 Validation loss 0.26946666836738586\n",
      "Epoch 2300/30000  training loss: 0.2733057141304016 Validation loss 0.2693920433521271\n",
      "Epoch 2310/30000  training loss: 0.27323105931282043 Validation loss 0.2693173587322235\n",
      "Epoch 2320/30000  training loss: 0.2731563150882721 Validation loss 0.2692425549030304\n",
      "Epoch 2330/30000  training loss: 0.2730814516544342 Validation loss 0.2691676914691925\n",
      "Epoch 2340/30000  training loss: 0.27300649881362915 Validation loss 0.2690926790237427\n",
      "Epoch 2350/30000  training loss: 0.2729314863681793 Validation loss 0.2690175771713257\n",
      "Epoch 2360/30000  training loss: 0.2728564143180847 Validation loss 0.2689424753189087\n",
      "Epoch 2370/30000  training loss: 0.27278122305870056 Validation loss 0.26886725425720215\n",
      "Epoch 2380/30000  training loss: 0.27270591259002686 Validation loss 0.26879188418388367\n",
      "Epoch 2390/30000  training loss: 0.272630512714386 Validation loss 0.2687164545059204\n",
      "Epoch 2400/30000  training loss: 0.27255502343177795 Validation loss 0.2686409056186676\n",
      "Epoch 2410/30000  training loss: 0.27247947454452515 Validation loss 0.2685653269290924\n",
      "Epoch 2420/30000  training loss: 0.2724038064479828 Validation loss 0.26848962903022766\n",
      "Epoch 2430/30000  training loss: 0.27232807874679565 Validation loss 0.26841387152671814\n",
      "Epoch 2440/30000  training loss: 0.27225223183631897 Validation loss 0.2683379650115967\n",
      "Epoch 2450/30000  training loss: 0.2721762955188751 Validation loss 0.26826196908950806\n",
      "Epoch 2460/30000  training loss: 0.2721002995967865 Validation loss 0.26818591356277466\n",
      "Epoch 2470/30000  training loss: 0.2720240354537964 Validation loss 0.26810961961746216\n",
      "Epoch 2480/30000  training loss: 0.2719476819038391 Validation loss 0.2680332362651825\n",
      "Epoch 2490/30000  training loss: 0.27187126874923706 Validation loss 0.2679567337036133\n",
      "Epoch 2500/30000  training loss: 0.27179470658302307 Validation loss 0.2678801715373993\n",
      "Epoch 2510/30000  training loss: 0.2717180550098419 Validation loss 0.26780351996421814\n",
      "Epoch 2520/30000  training loss: 0.271641343832016 Validation loss 0.26772671937942505\n",
      "Epoch 2530/30000  training loss: 0.2715645134449005 Validation loss 0.26764991879463196\n",
      "Epoch 2540/30000  training loss: 0.27148762345314026 Validation loss 0.2675729990005493\n",
      "Epoch 2550/30000  training loss: 0.27141061425209045 Validation loss 0.2674959599971771\n",
      "Epoch 2560/30000  training loss: 0.2713335156440735 Validation loss 0.26741883158683777\n",
      "Epoch 2570/30000  training loss: 0.27125632762908936 Validation loss 0.26734161376953125\n",
      "Epoch 2580/30000  training loss: 0.2711790204048157 Validation loss 0.26726433634757996\n",
      "Epoch 2590/30000  training loss: 0.271101713180542 Validation loss 0.2671869099140167\n",
      "Epoch 2600/30000  training loss: 0.27102425694465637 Validation loss 0.2671094834804535\n",
      "Epoch 2610/30000  training loss: 0.2709467113018036 Validation loss 0.26703184843063354\n",
      "Epoch 2620/30000  training loss: 0.27086904644966125 Validation loss 0.2669542133808136\n",
      "Epoch 2630/30000  training loss: 0.27079129219055176 Validation loss 0.2668763995170593\n",
      "Epoch 2640/30000  training loss: 0.2707134485244751 Validation loss 0.2667985260486603\n",
      "Epoch 2650/30000  training loss: 0.2706353962421417 Validation loss 0.26672041416168213\n",
      "Epoch 2660/30000  training loss: 0.2705572247505188 Validation loss 0.2666422426700592\n",
      "Epoch 2670/30000  training loss: 0.2704789340496063 Validation loss 0.2665639817714691\n",
      "Epoch 2680/30000  training loss: 0.2704005837440491 Validation loss 0.2664855718612671\n",
      "Epoch 2690/30000  training loss: 0.27032214403152466 Validation loss 0.2664071321487427\n",
      "Epoch 2700/30000  training loss: 0.2702435851097107 Validation loss 0.2663285434246063\n",
      "Epoch 2710/30000  training loss: 0.27016496658325195 Validation loss 0.2662498950958252\n",
      "Epoch 2720/30000  training loss: 0.27008625864982605 Validation loss 0.2661711573600769\n",
      "Epoch 2730/30000  training loss: 0.2700074017047882 Validation loss 0.26609230041503906\n",
      "Epoch 2740/30000  training loss: 0.2699284553527832 Validation loss 0.26601335406303406\n",
      "Epoch 2750/30000  training loss: 0.2698494493961334 Validation loss 0.2659343481063843\n",
      "Epoch 2760/30000  training loss: 0.26977038383483887 Validation loss 0.26585519313812256\n",
      "Epoch 2770/30000  training loss: 0.2696911692619324 Validation loss 0.26577603816986084\n",
      "Epoch 2780/30000  training loss: 0.2696118652820587 Validation loss 0.2656967043876648\n",
      "Epoch 2790/30000  training loss: 0.2695324718952179 Validation loss 0.2656172811985016\n",
      "Epoch 2800/30000  training loss: 0.2694529891014099 Validation loss 0.265537828207016\n",
      "Epoch 2810/30000  training loss: 0.26937341690063477 Validation loss 0.26545819640159607\n",
      "Epoch 2820/30000  training loss: 0.2692936360836029 Validation loss 0.2653784453868866\n",
      "Epoch 2830/30000  training loss: 0.2692137360572815 Validation loss 0.2652985155582428\n",
      "Epoch 2840/30000  training loss: 0.2691337466239929 Validation loss 0.26521849632263184\n",
      "Epoch 2850/30000  training loss: 0.26905357837677 Validation loss 0.2651383578777313\n",
      "Epoch 2860/30000  training loss: 0.26897335052490234 Validation loss 0.26505813002586365\n",
      "Epoch 2870/30000  training loss: 0.2688930928707123 Validation loss 0.2649778127670288\n",
      "Epoch 2880/30000  training loss: 0.2688126564025879 Validation loss 0.2648974359035492\n",
      "Epoch 2890/30000  training loss: 0.2687321901321411 Validation loss 0.26481693983078003\n",
      "Epoch 2900/30000  training loss: 0.2686515748500824 Validation loss 0.2647363543510437\n",
      "Epoch 2910/30000  training loss: 0.2685708999633789 Validation loss 0.2646556794643402\n",
      "Epoch 2920/30000  training loss: 0.26849013566970825 Validation loss 0.2645748555660248\n",
      "Epoch 2930/30000  training loss: 0.26840925216674805 Validation loss 0.26449400186538696\n",
      "Epoch 2940/30000  training loss: 0.2683282792568207 Validation loss 0.2644130289554596\n",
      "Epoch 2950/30000  training loss: 0.26824718713760376 Validation loss 0.26433196663856506\n",
      "Epoch 2960/30000  training loss: 0.26816603541374207 Validation loss 0.26425081491470337\n",
      "Epoch 2970/30000  training loss: 0.2680847942829132 Validation loss 0.2641695737838745\n",
      "Epoch 2980/30000  training loss: 0.26800337433815 Validation loss 0.2640882134437561\n",
      "Epoch 2990/30000  training loss: 0.26792192459106445 Validation loss 0.26400670409202576\n",
      "Epoch 3000/30000  training loss: 0.2678402066230774 Validation loss 0.2639250159263611\n",
      "Epoch 3010/30000  training loss: 0.2677583694458008 Validation loss 0.2638431787490845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3020/30000  training loss: 0.2676764726638794 Validation loss 0.2637613117694855\n",
      "Epoch 3030/30000  training loss: 0.26759448647499084 Validation loss 0.26367929577827454\n",
      "Epoch 3040/30000  training loss: 0.26751241087913513 Validation loss 0.2635972499847412\n",
      "Epoch 3050/30000  training loss: 0.2674301862716675 Validation loss 0.26351505517959595\n",
      "Epoch 3060/30000  training loss: 0.26734793186187744 Validation loss 0.2634327709674835\n",
      "Epoch 3070/30000  training loss: 0.26726552844047546 Validation loss 0.2633504271507263\n",
      "Epoch 3080/30000  training loss: 0.2671830654144287 Validation loss 0.26326796412467957\n",
      "Epoch 3090/30000  training loss: 0.26710045337677 Validation loss 0.26318538188934326\n",
      "Epoch 3100/30000  training loss: 0.26701778173446655 Validation loss 0.2631027400493622\n",
      "Epoch 3110/30000  training loss: 0.26693499088287354 Validation loss 0.26301997900009155\n",
      "Epoch 3120/30000  training loss: 0.26685214042663574 Validation loss 0.26293712854385376\n",
      "Epoch 3130/30000  training loss: 0.2667692005634308 Validation loss 0.2628541886806488\n",
      "Epoch 3140/30000  training loss: 0.2666860818862915 Validation loss 0.2627711296081543\n",
      "Epoch 3150/30000  training loss: 0.26660293340682983 Validation loss 0.2626879811286926\n",
      "Epoch 3160/30000  training loss: 0.26651960611343384 Validation loss 0.262604683637619\n",
      "Epoch 3170/30000  training loss: 0.2664361000061035 Validation loss 0.2625212073326111\n",
      "Epoch 3180/30000  training loss: 0.26635247468948364 Validation loss 0.2624376118183136\n",
      "Epoch 3190/30000  training loss: 0.266268789768219 Validation loss 0.26235392689704895\n",
      "Epoch 3200/30000  training loss: 0.2661849558353424 Validation loss 0.26227015256881714\n",
      "Epoch 3210/30000  training loss: 0.26610103249549866 Validation loss 0.2621862292289734\n",
      "Epoch 3220/30000  training loss: 0.26601701974868774 Validation loss 0.26210224628448486\n",
      "Epoch 3230/30000  training loss: 0.26593291759490967 Validation loss 0.26201820373535156\n",
      "Epoch 3240/30000  training loss: 0.26584869623184204 Validation loss 0.2619340121746063\n",
      "Epoch 3250/30000  training loss: 0.26576438546180725 Validation loss 0.2618497610092163\n",
      "Epoch 3260/30000  training loss: 0.2656799852848053 Validation loss 0.26176536083221436\n",
      "Epoch 3270/30000  training loss: 0.2655954957008362 Validation loss 0.26168087124824524\n",
      "Epoch 3280/30000  training loss: 0.2655108869075775 Validation loss 0.26159632205963135\n",
      "Epoch 3290/30000  training loss: 0.2654261589050293 Validation loss 0.2615116536617279\n",
      "Epoch 3300/30000  training loss: 0.2653413712978363 Validation loss 0.2614269256591797\n",
      "Epoch 3310/30000  training loss: 0.26525646448135376 Validation loss 0.26134204864501953\n",
      "Epoch 3320/30000  training loss: 0.26517146825790405 Validation loss 0.2612571120262146\n",
      "Epoch 3330/30000  training loss: 0.26508626341819763 Validation loss 0.26117193698883057\n",
      "Epoch 3340/30000  training loss: 0.26500093936920166 Validation loss 0.261086642742157\n",
      "Epoch 3350/30000  training loss: 0.26491546630859375 Validation loss 0.26100122928619385\n",
      "Epoch 3360/30000  training loss: 0.26482993364334106 Validation loss 0.26091569662094116\n",
      "Epoch 3370/30000  training loss: 0.26474425196647644 Validation loss 0.2608300745487213\n",
      "Epoch 3380/30000  training loss: 0.26465851068496704 Validation loss 0.2607443928718567\n",
      "Epoch 3390/30000  training loss: 0.2645726501941681 Validation loss 0.2606585621833801\n",
      "Epoch 3400/30000  training loss: 0.2644866704940796 Validation loss 0.2605726718902588\n",
      "Epoch 3410/30000  training loss: 0.2644006013870239 Validation loss 0.2604866623878479\n",
      "Epoch 3420/30000  training loss: 0.2643144726753235 Validation loss 0.26040053367614746\n",
      "Epoch 3430/30000  training loss: 0.2642281949520111 Validation loss 0.26031437516212463\n",
      "Epoch 3440/30000  training loss: 0.26414182782173157 Validation loss 0.2602280378341675\n",
      "Epoch 3450/30000  training loss: 0.26405537128448486 Validation loss 0.26014167070388794\n",
      "Epoch 3460/30000  training loss: 0.263968825340271 Validation loss 0.26005515456199646\n",
      "Epoch 3470/30000  training loss: 0.2638821601867676 Validation loss 0.25996851921081543\n",
      "Epoch 3480/30000  training loss: 0.2637953460216522 Validation loss 0.259881854057312\n",
      "Epoch 3490/30000  training loss: 0.26370853185653687 Validation loss 0.25979506969451904\n",
      "Epoch 3500/30000  training loss: 0.26362138986587524 Validation loss 0.2597079575061798\n",
      "Epoch 3510/30000  training loss: 0.26353418827056885 Validation loss 0.2596208155155182\n",
      "Epoch 3520/30000  training loss: 0.2634468674659729 Validation loss 0.25953352451324463\n",
      "Epoch 3530/30000  training loss: 0.2633594274520874 Validation loss 0.2594461739063263\n",
      "Epoch 3540/30000  training loss: 0.26327189803123474 Validation loss 0.2593587040901184\n",
      "Epoch 3550/30000  training loss: 0.2631842792034149 Validation loss 0.25927117466926575\n",
      "Epoch 3560/30000  training loss: 0.26309654116630554 Validation loss 0.25918349623680115\n",
      "Epoch 3570/30000  training loss: 0.2630086839199066 Validation loss 0.2590957581996918\n",
      "Epoch 3580/30000  training loss: 0.2629207670688629 Validation loss 0.25900787115097046\n",
      "Epoch 3590/30000  training loss: 0.26283273100852966 Validation loss 0.25891992449760437\n",
      "Epoch 3600/30000  training loss: 0.26274457573890686 Validation loss 0.25883182883262634\n",
      "Epoch 3610/30000  training loss: 0.2626563608646393 Validation loss 0.2587437033653259\n",
      "Epoch 3620/30000  training loss: 0.26256799697875977 Validation loss 0.25865545868873596\n",
      "Epoch 3630/30000  training loss: 0.2624795436859131 Validation loss 0.25856709480285645\n",
      "Epoch 3640/30000  training loss: 0.26239103078842163 Validation loss 0.2584786117076874\n",
      "Epoch 3650/30000  training loss: 0.26230236887931824 Validation loss 0.25839006900787354\n",
      "Epoch 3660/30000  training loss: 0.26221349835395813 Validation loss 0.2583012282848358\n",
      "Epoch 3670/30000  training loss: 0.2621244788169861 Validation loss 0.25821229815483093\n",
      "Epoch 3680/30000  training loss: 0.2620353698730469 Validation loss 0.2581233084201813\n",
      "Epoch 3690/30000  training loss: 0.2619461715221405 Validation loss 0.2580341696739197\n",
      "Epoch 3700/30000  training loss: 0.2618568241596222 Validation loss 0.2579449415206909\n",
      "Epoch 3710/30000  training loss: 0.2617674171924591 Validation loss 0.2578555941581726\n",
      "Epoch 3720/30000  training loss: 0.2616778612136841 Validation loss 0.25776615738868713\n",
      "Epoch 3730/30000  training loss: 0.2615882456302643 Validation loss 0.2576766014099121\n",
      "Epoch 3740/30000  training loss: 0.26149851083755493 Validation loss 0.2575869858264923\n",
      "Epoch 3750/30000  training loss: 0.2614086866378784 Validation loss 0.25749722123146057\n",
      "Epoch 3760/30000  training loss: 0.26131877303123474 Validation loss 0.25740739703178406\n",
      "Epoch 3770/30000  training loss: 0.26122868061065674 Validation loss 0.2573174238204956\n",
      "Epoch 3780/30000  training loss: 0.26113858819007874 Validation loss 0.2572273910045624\n",
      "Epoch 3790/30000  training loss: 0.261048287153244 Validation loss 0.257137268781662\n",
      "Epoch 3800/30000  training loss: 0.2609579861164093 Validation loss 0.25704702734947205\n",
      "Epoch 3810/30000  training loss: 0.26086750626564026 Validation loss 0.25695669651031494\n",
      "Epoch 3820/30000  training loss: 0.2607768177986145 Validation loss 0.25686609745025635\n",
      "Epoch 3830/30000  training loss: 0.2606860101222992 Validation loss 0.2567753791809082\n",
      "Epoch 3840/30000  training loss: 0.26059508323669434 Validation loss 0.2566845417022705\n",
      "Epoch 3850/30000  training loss: 0.2605040371417999 Validation loss 0.25659364461898804\n",
      "Epoch 3860/30000  training loss: 0.26041290163993835 Validation loss 0.25650259852409363\n",
      "Epoch 3870/30000  training loss: 0.2603216767311096 Validation loss 0.25641149282455444\n",
      "Epoch 3880/30000  training loss: 0.2602303624153137 Validation loss 0.2563202679157257\n",
      "Epoch 3890/30000  training loss: 0.2601388990879059 Validation loss 0.2562289237976074\n",
      "Epoch 3900/30000  training loss: 0.2600473463535309 Validation loss 0.256137490272522\n",
      "Epoch 3910/30000  training loss: 0.25995567440986633 Validation loss 0.256045937538147\n",
      "Epoch 3920/30000  training loss: 0.259863942861557 Validation loss 0.2559543251991272\n",
      "Epoch 3930/30000  training loss: 0.25977209210395813 Validation loss 0.25586259365081787\n",
      "Epoch 3940/30000  training loss: 0.2596801221370697 Validation loss 0.255770742893219\n",
      "Epoch 3950/30000  training loss: 0.2595880627632141 Validation loss 0.25567880272865295\n",
      "Epoch 3960/30000  training loss: 0.25949588418006897 Validation loss 0.25558674335479736\n",
      "Epoch 3970/30000  training loss: 0.25940361618995667 Validation loss 0.255494624376297\n",
      "Epoch 3980/30000  training loss: 0.25931107997894287 Validation loss 0.25540217757225037\n",
      "Epoch 3990/30000  training loss: 0.2592184245586395 Validation loss 0.25530967116355896\n",
      "Epoch 4000/30000  training loss: 0.25912564992904663 Validation loss 0.255217045545578\n",
      "Epoch 4010/30000  training loss: 0.25903281569480896 Validation loss 0.2551243007183075\n",
      "Epoch 4020/30000  training loss: 0.25893983244895935 Validation loss 0.2550314962863922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4030/30000  training loss: 0.2588467299938202 Validation loss 0.254938542842865\n",
      "Epoch 4040/30000  training loss: 0.25875359773635864 Validation loss 0.2548454999923706\n",
      "Epoch 4050/30000  training loss: 0.25866028666496277 Validation loss 0.25475239753723145\n",
      "Epoch 4060/30000  training loss: 0.2585669159889221 Validation loss 0.25465911626815796\n",
      "Epoch 4070/30000  training loss: 0.2584734559059143 Validation loss 0.2545657455921173\n",
      "Epoch 4080/30000  training loss: 0.25837981700897217 Validation loss 0.2544723153114319\n",
      "Epoch 4090/30000  training loss: 0.25828614830970764 Validation loss 0.2543787658214569\n",
      "Epoch 4100/30000  training loss: 0.2581923305988312 Validation loss 0.25428512692451477\n",
      "Epoch 4110/30000  training loss: 0.25809845328330994 Validation loss 0.2541913688182831\n",
      "Epoch 4120/30000  training loss: 0.25800439715385437 Validation loss 0.25409749150276184\n",
      "Epoch 4130/30000  training loss: 0.25791025161743164 Validation loss 0.2540034353733063\n",
      "Epoch 4140/30000  training loss: 0.2578158378601074 Validation loss 0.25390923023223877\n",
      "Epoch 4150/30000  training loss: 0.2577213644981384 Validation loss 0.2538148760795593\n",
      "Epoch 4160/30000  training loss: 0.2576267719268799 Validation loss 0.2537204325199127\n",
      "Epoch 4170/30000  training loss: 0.2575320303440094 Validation loss 0.25362586975097656\n",
      "Epoch 4180/30000  training loss: 0.25743722915649414 Validation loss 0.25353121757507324\n",
      "Epoch 4190/30000  training loss: 0.25734230875968933 Validation loss 0.25343644618988037\n",
      "Epoch 4200/30000  training loss: 0.25724726915359497 Validation loss 0.25334158539772034\n",
      "Epoch 4210/30000  training loss: 0.25715214014053345 Validation loss 0.25324663519859314\n",
      "Epoch 4220/30000  training loss: 0.2570568919181824 Validation loss 0.253151535987854\n",
      "Epoch 4230/30000  training loss: 0.25696152448654175 Validation loss 0.2530563473701477\n",
      "Epoch 4240/30000  training loss: 0.25686609745025635 Validation loss 0.25296106934547424\n",
      "Epoch 4250/30000  training loss: 0.256770521402359 Validation loss 0.25286567211151123\n",
      "Epoch 4260/30000  training loss: 0.2566749155521393 Validation loss 0.25277021527290344\n",
      "Epoch 4270/30000  training loss: 0.25657913088798523 Validation loss 0.2526746392250061\n",
      "Epoch 4280/30000  training loss: 0.256483256816864 Validation loss 0.2525789439678192\n",
      "Epoch 4290/30000  training loss: 0.2563871443271637 Validation loss 0.25248298048973083\n",
      "Epoch 4300/30000  training loss: 0.25629088282585144 Validation loss 0.2523868680000305\n",
      "Epoch 4310/30000  training loss: 0.25619450211524963 Validation loss 0.2522907257080078\n",
      "Epoch 4320/30000  training loss: 0.25609806180000305 Validation loss 0.25219443440437317\n",
      "Epoch 4330/30000  training loss: 0.25600147247314453 Validation loss 0.252098023891449\n",
      "Epoch 4340/30000  training loss: 0.25590479373931885 Validation loss 0.2520015239715576\n",
      "Epoch 4350/30000  training loss: 0.255808025598526 Validation loss 0.2519049346446991\n",
      "Epoch 4360/30000  training loss: 0.2557111382484436 Validation loss 0.251808226108551\n",
      "Epoch 4370/30000  training loss: 0.25561410188674927 Validation loss 0.2517113983631134\n",
      "Epoch 4380/30000  training loss: 0.25551703572273254 Validation loss 0.251614511013031\n",
      "Epoch 4390/30000  training loss: 0.2554198205471039 Validation loss 0.25151750445365906\n",
      "Epoch 4400/30000  training loss: 0.25532248616218567 Validation loss 0.25142034888267517\n",
      "Epoch 4410/30000  training loss: 0.2552250623703003 Validation loss 0.2513231337070465\n",
      "Epoch 4420/30000  training loss: 0.25512754917144775 Validation loss 0.2512258291244507\n",
      "Epoch 4430/30000  training loss: 0.2550298869609833 Validation loss 0.2511283755302429\n",
      "Epoch 4440/30000  training loss: 0.25493207573890686 Validation loss 0.25103071331977844\n",
      "Epoch 4450/30000  training loss: 0.25483402609825134 Validation loss 0.250932902097702\n",
      "Epoch 4460/30000  training loss: 0.25473591685295105 Validation loss 0.25083497166633606\n",
      "Epoch 4470/30000  training loss: 0.2546376585960388 Validation loss 0.25073692202568054\n",
      "Epoch 4480/30000  training loss: 0.2545393109321594 Validation loss 0.25063878297805786\n",
      "Epoch 4490/30000  training loss: 0.2544408440589905 Validation loss 0.250540554523468\n",
      "Epoch 4500/30000  training loss: 0.25434228777885437 Validation loss 0.2504422068595886\n",
      "Epoch 4510/30000  training loss: 0.2542436122894287 Validation loss 0.25034376978874207\n",
      "Epoch 4520/30000  training loss: 0.2541448473930359 Validation loss 0.25024518370628357\n",
      "Epoch 4530/30000  training loss: 0.2540459632873535 Validation loss 0.2501465380191803\n",
      "Epoch 4540/30000  training loss: 0.253946989774704 Validation loss 0.25004780292510986\n",
      "Epoch 4550/30000  training loss: 0.2538478970527649 Validation loss 0.2499489039182663\n",
      "Epoch 4560/30000  training loss: 0.25374871492385864 Validation loss 0.24984993040561676\n",
      "Epoch 4570/30000  training loss: 0.25364938378334045 Validation loss 0.24975085258483887\n",
      "Epoch 4580/30000  training loss: 0.2535499632358551 Validation loss 0.24965164065361023\n",
      "Epoch 4590/30000  training loss: 0.2534503936767578 Validation loss 0.24955224990844727\n",
      "Epoch 4600/30000  training loss: 0.2533505856990814 Validation loss 0.24945268034934998\n",
      "Epoch 4610/30000  training loss: 0.2532506287097931 Validation loss 0.24935300648212433\n",
      "Epoch 4620/30000  training loss: 0.25315061211586 Validation loss 0.24925322830677032\n",
      "Epoch 4630/30000  training loss: 0.2530505061149597 Validation loss 0.24915331602096558\n",
      "Epoch 4640/30000  training loss: 0.2529502809047699 Validation loss 0.24905332922935486\n",
      "Epoch 4650/30000  training loss: 0.25284990668296814 Validation loss 0.2489532232284546\n",
      "Epoch 4660/30000  training loss: 0.2527494728565216 Validation loss 0.24885302782058716\n",
      "Epoch 4670/30000  training loss: 0.25264889001846313 Validation loss 0.24875272810459137\n",
      "Epoch 4680/30000  training loss: 0.2525482475757599 Validation loss 0.24865229427814484\n",
      "Epoch 4690/30000  training loss: 0.2524474859237671 Validation loss 0.24855177104473114\n",
      "Epoch 4700/30000  training loss: 0.25234663486480713 Validation loss 0.2484511435031891\n",
      "Epoch 4710/30000  training loss: 0.25224563479423523 Validation loss 0.24835041165351868\n",
      "Epoch 4720/30000  training loss: 0.2521445155143738 Validation loss 0.24824956059455872\n",
      "Epoch 4730/30000  training loss: 0.25204333662986755 Validation loss 0.24814866483211517\n",
      "Epoch 4740/30000  training loss: 0.2519419491291046 Validation loss 0.24804747104644775\n",
      "Epoch 4750/30000  training loss: 0.2518403232097626 Validation loss 0.2479461431503296\n",
      "Epoch 4760/30000  training loss: 0.25173863768577576 Validation loss 0.24784471094608307\n",
      "Epoch 4770/30000  training loss: 0.251636803150177 Validation loss 0.2477431446313858\n",
      "Epoch 4780/30000  training loss: 0.25153490900993347 Validation loss 0.24764150381088257\n",
      "Epoch 4790/30000  training loss: 0.251432865858078 Validation loss 0.24753974378108978\n",
      "Epoch 4800/30000  training loss: 0.25133076310157776 Validation loss 0.24743787944316864\n",
      "Epoch 4810/30000  training loss: 0.2512285113334656 Validation loss 0.24733591079711914\n",
      "Epoch 4820/30000  training loss: 0.25112614035606384 Validation loss 0.24723383784294128\n",
      "Epoch 4830/30000  training loss: 0.25102370977401733 Validation loss 0.24713164567947388\n",
      "Epoch 4840/30000  training loss: 0.2509211301803589 Validation loss 0.2470293641090393\n",
      "Epoch 4850/30000  training loss: 0.25081849098205566 Validation loss 0.24692697823047638\n",
      "Epoch 4860/30000  training loss: 0.2507157325744629 Validation loss 0.2468244880437851\n",
      "Epoch 4870/30000  training loss: 0.2506128251552582 Validation loss 0.24672187864780426\n",
      "Epoch 4880/30000  training loss: 0.2505098283290863 Validation loss 0.24661917984485626\n",
      "Epoch 4890/30000  training loss: 0.25040656328201294 Validation loss 0.24651621282100677\n",
      "Epoch 4900/30000  training loss: 0.2503032088279724 Validation loss 0.24641308188438416\n",
      "Epoch 4910/30000  training loss: 0.25019970536231995 Validation loss 0.24630990624427795\n",
      "Epoch 4920/30000  training loss: 0.25009608268737793 Validation loss 0.24620656669139862\n",
      "Epoch 4930/30000  training loss: 0.24999238550662994 Validation loss 0.2461031675338745\n",
      "Epoch 4940/30000  training loss: 0.24988853931427002 Validation loss 0.24599964916706085\n",
      "Epoch 4950/30000  training loss: 0.24978463351726532 Validation loss 0.24589602649211884\n",
      "Epoch 4960/30000  training loss: 0.24968057870864868 Validation loss 0.24579226970672607\n",
      "Epoch 4970/30000  training loss: 0.24957644939422607 Validation loss 0.24568843841552734\n",
      "Epoch 4980/30000  training loss: 0.24947218596935272 Validation loss 0.24558450281620026\n",
      "Epoch 4990/30000  training loss: 0.2493678331375122 Validation loss 0.24548044800758362\n",
      "Epoch 5000/30000  training loss: 0.24926336109638214 Validation loss 0.24537628889083862\n",
      "Epoch 5010/30000  training loss: 0.2491587996482849 Validation loss 0.24527202546596527\n",
      "Epoch 5020/30000  training loss: 0.24905410408973694 Validation loss 0.24516767263412476\n",
      "Epoch 5030/30000  training loss: 0.2489493042230606 Validation loss 0.2450631707906723\n",
      "Epoch 5040/30000  training loss: 0.2488442212343216 Validation loss 0.24495838582515717\n",
      "Epoch 5050/30000  training loss: 0.24873901903629303 Validation loss 0.24485352635383606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5060/30000  training loss: 0.2486337125301361 Validation loss 0.2447485327720642\n",
      "Epoch 5070/30000  training loss: 0.24852830171585083 Validation loss 0.2446434497833252\n",
      "Epoch 5080/30000  training loss: 0.2484227865934372 Validation loss 0.24453827738761902\n",
      "Epoch 5090/30000  training loss: 0.2483171969652176 Validation loss 0.2444329857826233\n",
      "Epoch 5100/30000  training loss: 0.24821145832538605 Validation loss 0.2443275898694992\n",
      "Epoch 5110/30000  training loss: 0.24810561537742615 Validation loss 0.24422207474708557\n",
      "Epoch 5120/30000  training loss: 0.2479996681213379 Validation loss 0.24411647021770477\n",
      "Epoch 5130/30000  training loss: 0.24789360165596008 Validation loss 0.24401076138019562\n",
      "Epoch 5140/30000  training loss: 0.2477874606847763 Validation loss 0.2439049631357193\n",
      "Epoch 5150/30000  training loss: 0.24768120050430298 Validation loss 0.24379903078079224\n",
      "Epoch 5160/30000  training loss: 0.2475748211145401 Validation loss 0.24369297921657562\n",
      "Epoch 5170/30000  training loss: 0.24746832251548767 Validation loss 0.24358682334423065\n",
      "Epoch 5180/30000  training loss: 0.24736161530017853 Validation loss 0.24348045885562897\n",
      "Epoch 5190/30000  training loss: 0.24725472927093506 Validation loss 0.24337396025657654\n",
      "Epoch 5200/30000  training loss: 0.24714772403240204 Validation loss 0.24326732754707336\n",
      "Epoch 5210/30000  training loss: 0.24704064428806305 Validation loss 0.24316056072711945\n",
      "Epoch 5220/30000  training loss: 0.2469334453344345 Validation loss 0.24305370450019836\n",
      "Epoch 5230/30000  training loss: 0.24682611227035522 Validation loss 0.24294675886631012\n",
      "Epoch 5240/30000  training loss: 0.24671868979930878 Validation loss 0.2428397238254547\n",
      "Epoch 5250/30000  training loss: 0.24661116302013397 Validation loss 0.24273253977298737\n",
      "Epoch 5260/30000  training loss: 0.24650351703166962 Validation loss 0.24262526631355286\n",
      "Epoch 5270/30000  training loss: 0.2463957816362381 Validation loss 0.24251788854599\n",
      "Epoch 5280/30000  training loss: 0.24628794193267822 Validation loss 0.24241040647029877\n",
      "Epoch 5290/30000  training loss: 0.2461799681186676 Validation loss 0.24230286478996277\n",
      "Epoch 5300/30000  training loss: 0.24607188999652863 Validation loss 0.24219514429569244\n",
      "Epoch 5310/30000  training loss: 0.24596372246742249 Validation loss 0.24208731949329376\n",
      "Epoch 5320/30000  training loss: 0.2458554357290268 Validation loss 0.2419794201850891\n",
      "Epoch 5330/30000  training loss: 0.24574685096740723 Validation loss 0.2418711930513382\n",
      "Epoch 5340/30000  training loss: 0.24563813209533691 Validation loss 0.2417629063129425\n",
      "Epoch 5350/30000  training loss: 0.24552935361862183 Validation loss 0.24165450036525726\n",
      "Epoch 5360/30000  training loss: 0.2454204559326172 Validation loss 0.24154599010944366\n",
      "Epoch 5370/30000  training loss: 0.245311439037323 Validation loss 0.24143736064434052\n",
      "Epoch 5380/30000  training loss: 0.24520231783390045 Validation loss 0.2413286715745926\n",
      "Epoch 5390/30000  training loss: 0.24509309232234955 Validation loss 0.24121983349323273\n",
      "Epoch 5400/30000  training loss: 0.2449837625026703 Validation loss 0.2411108911037445\n",
      "Epoch 5410/30000  training loss: 0.24487432837486267 Validation loss 0.24100185930728912\n",
      "Epoch 5420/30000  training loss: 0.2447647750377655 Validation loss 0.24089272320270538\n",
      "Epoch 5430/30000  training loss: 0.24465514719486237 Validation loss 0.2407834827899933\n",
      "Epoch 5440/30000  training loss: 0.2445453703403473 Validation loss 0.24067415297031403\n",
      "Epoch 5450/30000  training loss: 0.24443550407886505 Validation loss 0.24056468904018402\n",
      "Epoch 5460/30000  training loss: 0.24432553350925446 Validation loss 0.24045513570308685\n",
      "Epoch 5470/30000  training loss: 0.24421529471874237 Validation loss 0.240345299243927\n",
      "Epoch 5480/30000  training loss: 0.24410492181777954 Validation loss 0.2402353435754776\n",
      "Epoch 5490/30000  training loss: 0.24399444460868835 Validation loss 0.24012528359889984\n",
      "Epoch 5500/30000  training loss: 0.24388383328914642 Validation loss 0.24001510441303253\n",
      "Epoch 5510/30000  training loss: 0.24377313256263733 Validation loss 0.23990483582019806\n",
      "Epoch 5520/30000  training loss: 0.24366231262683868 Validation loss 0.23979444801807404\n",
      "Epoch 5530/30000  training loss: 0.24355141818523407 Validation loss 0.23968398571014404\n",
      "Epoch 5540/30000  training loss: 0.24344037473201752 Validation loss 0.2395734190940857\n",
      "Epoch 5550/30000  training loss: 0.243329256772995 Validation loss 0.2394627183675766\n",
      "Epoch 5560/30000  training loss: 0.24321803450584412 Validation loss 0.23935192823410034\n",
      "Epoch 5570/30000  training loss: 0.2431066781282425 Validation loss 0.23924100399017334\n",
      "Epoch 5580/30000  training loss: 0.2429952472448349 Validation loss 0.23913002014160156\n",
      "Epoch 5590/30000  training loss: 0.24288368225097656 Validation loss 0.23901891708374023\n",
      "Epoch 5600/30000  training loss: 0.24277202785015106 Validation loss 0.23890772461891174\n",
      "Epoch 5610/30000  training loss: 0.24266014993190765 Validation loss 0.23879624903202057\n",
      "Epoch 5620/30000  training loss: 0.24254807829856873 Validation loss 0.23868463933467865\n",
      "Epoch 5630/30000  training loss: 0.24243588745594025 Validation loss 0.23857292532920837\n",
      "Epoch 5640/30000  training loss: 0.2423236221075058 Validation loss 0.23846113681793213\n",
      "Epoch 5650/30000  training loss: 0.2422112375497818 Validation loss 0.23834919929504395\n",
      "Epoch 5660/30000  training loss: 0.24209873378276825 Validation loss 0.2382371574640274\n",
      "Epoch 5670/30000  training loss: 0.24198614060878754 Validation loss 0.2381250560283661\n",
      "Epoch 5680/30000  training loss: 0.24187342822551727 Validation loss 0.23801282048225403\n",
      "Epoch 5690/30000  training loss: 0.24176061153411865 Validation loss 0.2379004955291748\n",
      "Epoch 5700/30000  training loss: 0.24164770543575287 Validation loss 0.23778803646564484\n",
      "Epoch 5710/30000  training loss: 0.24153469502925873 Validation loss 0.2376754879951477\n",
      "Epoch 5720/30000  training loss: 0.24142156541347504 Validation loss 0.2375628799200058\n",
      "Epoch 5730/30000  training loss: 0.241308331489563 Validation loss 0.23745009303092957\n",
      "Epoch 5740/30000  training loss: 0.24119499325752258 Validation loss 0.23733726143836975\n",
      "Epoch 5750/30000  training loss: 0.24108146131038666 Validation loss 0.23722423613071442\n",
      "Epoch 5760/30000  training loss: 0.2409677356481552 Validation loss 0.2371109575033188\n",
      "Epoch 5770/30000  training loss: 0.24085387587547302 Validation loss 0.23699763417243958\n",
      "Epoch 5780/30000  training loss: 0.24073989689350128 Validation loss 0.23688414692878723\n",
      "Epoch 5790/30000  training loss: 0.24062584340572357 Validation loss 0.2367706149816513\n",
      "Epoch 5800/30000  training loss: 0.2405116856098175 Validation loss 0.23665694892406464\n",
      "Epoch 5810/30000  training loss: 0.2403974086046219 Validation loss 0.2365431785583496\n",
      "Epoch 5820/30000  training loss: 0.2402830272912979 Validation loss 0.23642927408218384\n",
      "Epoch 5830/30000  training loss: 0.2401685267686844 Validation loss 0.23631533980369568\n",
      "Epoch 5840/30000  training loss: 0.2400539666414261 Validation loss 0.23620128631591797\n",
      "Epoch 5850/30000  training loss: 0.23993925750255585 Validation loss 0.23608708381652832\n",
      "Epoch 5860/30000  training loss: 0.23982444405555725 Validation loss 0.2359728217124939\n",
      "Epoch 5870/30000  training loss: 0.23970958590507507 Validation loss 0.2358584702014923\n",
      "Epoch 5880/30000  training loss: 0.23959454894065857 Validation loss 0.2357439547777176\n",
      "Epoch 5890/30000  training loss: 0.23947931826114655 Validation loss 0.23562924563884735\n",
      "Epoch 5900/30000  training loss: 0.23936393857002258 Validation loss 0.23551437258720398\n",
      "Epoch 5910/30000  training loss: 0.23924843966960907 Validation loss 0.23539942502975464\n",
      "Epoch 5920/30000  training loss: 0.2391328066587448 Validation loss 0.23528434336185455\n",
      "Epoch 5930/30000  training loss: 0.23901702463626862 Validation loss 0.23516912758350372\n",
      "Epoch 5940/30000  training loss: 0.23890121281147003 Validation loss 0.23505383729934692\n",
      "Epoch 5950/30000  training loss: 0.2387852966785431 Validation loss 0.23493845760822296\n",
      "Epoch 5960/30000  training loss: 0.2386692315340042 Validation loss 0.23482297360897064\n",
      "Epoch 5970/30000  training loss: 0.23855307698249817 Validation loss 0.23470735549926758\n",
      "Epoch 5980/30000  training loss: 0.23843683302402496 Validation loss 0.23459167778491974\n",
      "Epoch 5990/30000  training loss: 0.2383205145597458 Validation loss 0.23447588086128235\n",
      "Epoch 6000/30000  training loss: 0.23820403218269348 Validation loss 0.2343599796295166\n",
      "Epoch 6010/30000  training loss: 0.23808744549751282 Validation loss 0.2342439889907837\n",
      "Epoch 6020/30000  training loss: 0.23797079920768738 Validation loss 0.23412787914276123\n",
      "Epoch 6030/30000  training loss: 0.23785395920276642 Validation loss 0.23401160538196564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6040/30000  training loss: 0.23773686587810516 Validation loss 0.23389507830142975\n",
      "Epoch 6050/30000  training loss: 0.23761968314647675 Validation loss 0.2337784767150879\n",
      "Epoch 6060/30000  training loss: 0.23750239610671997 Validation loss 0.23366177082061768\n",
      "Epoch 6070/30000  training loss: 0.23738503456115723 Validation loss 0.23354500532150269\n",
      "Epoch 6080/30000  training loss: 0.23726750910282135 Validation loss 0.23342804610729218\n",
      "Epoch 6090/30000  training loss: 0.2371499091386795 Validation loss 0.23331105709075928\n",
      "Epoch 6100/30000  training loss: 0.2370322197675705 Validation loss 0.23319396376609802\n",
      "Epoch 6110/30000  training loss: 0.2369144707918167 Validation loss 0.2330767959356308\n",
      "Epoch 6120/30000  training loss: 0.2367965430021286 Validation loss 0.23295949399471283\n",
      "Epoch 6130/30000  training loss: 0.23667854070663452 Validation loss 0.23284205794334412\n",
      "Epoch 6140/30000  training loss: 0.23656043410301208 Validation loss 0.23272456228733063\n",
      "Epoch 6150/30000  training loss: 0.23644223809242249 Validation loss 0.23260697722434998\n",
      "Epoch 6160/30000  training loss: 0.23632390797138214 Validation loss 0.23248927295207977\n",
      "Epoch 6170/30000  training loss: 0.23620541393756866 Validation loss 0.23237136006355286\n",
      "Epoch 6180/30000  training loss: 0.23608677089214325 Validation loss 0.23225334286689758\n",
      "Epoch 6190/30000  training loss: 0.23596803843975067 Validation loss 0.23213522136211395\n",
      "Epoch 6200/30000  training loss: 0.23584921658039093 Validation loss 0.23201702535152435\n",
      "Epoch 6210/30000  training loss: 0.23573018610477448 Validation loss 0.23189862072467804\n",
      "Epoch 6220/30000  training loss: 0.23561109602451324 Validation loss 0.23178014159202576\n",
      "Epoch 6230/30000  training loss: 0.23549184203147888 Validation loss 0.23166154325008392\n",
      "Epoch 6240/30000  training loss: 0.23537252843379974 Validation loss 0.23154285550117493\n",
      "Epoch 6250/30000  training loss: 0.23525309562683105 Validation loss 0.23142406344413757\n",
      "Epoch 6260/30000  training loss: 0.2351335734128952 Validation loss 0.23130519688129425\n",
      "Epoch 6270/30000  training loss: 0.2350139319896698 Validation loss 0.231186181306839\n",
      "Epoch 6280/30000  training loss: 0.23489408195018768 Validation loss 0.2310670018196106\n",
      "Epoch 6290/30000  training loss: 0.2347741723060608 Validation loss 0.23094771802425385\n",
      "Epoch 6300/30000  training loss: 0.23465414345264435 Validation loss 0.23082832992076874\n",
      "Epoch 6310/30000  training loss: 0.23453396558761597 Validation loss 0.23070883750915527\n",
      "Epoch 6320/30000  training loss: 0.234413743019104 Validation loss 0.23058927059173584\n",
      "Epoch 6330/30000  training loss: 0.23429341614246368 Validation loss 0.23046962916851044\n",
      "Epoch 6340/30000  training loss: 0.234172984957695 Validation loss 0.2303498387336731\n",
      "Epoch 6350/30000  training loss: 0.23405231535434723 Validation loss 0.23022985458374023\n",
      "Epoch 6360/30000  training loss: 0.23393158614635468 Validation loss 0.2301097810268402\n",
      "Epoch 6370/30000  training loss: 0.23381078243255615 Validation loss 0.2299896478652954\n",
      "Epoch 6380/30000  training loss: 0.2336898297071457 Validation loss 0.22986936569213867\n",
      "Epoch 6390/30000  training loss: 0.23356878757476807 Validation loss 0.22974900901317596\n",
      "Epoch 6400/30000  training loss: 0.2334476113319397 Validation loss 0.2296285331249237\n",
      "Epoch 6410/30000  training loss: 0.23332639038562775 Validation loss 0.22950799763202667\n",
      "Epoch 6420/30000  training loss: 0.2332049310207367 Validation loss 0.22938722372055054\n",
      "Epoch 6430/30000  training loss: 0.23308339715003967 Validation loss 0.22926640510559082\n",
      "Epoch 6440/30000  training loss: 0.2329617440700531 Validation loss 0.22914546728134155\n",
      "Epoch 6450/30000  training loss: 0.23284001648426056 Validation loss 0.22902442514896393\n",
      "Epoch 6460/30000  training loss: 0.23271818459033966 Validation loss 0.22890329360961914\n",
      "Epoch 6470/30000  training loss: 0.2325962632894516 Validation loss 0.228782057762146\n",
      "Epoch 6480/30000  training loss: 0.2324742078781128 Validation loss 0.2286607176065445\n",
      "Epoch 6490/30000  training loss: 0.23235194385051727 Validation loss 0.22853918373584747\n",
      "Epoch 6500/30000  training loss: 0.23222962021827698 Validation loss 0.2284175604581833\n",
      "Epoch 6510/30000  training loss: 0.23210717737674713 Validation loss 0.22829587757587433\n",
      "Epoch 6520/30000  training loss: 0.2319846749305725 Validation loss 0.22817407548427582\n",
      "Epoch 6530/30000  training loss: 0.23186203837394714 Validation loss 0.22805215418338776\n",
      "Epoch 6540/30000  training loss: 0.23173928260803223 Validation loss 0.22793014347553253\n",
      "Epoch 6550/30000  training loss: 0.23161642253398895 Validation loss 0.22780804336071014\n",
      "Epoch 6560/30000  training loss: 0.23149341344833374 Validation loss 0.22768576443195343\n",
      "Epoch 6570/30000  training loss: 0.23137032985687256 Validation loss 0.22756339609622955\n",
      "Epoch 6580/30000  training loss: 0.23124706745147705 Validation loss 0.22744090855121613\n",
      "Epoch 6590/30000  training loss: 0.23112376034259796 Validation loss 0.22731833159923553\n",
      "Epoch 6600/30000  training loss: 0.23100034892559052 Validation loss 0.22719570994377136\n",
      "Epoch 6610/30000  training loss: 0.23087681829929352 Validation loss 0.22707293927669525\n",
      "Epoch 6620/30000  training loss: 0.23075321316719055 Validation loss 0.22695006430149078\n",
      "Epoch 6630/30000  training loss: 0.23062939941883087 Validation loss 0.2268269807100296\n",
      "Epoch 6640/30000  training loss: 0.23050549626350403 Validation loss 0.22670386731624603\n",
      "Epoch 6650/30000  training loss: 0.23038147389888763 Validation loss 0.22658061981201172\n",
      "Epoch 6660/30000  training loss: 0.23025736212730408 Validation loss 0.22645729780197144\n",
      "Epoch 6670/30000  training loss: 0.23013320565223694 Validation loss 0.22633390128612518\n",
      "Epoch 6680/30000  training loss: 0.23000892996788025 Validation loss 0.22621040046215057\n",
      "Epoch 6690/30000  training loss: 0.22988447546958923 Validation loss 0.22608673572540283\n",
      "Epoch 6700/30000  training loss: 0.22975988686084747 Validation loss 0.22596292197704315\n",
      "Epoch 6710/30000  training loss: 0.22963522374629974 Validation loss 0.2258390635251999\n",
      "Epoch 6720/30000  training loss: 0.22951047122478485 Validation loss 0.22571511566638947\n",
      "Epoch 6730/30000  training loss: 0.2293855845928192 Validation loss 0.2255910336971283\n",
      "Epoch 6740/30000  training loss: 0.22926059365272522 Validation loss 0.22546686232089996\n",
      "Epoch 6750/30000  training loss: 0.22913554310798645 Validation loss 0.22534263134002686\n",
      "Epoch 6760/30000  training loss: 0.22901037335395813 Validation loss 0.22521822154521942\n",
      "Epoch 6770/30000  training loss: 0.2288849800825119 Validation loss 0.22509366273880005\n",
      "Epoch 6780/30000  training loss: 0.22875957190990448 Validation loss 0.2249690592288971\n",
      "Epoch 6790/30000  training loss: 0.2286340445280075 Validation loss 0.22484436631202698\n",
      "Epoch 6800/30000  training loss: 0.22850839793682098 Validation loss 0.2247195541858673\n",
      "Epoch 6810/30000  training loss: 0.2283826619386673 Validation loss 0.22459463775157928\n",
      "Epoch 6820/30000  training loss: 0.22825685143470764 Validation loss 0.2244696319103241\n",
      "Epoch 6830/30000  training loss: 0.22813089191913605 Validation loss 0.22434452176094055\n",
      "Epoch 6840/30000  training loss: 0.2280047982931137 Validation loss 0.22421924769878387\n",
      "Epoch 6850/30000  training loss: 0.22787855565547943 Validation loss 0.22409389913082123\n",
      "Epoch 6860/30000  training loss: 0.22775228321552277 Validation loss 0.22396846115589142\n",
      "Epoch 6870/30000  training loss: 0.22762589156627655 Validation loss 0.22384291887283325\n",
      "Epoch 6880/30000  training loss: 0.22749942541122437 Validation loss 0.22371728718280792\n",
      "Epoch 6890/30000  training loss: 0.22737284004688263 Validation loss 0.22359158098697662\n",
      "Epoch 6900/30000  training loss: 0.22724613547325134 Validation loss 0.2234657108783722\n",
      "Epoch 6910/30000  training loss: 0.2271192967891693 Validation loss 0.2233397364616394\n",
      "Epoch 6920/30000  training loss: 0.22699229419231415 Validation loss 0.22321362793445587\n",
      "Epoch 6930/30000  training loss: 0.2268652468919754 Validation loss 0.22308743000030518\n",
      "Epoch 6940/30000  training loss: 0.2267381250858307 Validation loss 0.2229612022638321\n",
      "Epoch 6950/30000  training loss: 0.2266109436750412 Validation loss 0.22283487021923065\n",
      "Epoch 6960/30000  training loss: 0.22648359835147858 Validation loss 0.22270844876766205\n",
      "Epoch 6970/30000  training loss: 0.22635610401630402 Validation loss 0.22258183360099792\n",
      "Epoch 6980/30000  training loss: 0.2262285351753235 Validation loss 0.22245512902736664\n",
      "Epoch 6990/30000  training loss: 0.2261008471250534 Validation loss 0.22232834994792938\n",
      "Epoch 7000/30000  training loss: 0.22597302496433258 Validation loss 0.2222014218568802\n",
      "Epoch 7010/30000  training loss: 0.22584517300128937 Validation loss 0.2220744639635086\n",
      "Epoch 7020/30000  training loss: 0.225717231631279 Validation loss 0.22194746136665344\n",
      "Epoch 7030/30000  training loss: 0.22558918595314026 Validation loss 0.22182030975818634\n",
      "Epoch 7040/30000  training loss: 0.225460946559906 Validation loss 0.2216929793357849\n",
      "Epoch 7050/30000  training loss: 0.2253326028585434 Validation loss 0.22156557440757751\n",
      "Epoch 7060/30000  training loss: 0.225204199552536 Validation loss 0.22143806517124176\n",
      "Epoch 7070/30000  training loss: 0.22507570683956146 Validation loss 0.22131052613258362\n",
      "Epoch 7080/30000  training loss: 0.22494709491729736 Validation loss 0.22118280827999115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7090/30000  training loss: 0.2248183786869049 Validation loss 0.2210550755262375\n",
      "Epoch 7100/30000  training loss: 0.22468963265419006 Validation loss 0.22092723846435547\n",
      "Epoch 7110/30000  training loss: 0.2245607227087021 Validation loss 0.22079923748970032\n",
      "Epoch 7120/30000  training loss: 0.22443163394927979 Validation loss 0.22067110240459442\n",
      "Epoch 7130/30000  training loss: 0.22430245578289032 Validation loss 0.22054292261600494\n",
      "Epoch 7140/30000  training loss: 0.22417326271533966 Validation loss 0.2204146534204483\n",
      "Epoch 7150/30000  training loss: 0.22404396533966064 Validation loss 0.2202863097190857\n",
      "Epoch 7160/30000  training loss: 0.22391454875469208 Validation loss 0.22015786170959473\n",
      "Epoch 7170/30000  training loss: 0.22378504276275635 Validation loss 0.2200292944908142\n",
      "Epoch 7180/30000  training loss: 0.22365540266036987 Validation loss 0.21990063786506653\n",
      "Epoch 7190/30000  training loss: 0.22352562844753265 Validation loss 0.21977181732654572\n",
      "Epoch 7200/30000  training loss: 0.22339574992656708 Validation loss 0.21964292228221893\n",
      "Epoch 7210/30000  training loss: 0.22326579689979553 Validation loss 0.21951398253440857\n",
      "Epoch 7220/30000  training loss: 0.2231358140707016 Validation loss 0.21938496828079224\n",
      "Epoch 7230/30000  training loss: 0.22300569713115692 Validation loss 0.21925583481788635\n",
      "Epoch 7240/30000  training loss: 0.22287549078464508 Validation loss 0.2191266268491745\n",
      "Epoch 7250/30000  training loss: 0.2227451205253601 Validation loss 0.2189972698688507\n",
      "Epoch 7260/30000  training loss: 0.22261467576026917 Validation loss 0.21886779367923737\n",
      "Epoch 7270/30000  training loss: 0.22248411178588867 Validation loss 0.21873824298381805\n",
      "Epoch 7280/30000  training loss: 0.22235345840454102 Validation loss 0.21860861778259277\n",
      "Epoch 7290/30000  training loss: 0.2222227305173874 Validation loss 0.21847888827323914\n",
      "Epoch 7300/30000  training loss: 0.22209195792675018 Validation loss 0.2183491438627243\n",
      "Epoch 7310/30000  training loss: 0.22196106612682343 Validation loss 0.21821928024291992\n",
      "Epoch 7320/30000  training loss: 0.22182999551296234 Validation loss 0.2180892378091812\n",
      "Epoch 7330/30000  training loss: 0.2216988503932953 Validation loss 0.21795909106731415\n",
      "Epoch 7340/30000  training loss: 0.22156761586666107 Validation loss 0.2178288698196411\n",
      "Epoch 7350/30000  training loss: 0.2214362770318985 Validation loss 0.2176986038684845\n",
      "Epoch 7360/30000  training loss: 0.22130484879016876 Validation loss 0.21756820380687714\n",
      "Epoch 7370/30000  training loss: 0.22117337584495544 Validation loss 0.2174377590417862\n",
      "Epoch 7380/30000  training loss: 0.22104181349277496 Validation loss 0.21730726957321167\n",
      "Epoch 7390/30000  training loss: 0.22091008722782135 Validation loss 0.21717660129070282\n",
      "Epoch 7400/30000  training loss: 0.22077825665473938 Validation loss 0.21704579889774323\n",
      "Epoch 7410/30000  training loss: 0.22064633667469025 Validation loss 0.21691495180130005\n",
      "Epoch 7420/30000  training loss: 0.22051432728767395 Validation loss 0.2167840301990509\n",
      "Epoch 7430/30000  training loss: 0.2203822284936905 Validation loss 0.2166530191898346\n",
      "Epoch 7440/30000  training loss: 0.22025005519390106 Validation loss 0.21652188897132874\n",
      "Epoch 7450/30000  training loss: 0.22011782228946686 Validation loss 0.21639074385166168\n",
      "Epoch 7460/30000  training loss: 0.2199854552745819 Validation loss 0.2162594497203827\n",
      "Epoch 7470/30000  training loss: 0.2198529690504074 Validation loss 0.21612800657749176\n",
      "Epoch 7480/30000  training loss: 0.21972034871578217 Validation loss 0.21599651873111725\n",
      "Epoch 7490/30000  training loss: 0.21958771347999573 Validation loss 0.21586497128009796\n",
      "Epoch 7500/30000  training loss: 0.21945498883724213 Validation loss 0.2157333344221115\n",
      "Epoch 7510/30000  training loss: 0.21932215988636017 Validation loss 0.2156016230583191\n",
      "Epoch 7520/30000  training loss: 0.21918924152851105 Validation loss 0.2154698222875595\n",
      "Epoch 7530/30000  training loss: 0.21905617415905 Validation loss 0.215337872505188\n",
      "Epoch 7540/30000  training loss: 0.21892303228378296 Validation loss 0.2152058333158493\n",
      "Epoch 7550/30000  training loss: 0.21878981590270996 Validation loss 0.21507371962070465\n",
      "Epoch 7560/30000  training loss: 0.21865646541118622 Validation loss 0.21494151651859283\n",
      "Epoch 7570/30000  training loss: 0.2185230702161789 Validation loss 0.21480923891067505\n",
      "Epoch 7580/30000  training loss: 0.2183896154165268 Validation loss 0.21467696130275726\n",
      "Epoch 7590/30000  training loss: 0.21825608611106873 Validation loss 0.21454453468322754\n",
      "Epoch 7600/30000  training loss: 0.2181224226951599 Validation loss 0.21441200375556946\n",
      "Epoch 7610/30000  training loss: 0.21798861026763916 Validation loss 0.21427935361862183\n",
      "Epoch 7620/30000  training loss: 0.21785473823547363 Validation loss 0.21414662897586823\n",
      "Epoch 7630/30000  training loss: 0.21772077679634094 Validation loss 0.21401382982730865\n",
      "Epoch 7640/30000  training loss: 0.21758675575256348 Validation loss 0.21388092637062073\n",
      "Epoch 7650/30000  training loss: 0.21745263040065765 Validation loss 0.2137480080127716\n",
      "Epoch 7660/30000  training loss: 0.21731846034526825 Validation loss 0.21361500024795532\n",
      "Epoch 7670/30000  training loss: 0.2171841859817505 Validation loss 0.21348190307617188\n",
      "Epoch 7680/30000  training loss: 0.2170497328042984 Validation loss 0.2133486121892929\n",
      "Epoch 7690/30000  training loss: 0.21691520512104034 Validation loss 0.21321529150009155\n",
      "Epoch 7700/30000  training loss: 0.2167806327342987 Validation loss 0.21308192610740662\n",
      "Epoch 7710/30000  training loss: 0.21664603054523468 Validation loss 0.2129484862089157\n",
      "Epoch 7720/30000  training loss: 0.2165113091468811 Validation loss 0.21281494200229645\n",
      "Epoch 7730/30000  training loss: 0.21637651324272156 Validation loss 0.2126813530921936\n",
      "Epoch 7740/30000  training loss: 0.21624164283275604 Validation loss 0.2125476747751236\n",
      "Epoch 7750/30000  training loss: 0.2161066234111786 Validation loss 0.21241387724876404\n",
      "Epoch 7760/30000  training loss: 0.2159714698791504 Validation loss 0.21227991580963135\n",
      "Epoch 7770/30000  training loss: 0.21583625674247742 Validation loss 0.21214595437049866\n",
      "Epoch 7780/30000  training loss: 0.21570101380348206 Validation loss 0.2120119035243988\n",
      "Epoch 7790/30000  training loss: 0.21556568145751953 Validation loss 0.21187783777713776\n",
      "Epoch 7800/30000  training loss: 0.21543028950691223 Validation loss 0.21174363791942596\n",
      "Epoch 7810/30000  training loss: 0.21529479324817657 Validation loss 0.2116093784570694\n",
      "Epoch 7820/30000  training loss: 0.21515913307666779 Validation loss 0.2114749550819397\n",
      "Epoch 7830/30000  training loss: 0.21502342820167542 Validation loss 0.2113405168056488\n",
      "Epoch 7840/30000  training loss: 0.21488766372203827 Validation loss 0.21120597422122955\n",
      "Epoch 7850/30000  training loss: 0.21475180983543396 Validation loss 0.21107138693332672\n",
      "Epoch 7860/30000  training loss: 0.21461589634418488 Validation loss 0.21093669533729553\n",
      "Epoch 7870/30000  training loss: 0.21447989344596863 Validation loss 0.21080200374126434\n",
      "Epoch 7880/30000  training loss: 0.21434390544891357 Validation loss 0.21066725254058838\n",
      "Epoch 7890/30000  training loss: 0.21420766413211823 Validation loss 0.2105322927236557\n",
      "Epoch 7900/30000  training loss: 0.2140713632106781 Validation loss 0.21039727330207825\n",
      "Epoch 7910/30000  training loss: 0.2139350324869156 Validation loss 0.21026219427585602\n",
      "Epoch 7920/30000  training loss: 0.21379859745502472 Validation loss 0.2101270854473114\n",
      "Epoch 7930/30000  training loss: 0.21366211771965027 Validation loss 0.20999187231063843\n",
      "Epoch 7940/30000  training loss: 0.21352557837963104 Validation loss 0.20985662937164307\n",
      "Epoch 7950/30000  training loss: 0.21338894963264465 Validation loss 0.20972129702568054\n",
      "Epoch 7960/30000  training loss: 0.2132522016763687 Validation loss 0.20958584547042847\n",
      "Epoch 7970/30000  training loss: 0.21311533451080322 Validation loss 0.20945025980472565\n",
      "Epoch 7980/30000  training loss: 0.21297837793827057 Validation loss 0.20931464433670044\n",
      "Epoch 7990/30000  training loss: 0.21284137666225433 Validation loss 0.20917895436286926\n",
      "Epoch 8000/30000  training loss: 0.21270433068275452 Validation loss 0.2090432196855545\n",
      "Epoch 8010/30000  training loss: 0.21256718039512634 Validation loss 0.20890741050243378\n",
      "Epoch 8020/30000  training loss: 0.2124299705028534 Validation loss 0.20877152681350708\n",
      "Epoch 8030/30000  training loss: 0.21229268610477448 Validation loss 0.208635613322258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8040/30000  training loss: 0.2121553122997284 Validation loss 0.20849955081939697\n",
      "Epoch 8050/30000  training loss: 0.21201784908771515 Validation loss 0.2083633989095688\n",
      "Epoch 8060/30000  training loss: 0.21188026666641235 Validation loss 0.20822720229625702\n",
      "Epoch 8070/30000  training loss: 0.21174265444278717 Validation loss 0.20809093117713928\n",
      "Epoch 8080/30000  training loss: 0.2116049975156784 Validation loss 0.20795464515686035\n",
      "Epoch 8090/30000  training loss: 0.21146729588508606 Validation loss 0.20781832933425903\n",
      "Epoch 8100/30000  training loss: 0.21132953464984894 Validation loss 0.20768189430236816\n",
      "Epoch 8110/30000  training loss: 0.2111915647983551 Validation loss 0.20754529535770416\n",
      "Epoch 8120/30000  training loss: 0.2110535353422165 Validation loss 0.20740865170955658\n",
      "Epoch 8130/30000  training loss: 0.2109154611825943 Validation loss 0.2072719931602478\n",
      "Epoch 8140/30000  training loss: 0.21077735722064972 Validation loss 0.20713527500629425\n",
      "Epoch 8150/30000  training loss: 0.21063917875289917 Validation loss 0.20699845254421234\n",
      "Epoch 8160/30000  training loss: 0.21050094068050385 Validation loss 0.20686160027980804\n",
      "Epoch 8170/30000  training loss: 0.21036262810230255 Validation loss 0.20672468841075897\n",
      "Epoch 8180/30000  training loss: 0.2102242112159729 Validation loss 0.20658770203590393\n",
      "Epoch 8190/30000  training loss: 0.2100856900215149 Validation loss 0.20645056664943695\n",
      "Epoch 8200/30000  training loss: 0.2099471092224121 Validation loss 0.20631340146064758\n",
      "Epoch 8210/30000  training loss: 0.20980845391750336 Validation loss 0.20617617666721344\n",
      "Epoch 8220/30000  training loss: 0.2096697986125946 Validation loss 0.2060389667749405\n",
      "Epoch 8230/30000  training loss: 0.2095310539007187 Validation loss 0.2059016227722168\n",
      "Epoch 8240/30000  training loss: 0.2093922197818756 Validation loss 0.20576421916484833\n",
      "Epoch 8250/30000  training loss: 0.20925335586071014 Validation loss 0.20562677085399628\n",
      "Epoch 8260/30000  training loss: 0.20911431312561035 Validation loss 0.20548923313617706\n",
      "Epoch 8270/30000  training loss: 0.20897527039051056 Validation loss 0.20535162091255188\n",
      "Epoch 8280/30000  training loss: 0.2088361233472824 Validation loss 0.20521388947963715\n",
      "Epoch 8290/30000  training loss: 0.20869693160057068 Validation loss 0.205076202750206\n",
      "Epoch 8300/30000  training loss: 0.20855772495269775 Validation loss 0.20493842661380768\n",
      "Epoch 8310/30000  training loss: 0.20841842889785767 Validation loss 0.2048005759716034\n",
      "Epoch 8320/30000  training loss: 0.2082790732383728 Validation loss 0.2046627253293991\n",
      "Epoch 8330/30000  training loss: 0.2081395983695984 Validation loss 0.2045247107744217\n",
      "Epoch 8340/30000  training loss: 0.2080000340938568 Validation loss 0.2043866217136383\n",
      "Epoch 8350/30000  training loss: 0.20786044001579285 Validation loss 0.20424850285053253\n",
      "Epoch 8360/30000  training loss: 0.2077207714319229 Validation loss 0.20411038398742676\n",
      "Epoch 8370/30000  training loss: 0.2075810730457306 Validation loss 0.20397216081619263\n",
      "Epoch 8380/30000  training loss: 0.2074413150548935 Validation loss 0.20383389294147491\n",
      "Epoch 8390/30000  training loss: 0.20730146765708923 Validation loss 0.20369555056095123\n",
      "Epoch 8400/30000  training loss: 0.20716162025928497 Validation loss 0.20355722308158875\n",
      "Epoch 8410/30000  training loss: 0.2070215940475464 Validation loss 0.20341870188713074\n",
      "Epoch 8420/30000  training loss: 0.20688149333000183 Validation loss 0.20328010618686676\n",
      "Epoch 8430/30000  training loss: 0.2067413628101349 Validation loss 0.20314152538776398\n",
      "Epoch 8440/30000  training loss: 0.20660120248794556 Validation loss 0.20300288498401642\n",
      "Epoch 8450/30000  training loss: 0.20646105706691742 Validation loss 0.20286425948143005\n",
      "Epoch 8460/30000  training loss: 0.20632077753543854 Validation loss 0.20272555947303772\n",
      "Epoch 8470/30000  training loss: 0.20618049800395966 Validation loss 0.2025868147611618\n",
      "Epoch 8480/30000  training loss: 0.206040158867836 Validation loss 0.2024480104446411\n",
      "Epoch 8490/30000  training loss: 0.2058996558189392 Validation loss 0.2023090422153473\n",
      "Epoch 8500/30000  training loss: 0.20575910806655884 Validation loss 0.20217005908489227\n",
      "Epoch 8510/30000  training loss: 0.2056184858083725 Validation loss 0.2020310014486313\n",
      "Epoch 8520/30000  training loss: 0.20547783374786377 Validation loss 0.2018919587135315\n",
      "Epoch 8530/30000  training loss: 0.20533715188503265 Validation loss 0.20175282657146454\n",
      "Epoch 8540/30000  training loss: 0.20519645512104034 Validation loss 0.20161370933055878\n",
      "Epoch 8550/30000  training loss: 0.20505571365356445 Validation loss 0.20147454738616943\n",
      "Epoch 8560/30000  training loss: 0.20491479337215424 Validation loss 0.20133522152900696\n",
      "Epoch 8570/30000  training loss: 0.20477384328842163 Validation loss 0.2011958509683609\n",
      "Epoch 8580/30000  training loss: 0.20463284850120544 Validation loss 0.20105645060539246\n",
      "Epoch 8590/30000  training loss: 0.20449180901050568 Validation loss 0.20091702044010162\n",
      "Epoch 8600/30000  training loss: 0.20435073971748352 Validation loss 0.2007775753736496\n",
      "Epoch 8610/30000  training loss: 0.2042095959186554 Validation loss 0.2006380409002304\n",
      "Epoch 8620/30000  training loss: 0.2040684074163437 Validation loss 0.20049846172332764\n",
      "Epoch 8630/30000  training loss: 0.2039271742105484 Validation loss 0.20035886764526367\n",
      "Epoch 8640/30000  training loss: 0.20378586649894714 Validation loss 0.20021916925907135\n",
      "Epoch 8650/30000  training loss: 0.20364443957805634 Validation loss 0.20007936656475067\n",
      "Epoch 8660/30000  training loss: 0.20350296795368195 Validation loss 0.1999395340681076\n",
      "Epoch 8670/30000  training loss: 0.20336148142814636 Validation loss 0.19979971647262573\n",
      "Epoch 8680/30000  training loss: 0.20321998000144958 Validation loss 0.19965985417366028\n",
      "Epoch 8690/30000  training loss: 0.20307841897010803 Validation loss 0.19951996207237244\n",
      "Epoch 8700/30000  training loss: 0.2029368281364441 Validation loss 0.19938001036643982\n",
      "Epoch 8710/30000  training loss: 0.20279517769813538 Validation loss 0.19924002885818481\n",
      "Epoch 8720/30000  training loss: 0.2026534080505371 Validation loss 0.19909991323947906\n",
      "Epoch 8730/30000  training loss: 0.20251157879829407 Validation loss 0.19895978271961212\n",
      "Epoch 8740/30000  training loss: 0.20236973464488983 Validation loss 0.1988196223974228\n",
      "Epoch 8750/30000  training loss: 0.20222784578800201 Validation loss 0.19867940247058868\n",
      "Epoch 8760/30000  training loss: 0.20208591222763062 Validation loss 0.19853916764259338\n",
      "Epoch 8770/30000  training loss: 0.20194396376609802 Validation loss 0.1983989030122757\n",
      "Epoch 8780/30000  training loss: 0.20180200040340424 Validation loss 0.198258638381958\n",
      "Epoch 8790/30000  training loss: 0.20166002213954926 Validation loss 0.19811835885047913\n",
      "Epoch 8800/30000  training loss: 0.20151787996292114 Validation loss 0.19797790050506592\n",
      "Epoch 8810/30000  training loss: 0.20137564837932587 Validation loss 0.19783741235733032\n",
      "Epoch 8820/30000  training loss: 0.2012334167957306 Validation loss 0.19769689440727234\n",
      "Epoch 8830/30000  training loss: 0.20109117031097412 Validation loss 0.19755639135837555\n",
      "Epoch 8840/30000  training loss: 0.20094890892505646 Validation loss 0.19741585850715637\n",
      "Epoch 8850/30000  training loss: 0.20080657303333282 Validation loss 0.19727522134780884\n",
      "Epoch 8860/30000  training loss: 0.2006641924381256 Validation loss 0.1971346139907837\n",
      "Epoch 8870/30000  training loss: 0.2005218118429184 Validation loss 0.19699396193027496\n",
      "Epoch 8880/30000  training loss: 0.2003793567419052 Validation loss 0.19685325026512146\n",
      "Epoch 8890/30000  training loss: 0.20023682713508606 Validation loss 0.196712464094162\n",
      "Epoch 8900/30000  training loss: 0.20009425282478333 Validation loss 0.19657164812088013\n",
      "Epoch 8910/30000  training loss: 0.19995161890983582 Validation loss 0.1964307725429535\n",
      "Epoch 8920/30000  training loss: 0.1998089849948883 Validation loss 0.19628991186618805\n",
      "Epoch 8930/30000  training loss: 0.1996663212776184 Validation loss 0.1961490362882614\n",
      "Epoch 8940/30000  training loss: 0.19952362775802612 Validation loss 0.1960081309080124\n",
      "Epoch 8950/30000  training loss: 0.19938090443611145 Validation loss 0.19586718082427979\n",
      "Epoch 8960/30000  training loss: 0.1992381066083908 Validation loss 0.19572614133358002\n",
      "Epoch 8970/30000  training loss: 0.19909518957138062 Validation loss 0.19558507204055786\n",
      "Epoch 8980/30000  training loss: 0.19895227253437042 Validation loss 0.19544395804405212\n",
      "Epoch 8990/30000  training loss: 0.19880937039852142 Validation loss 0.1953028440475464\n",
      "Epoch 9000/30000  training loss: 0.19866640865802765 Validation loss 0.19516170024871826\n",
      "Epoch 9010/30000  training loss: 0.1985234022140503 Validation loss 0.19502049684524536\n",
      "Epoch 9020/30000  training loss: 0.19838038086891174 Validation loss 0.19487929344177246\n",
      "Epoch 9030/30000  training loss: 0.19823729991912842 Validation loss 0.19473807513713837\n",
      "Epoch 9040/30000  training loss: 0.19809426367282867 Validation loss 0.19459682703018188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9050/30000  training loss: 0.19795109331607819 Validation loss 0.19445548951625824\n",
      "Epoch 9060/30000  training loss: 0.19780781865119934 Validation loss 0.19431406259536743\n",
      "Epoch 9070/30000  training loss: 0.19766458868980408 Validation loss 0.1941726803779602\n",
      "Epoch 9080/30000  training loss: 0.19752132892608643 Validation loss 0.19403129816055298\n",
      "Epoch 9090/30000  training loss: 0.19737809896469116 Validation loss 0.19388991594314575\n",
      "Epoch 9100/30000  training loss: 0.19723482429981232 Validation loss 0.19374850392341614\n",
      "Epoch 9110/30000  training loss: 0.19709156453609467 Validation loss 0.19360709190368652\n",
      "Epoch 9120/30000  training loss: 0.19694820046424866 Validation loss 0.19346565008163452\n",
      "Epoch 9130/30000  training loss: 0.19680479168891907 Validation loss 0.19332407414913177\n",
      "Epoch 9140/30000  training loss: 0.1966613084077835 Validation loss 0.19318248331546783\n",
      "Epoch 9150/30000  training loss: 0.19651785492897034 Validation loss 0.1930408626794815\n",
      "Epoch 9160/30000  training loss: 0.1963743418455124 Validation loss 0.19289927184581757\n",
      "Epoch 9170/30000  training loss: 0.19623075425624847 Validation loss 0.19275760650634766\n",
      "Epoch 9180/30000  training loss: 0.19608719646930695 Validation loss 0.19261595606803894\n",
      "Epoch 9190/30000  training loss: 0.19594362378120422 Validation loss 0.19247429072856903\n",
      "Epoch 9200/30000  training loss: 0.1958000510931015 Validation loss 0.19233262538909912\n",
      "Epoch 9210/30000  training loss: 0.19565650820732117 Validation loss 0.1921909898519516\n",
      "Epoch 9220/30000  training loss: 0.1955128163099289 Validation loss 0.19204919040203094\n",
      "Epoch 9230/30000  training loss: 0.19536904990673065 Validation loss 0.1919073611497879\n",
      "Epoch 9240/30000  training loss: 0.1952253133058548 Validation loss 0.19176557660102844\n",
      "Epoch 9250/30000  training loss: 0.19508154690265656 Validation loss 0.1916237622499466\n",
      "Epoch 9260/30000  training loss: 0.1949378103017807 Validation loss 0.19148194789886475\n",
      "Epoch 9270/30000  training loss: 0.19479404389858246 Validation loss 0.1913401484489441\n",
      "Epoch 9280/30000  training loss: 0.19465021789073944 Validation loss 0.19119828939437866\n",
      "Epoch 9290/30000  training loss: 0.19450636208057404 Validation loss 0.19105640053749084\n",
      "Epoch 9300/30000  training loss: 0.1943625658750534 Validation loss 0.19091452658176422\n",
      "Epoch 9310/30000  training loss: 0.19421863555908203 Validation loss 0.19077257812023163\n",
      "Epoch 9320/30000  training loss: 0.19407470524311066 Validation loss 0.19063061475753784\n",
      "Epoch 9330/30000  training loss: 0.1939307600259781 Validation loss 0.19048865139484406\n",
      "Epoch 9340/30000  training loss: 0.19378678500652313 Validation loss 0.19034665822982788\n",
      "Epoch 9350/30000  training loss: 0.19364279508590698 Validation loss 0.1902046799659729\n",
      "Epoch 9360/30000  training loss: 0.19349882006645203 Validation loss 0.19006267189979553\n",
      "Epoch 9370/30000  training loss: 0.19335483014583588 Validation loss 0.18992070853710175\n",
      "Epoch 9380/30000  training loss: 0.19321085512638092 Validation loss 0.18977874517440796\n",
      "Epoch 9390/30000  training loss: 0.19306688010692596 Validation loss 0.18963676691055298\n",
      "Epoch 9400/30000  training loss: 0.19292272627353668 Validation loss 0.18949462473392487\n",
      "Epoch 9410/30000  training loss: 0.1927785873413086 Validation loss 0.18935251235961914\n",
      "Epoch 9420/30000  training loss: 0.1926344633102417 Validation loss 0.1892104148864746\n",
      "Epoch 9430/30000  training loss: 0.19249030947685242 Validation loss 0.18906830251216888\n",
      "Epoch 9440/30000  training loss: 0.19234615564346313 Validation loss 0.18892619013786316\n",
      "Epoch 9450/30000  training loss: 0.19220206141471863 Validation loss 0.1887841671705246\n",
      "Epoch 9460/30000  training loss: 0.19205793738365173 Validation loss 0.18864206969738007\n",
      "Epoch 9470/30000  training loss: 0.19191379845142365 Validation loss 0.18849997222423553\n",
      "Epoch 9480/30000  training loss: 0.19176961481571198 Validation loss 0.18835783004760742\n",
      "Epoch 9490/30000  training loss: 0.19162534177303314 Validation loss 0.18821561336517334\n",
      "Epoch 9500/30000  training loss: 0.1914810687303543 Validation loss 0.18807339668273926\n",
      "Epoch 9510/30000  training loss: 0.19133684039115906 Validation loss 0.18793126940727234\n",
      "Epoch 9520/30000  training loss: 0.1911926120519638 Validation loss 0.18778908252716064\n",
      "Epoch 9530/30000  training loss: 0.19104830920696259 Validation loss 0.18764688074588776\n",
      "Epoch 9540/30000  training loss: 0.19090402126312256 Validation loss 0.18750469386577606\n",
      "Epoch 9550/30000  training loss: 0.1907597929239273 Validation loss 0.18736253678798676\n",
      "Epoch 9560/30000  training loss: 0.19061553478240967 Validation loss 0.18722039461135864\n",
      "Epoch 9570/30000  training loss: 0.1904713213443756 Validation loss 0.18707828223705292\n",
      "Epoch 9580/30000  training loss: 0.190326988697052 Validation loss 0.18693605065345764\n",
      "Epoch 9590/30000  training loss: 0.19018259644508362 Validation loss 0.18679377436637878\n",
      "Epoch 9600/30000  training loss: 0.19003821909427643 Validation loss 0.18665152788162231\n",
      "Epoch 9610/30000  training loss: 0.18989385664463043 Validation loss 0.18650928139686584\n",
      "Epoch 9620/30000  training loss: 0.18974953889846802 Validation loss 0.18636706471443176\n",
      "Epoch 9630/30000  training loss: 0.18960517644882202 Validation loss 0.18622490763664246\n",
      "Epoch 9640/30000  training loss: 0.189460888504982 Validation loss 0.18608275055885315\n",
      "Epoch 9650/30000  training loss: 0.1893165558576584 Validation loss 0.18594054877758026\n",
      "Epoch 9660/30000  training loss: 0.18917223811149597 Validation loss 0.18579834699630737\n",
      "Epoch 9670/30000  training loss: 0.18902787566184998 Validation loss 0.18565616011619568\n",
      "Epoch 9680/30000  training loss: 0.1888834536075592 Validation loss 0.18551389873027802\n",
      "Epoch 9690/30000  training loss: 0.18873903155326843 Validation loss 0.18537165224552155\n",
      "Epoch 9700/30000  training loss: 0.18859463930130005 Validation loss 0.18522942066192627\n",
      "Epoch 9710/30000  training loss: 0.18845027685165405 Validation loss 0.18508724868297577\n",
      "Epoch 9720/30000  training loss: 0.18830586969852448 Validation loss 0.1849450320005417\n",
      "Epoch 9730/30000  training loss: 0.18816149234771729 Validation loss 0.1848028153181076\n",
      "Epoch 9740/30000  training loss: 0.1880171000957489 Validation loss 0.1846606284379959\n",
      "Epoch 9750/30000  training loss: 0.1878727674484253 Validation loss 0.1845184713602066\n",
      "Epoch 9760/30000  training loss: 0.18772846460342407 Validation loss 0.18437640368938446\n",
      "Epoch 9770/30000  training loss: 0.18758413195610046 Validation loss 0.18423424661159515\n",
      "Epoch 9780/30000  training loss: 0.1874397099018097 Validation loss 0.18409204483032227\n",
      "Epoch 9790/30000  training loss: 0.18729527294635773 Validation loss 0.183949813246727\n",
      "Epoch 9800/30000  training loss: 0.18715086579322815 Validation loss 0.1838076263666153\n",
      "Epoch 9810/30000  training loss: 0.18700644373893738 Validation loss 0.1836654543876648\n",
      "Epoch 9820/30000  training loss: 0.1868620663881302 Validation loss 0.1835232973098755\n",
      "Epoch 9830/30000  training loss: 0.18671773374080658 Validation loss 0.18338122963905334\n",
      "Epoch 9840/30000  training loss: 0.18657343089580536 Validation loss 0.18323914706707\n",
      "Epoch 9850/30000  training loss: 0.18642914295196533 Validation loss 0.18309709429740906\n",
      "Epoch 9860/30000  training loss: 0.18628478050231934 Validation loss 0.18295501172542572\n",
      "Epoch 9870/30000  training loss: 0.1861405074596405 Validation loss 0.18281298875808716\n",
      "Epoch 9880/30000  training loss: 0.18599611520767212 Validation loss 0.18267086148262024\n",
      "Epoch 9890/30000  training loss: 0.18585173785686493 Validation loss 0.18252873420715332\n",
      "Epoch 9900/30000  training loss: 0.18570737540721893 Validation loss 0.18238666653633118\n",
      "Epoch 9910/30000  training loss: 0.1855630874633789 Validation loss 0.18224464356899261\n",
      "Epoch 9920/30000  training loss: 0.18541878461837769 Validation loss 0.18210263550281525\n",
      "Epoch 9930/30000  training loss: 0.18527449667453766 Validation loss 0.18196064233779907\n",
      "Epoch 9940/30000  training loss: 0.18513023853302002 Validation loss 0.18181867897510529\n",
      "Epoch 9950/30000  training loss: 0.18498599529266357 Validation loss 0.1816766858100891\n",
      "Epoch 9960/30000  training loss: 0.18484173715114594 Validation loss 0.1815347671508789\n",
      "Epoch 9970/30000  training loss: 0.18469755351543427 Validation loss 0.18139290809631348\n",
      "Epoch 9980/30000  training loss: 0.1845533400774002 Validation loss 0.18125100433826447\n",
      "Epoch 9990/30000  training loss: 0.18440909683704376 Validation loss 0.18110905587673187\n",
      "Epoch 10000/30000  training loss: 0.18426485359668732 Validation loss 0.18096713721752167\n",
      "Epoch 10010/30000  training loss: 0.18412065505981445 Validation loss 0.18082526326179504\n",
      "Epoch 10020/30000  training loss: 0.1839764267206192 Validation loss 0.18068338930606842\n",
      "Epoch 10030/30000  training loss: 0.18383222818374634 Validation loss 0.1805415153503418\n",
      "Epoch 10040/30000  training loss: 0.18368810415267944 Validation loss 0.18039973080158234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10050/30000  training loss: 0.18354398012161255 Validation loss 0.18025796115398407\n",
      "Epoch 10060/30000  training loss: 0.18339988589286804 Validation loss 0.18011625111103058\n",
      "Epoch 10070/30000  training loss: 0.1832558661699295 Validation loss 0.17997458577156067\n",
      "Epoch 10080/30000  training loss: 0.18311187624931335 Validation loss 0.17983295023441315\n",
      "Epoch 10090/30000  training loss: 0.18296778202056885 Validation loss 0.17969121038913727\n",
      "Epoch 10100/30000  training loss: 0.18282368779182434 Validation loss 0.17954950034618378\n",
      "Epoch 10110/30000  training loss: 0.18267962336540222 Validation loss 0.17940780520439148\n",
      "Epoch 10120/30000  training loss: 0.1825355589389801 Validation loss 0.17926612496376038\n",
      "Epoch 10130/30000  training loss: 0.18239155411720276 Validation loss 0.17912453413009644\n",
      "Epoch 10140/30000  training loss: 0.182247593998909 Validation loss 0.1789829432964325\n",
      "Epoch 10150/30000  training loss: 0.18210366368293762 Validation loss 0.17884142696857452\n",
      "Epoch 10160/30000  training loss: 0.18195979297161102 Validation loss 0.17869997024536133\n",
      "Epoch 10170/30000  training loss: 0.1818159520626068 Validation loss 0.17855852842330933\n",
      "Epoch 10180/30000  training loss: 0.1816720962524414 Validation loss 0.17841708660125732\n",
      "Epoch 10190/30000  training loss: 0.18152830004692078 Validation loss 0.1782757043838501\n",
      "Epoch 10200/30000  training loss: 0.18138448894023895 Validation loss 0.17813430726528168\n",
      "Epoch 10210/30000  training loss: 0.18124064803123474 Validation loss 0.17799286544322968\n",
      "Epoch 10220/30000  training loss: 0.18109679222106934 Validation loss 0.17785148322582245\n",
      "Epoch 10230/30000  training loss: 0.1809530109167099 Validation loss 0.17771016061306\n",
      "Epoch 10240/30000  training loss: 0.18080928921699524 Validation loss 0.17756886780261993\n",
      "Epoch 10250/30000  training loss: 0.18066561222076416 Validation loss 0.17742763459682465\n",
      "Epoch 10260/30000  training loss: 0.18052196502685547 Validation loss 0.17728646099567413\n",
      "Epoch 10270/30000  training loss: 0.18037831783294678 Validation loss 0.17714525759220123\n",
      "Epoch 10280/30000  training loss: 0.18023468554019928 Validation loss 0.1770041286945343\n",
      "Epoch 10290/30000  training loss: 0.18009115755558014 Validation loss 0.17686304450035095\n",
      "Epoch 10300/30000  training loss: 0.179947629570961 Validation loss 0.17672199010849\n",
      "Epoch 10310/30000  training loss: 0.17980416119098663 Validation loss 0.1765809953212738\n",
      "Epoch 10320/30000  training loss: 0.17966070771217346 Validation loss 0.1764400452375412\n",
      "Epoch 10330/30000  training loss: 0.17951719462871552 Validation loss 0.17629899084568024\n",
      "Epoch 10340/30000  training loss: 0.17937377095222473 Validation loss 0.17615804076194763\n",
      "Epoch 10350/30000  training loss: 0.17923034727573395 Validation loss 0.1760171502828598\n",
      "Epoch 10360/30000  training loss: 0.17908699810504913 Validation loss 0.17587627470493317\n",
      "Epoch 10370/30000  training loss: 0.17894363403320312 Validation loss 0.17573542892932892\n",
      "Epoch 10380/30000  training loss: 0.1788002997636795 Validation loss 0.17559459805488586\n",
      "Epoch 10390/30000  training loss: 0.17865705490112305 Validation loss 0.17545388638973236\n",
      "Epoch 10400/30000  training loss: 0.1785137951374054 Validation loss 0.17531317472457886\n",
      "Epoch 10410/30000  training loss: 0.1783706396818161 Validation loss 0.17517255246639252\n",
      "Epoch 10420/30000  training loss: 0.17822754383087158 Validation loss 0.17503196001052856\n",
      "Epoch 10430/30000  training loss: 0.17808449268341064 Validation loss 0.17489145696163177\n",
      "Epoch 10440/30000  training loss: 0.1779414862394333 Validation loss 0.17475098371505737\n",
      "Epoch 10450/30000  training loss: 0.17779846489429474 Validation loss 0.17461049556732178\n",
      "Epoch 10460/30000  training loss: 0.177655428647995 Validation loss 0.17447005212306976\n",
      "Epoch 10470/30000  training loss: 0.17751243710517883 Validation loss 0.17432957887649536\n",
      "Epoch 10480/30000  training loss: 0.17736947536468506 Validation loss 0.17418915033340454\n",
      "Epoch 10490/30000  training loss: 0.17722652852535248 Validation loss 0.17404881119728088\n",
      "Epoch 10500/30000  training loss: 0.17708365619182587 Validation loss 0.1739085167646408\n",
      "Epoch 10510/30000  training loss: 0.17694084346294403 Validation loss 0.17376826703548431\n",
      "Epoch 10520/30000  training loss: 0.17679809033870697 Validation loss 0.17362810671329498\n",
      "Epoch 10530/30000  training loss: 0.17665539681911469 Validation loss 0.17348800599575043\n",
      "Epoch 10540/30000  training loss: 0.17651276290416718 Validation loss 0.17334796488285065\n",
      "Epoch 10550/30000  training loss: 0.17637020349502563 Validation loss 0.17320796847343445\n",
      "Epoch 10560/30000  training loss: 0.1762276589870453 Validation loss 0.17306804656982422\n",
      "Epoch 10570/30000  training loss: 0.1760852038860321 Validation loss 0.17292818427085876\n",
      "Epoch 10580/30000  training loss: 0.1759428083896637 Validation loss 0.17278841137886047\n",
      "Epoch 10590/30000  training loss: 0.17580029368400574 Validation loss 0.17264848947525024\n",
      "Epoch 10600/30000  training loss: 0.17565780878067017 Validation loss 0.1725086271762848\n",
      "Epoch 10610/30000  training loss: 0.17551539838314056 Validation loss 0.1723688393831253\n",
      "Epoch 10620/30000  training loss: 0.17537303268909454 Validation loss 0.1722291111946106\n",
      "Epoch 10630/30000  training loss: 0.17523077130317688 Validation loss 0.17208945751190186\n",
      "Epoch 10640/30000  training loss: 0.17508850991725922 Validation loss 0.1719498485326767\n",
      "Epoch 10650/30000  training loss: 0.1749463677406311 Validation loss 0.1718103438615799\n",
      "Epoch 10660/30000  training loss: 0.17480424046516418 Validation loss 0.17167088389396667\n",
      "Epoch 10670/30000  training loss: 0.17466221749782562 Validation loss 0.17153151333332062\n",
      "Epoch 10680/30000  training loss: 0.17452023923397064 Validation loss 0.17139217257499695\n",
      "Epoch 10690/30000  training loss: 0.17437833547592163 Validation loss 0.17125295102596283\n",
      "Epoch 10700/30000  training loss: 0.1742364913225174 Validation loss 0.1711137443780899\n",
      "Epoch 10710/30000  training loss: 0.17409470677375793 Validation loss 0.17097465693950653\n",
      "Epoch 10720/30000  training loss: 0.17395296692848206 Validation loss 0.17083555459976196\n",
      "Epoch 10730/30000  training loss: 0.1738111972808838 Validation loss 0.17069648206233978\n",
      "Epoch 10740/30000  training loss: 0.17366941273212433 Validation loss 0.17055736482143402\n",
      "Epoch 10750/30000  training loss: 0.17352771759033203 Validation loss 0.1704183667898178\n",
      "Epoch 10760/30000  training loss: 0.17338606715202332 Validation loss 0.17027941346168518\n",
      "Epoch 10770/30000  training loss: 0.17324450612068176 Validation loss 0.17014053463935852\n",
      "Epoch 10780/30000  training loss: 0.1731029748916626 Validation loss 0.17000173032283783\n",
      "Epoch 10790/30000  training loss: 0.17296156287193298 Validation loss 0.1698630154132843\n",
      "Epoch 10800/30000  training loss: 0.17282019555568695 Validation loss 0.16972436010837555\n",
      "Epoch 10810/30000  training loss: 0.1726789027452469 Validation loss 0.16958576440811157\n",
      "Epoch 10820/30000  training loss: 0.1725376695394516 Validation loss 0.16944727301597595\n",
      "Epoch 10830/30000  training loss: 0.17239652574062347 Validation loss 0.1693088412284851\n",
      "Epoch 10840/30000  training loss: 0.17225545644760132 Validation loss 0.16917051374912262\n",
      "Epoch 10850/30000  training loss: 0.17211443185806274 Validation loss 0.16903221607208252\n",
      "Epoch 10860/30000  training loss: 0.17197351157665253 Validation loss 0.16889402270317078\n",
      "Epoch 10870/30000  training loss: 0.1718326210975647 Validation loss 0.16875587403774261\n",
      "Epoch 10880/30000  training loss: 0.17169176042079926 Validation loss 0.16861778497695923\n",
      "Epoch 10890/30000  training loss: 0.1715509444475174 Validation loss 0.16847966611385345\n",
      "Epoch 10900/30000  training loss: 0.17141008377075195 Validation loss 0.16834159195423126\n",
      "Epoch 10910/30000  training loss: 0.17126931250095367 Validation loss 0.16820359230041504\n",
      "Epoch 10920/30000  training loss: 0.17112863063812256 Validation loss 0.1680656373500824\n",
      "Epoch 10930/30000  training loss: 0.17098800837993622 Validation loss 0.1679278016090393\n",
      "Epoch 10940/30000  training loss: 0.17084746062755585 Validation loss 0.16779004037380219\n",
      "Epoch 10950/30000  training loss: 0.17070700228214264 Validation loss 0.16765236854553223\n",
      "Epoch 10960/30000  training loss: 0.1705666184425354 Validation loss 0.16751475632190704\n",
      "Epoch 10970/30000  training loss: 0.17042629420757294 Validation loss 0.16737723350524902\n",
      "Epoch 10980/30000  training loss: 0.17028607428073883 Validation loss 0.16723978519439697\n",
      "Epoch 10990/30000  training loss: 0.1701459139585495 Validation loss 0.1671024113893509\n",
      "Epoch 11000/30000  training loss: 0.17000582814216614 Validation loss 0.16696515679359436\n",
      "Epoch 11010/30000  training loss: 0.16986581683158875 Validation loss 0.1668279469013214\n",
      "Epoch 11020/30000  training loss: 0.1697259098291397 Validation loss 0.16669084131717682\n",
      "Epoch 11030/30000  training loss: 0.16958607733249664 Validation loss 0.1665538102388382\n",
      "Epoch 11040/30000  training loss: 0.16944630444049835 Validation loss 0.16641685366630554\n",
      "Epoch 11050/30000  training loss: 0.16930663585662842 Validation loss 0.16627998650074005\n",
      "Epoch 11060/30000  training loss: 0.16916704177856445 Validation loss 0.16614322364330292\n",
      "Epoch 11070/30000  training loss: 0.1690274178981781 Validation loss 0.1660064458847046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11080/30000  training loss: 0.16888777911663055 Validation loss 0.16586963832378387\n",
      "Epoch 11090/30000  training loss: 0.16874822974205017 Validation loss 0.16573293507099152\n",
      "Epoch 11100/30000  training loss: 0.16860873997211456 Validation loss 0.16559629142284393\n",
      "Epoch 11110/30000  training loss: 0.1684693694114685 Validation loss 0.1654597371816635\n",
      "Epoch 11120/30000  training loss: 0.16833007335662842 Validation loss 0.16532328724861145\n",
      "Epoch 11130/30000  training loss: 0.1681908518075943 Validation loss 0.16518692672252655\n",
      "Epoch 11140/30000  training loss: 0.16805168986320496 Validation loss 0.16505064070224762\n",
      "Epoch 11150/30000  training loss: 0.16791264712810516 Validation loss 0.16491444408893585\n",
      "Epoch 11160/30000  training loss: 0.16777366399765015 Validation loss 0.16477833688259125\n",
      "Epoch 11170/30000  training loss: 0.1676347851753235 Validation loss 0.164642333984375\n",
      "Epoch 11180/30000  training loss: 0.167495995759964 Validation loss 0.16450640559196472\n",
      "Epoch 11190/30000  training loss: 0.16735728085041046 Validation loss 0.1643705815076828\n",
      "Epoch 11200/30000  training loss: 0.1672186404466629 Validation loss 0.16423483192920685\n",
      "Epoch 11210/30000  training loss: 0.1670801192522049 Validation loss 0.16409918665885925\n",
      "Epoch 11220/30000  training loss: 0.16694165766239166 Validation loss 0.16396361589431763\n",
      "Epoch 11230/30000  training loss: 0.1668033003807068 Validation loss 0.16382814943790436\n",
      "Epoch 11240/30000  training loss: 0.16666501760482788 Validation loss 0.16369278728961945\n",
      "Epoch 11250/30000  training loss: 0.16652682423591614 Validation loss 0.1635575145483017\n",
      "Epoch 11260/30000  training loss: 0.16638873517513275 Validation loss 0.16342230141162872\n",
      "Epoch 11270/30000  training loss: 0.16625072062015533 Validation loss 0.1632872223854065\n",
      "Epoch 11280/30000  training loss: 0.1661127507686615 Validation loss 0.16315212845802307\n",
      "Epoch 11290/30000  training loss: 0.16597476601600647 Validation loss 0.16301706433296204\n",
      "Epoch 11300/30000  training loss: 0.16583691537380219 Validation loss 0.16288213431835175\n",
      "Epoch 11310/30000  training loss: 0.16569912433624268 Validation loss 0.16274727880954742\n",
      "Epoch 11320/30000  training loss: 0.16556140780448914 Validation loss 0.16261248290538788\n",
      "Epoch 11330/30000  training loss: 0.16542384028434753 Validation loss 0.16247785091400146\n",
      "Epoch 11340/30000  training loss: 0.16528630256652832 Validation loss 0.16234327852725983\n",
      "Epoch 11350/30000  training loss: 0.16514889895915985 Validation loss 0.16220881044864655\n",
      "Epoch 11360/30000  training loss: 0.16501156985759735 Validation loss 0.16207443177700043\n",
      "Epoch 11370/30000  training loss: 0.16487433016300201 Validation loss 0.16194014251232147\n",
      "Epoch 11380/30000  training loss: 0.16473720967769623 Validation loss 0.16180595755577087\n",
      "Epoch 11390/30000  training loss: 0.16460014879703522 Validation loss 0.16167187690734863\n",
      "Epoch 11400/30000  training loss: 0.16446320712566376 Validation loss 0.16153788566589355\n",
      "Epoch 11410/30000  training loss: 0.16432633996009827 Validation loss 0.16140399873256683\n",
      "Epoch 11420/30000  training loss: 0.16418959200382233 Validation loss 0.16127021610736847\n",
      "Epoch 11430/30000  training loss: 0.16405293345451355 Validation loss 0.16113652288913727\n",
      "Epoch 11440/30000  training loss: 0.16391636431217194 Validation loss 0.16100293397903442\n",
      "Epoch 11450/30000  training loss: 0.16377989947795868 Validation loss 0.16086944937705994\n",
      "Epoch 11460/30000  training loss: 0.1636435091495514 Validation loss 0.16073603928089142\n",
      "Epoch 11470/30000  training loss: 0.16350723803043365 Validation loss 0.16060276329517365\n",
      "Epoch 11480/30000  training loss: 0.16337105631828308 Validation loss 0.16046957671642303\n",
      "Epoch 11490/30000  training loss: 0.16323496401309967 Validation loss 0.1603364795446396\n",
      "Epoch 11500/30000  training loss: 0.16309894621372223 Validation loss 0.16020342707633972\n",
      "Epoch 11510/30000  training loss: 0.16296297311782837 Validation loss 0.1600704938173294\n",
      "Epoch 11520/30000  training loss: 0.16282713413238525 Validation loss 0.15993763506412506\n",
      "Epoch 11530/30000  training loss: 0.1626913845539093 Validation loss 0.15980489552021027\n",
      "Epoch 11540/30000  training loss: 0.1625557243824005 Validation loss 0.15967227518558502\n",
      "Epoch 11550/30000  training loss: 0.16242006421089172 Validation loss 0.159539595246315\n",
      "Epoch 11560/30000  training loss: 0.16228444874286652 Validation loss 0.15940703451633453\n",
      "Epoch 11570/30000  training loss: 0.16214896738529205 Validation loss 0.15927456319332123\n",
      "Epoch 11580/30000  training loss: 0.16201357543468475 Validation loss 0.15914218127727509\n",
      "Epoch 11590/30000  training loss: 0.16187827289104462 Validation loss 0.15900996327400208\n",
      "Epoch 11600/30000  training loss: 0.16174310445785522 Validation loss 0.15887778997421265\n",
      "Epoch 11610/30000  training loss: 0.161608025431633 Validation loss 0.15874576568603516\n",
      "Epoch 11620/30000  training loss: 0.16147305071353912 Validation loss 0.15861384570598602\n",
      "Epoch 11630/30000  training loss: 0.16133825480937958 Validation loss 0.15848208963871002\n",
      "Epoch 11640/30000  training loss: 0.1612035185098648 Validation loss 0.15835040807724\n",
      "Epoch 11650/30000  training loss: 0.16106893122196198 Validation loss 0.1582188606262207\n",
      "Epoch 11660/30000  training loss: 0.1609344333410263 Validation loss 0.15808743238449097\n",
      "Epoch 11670/30000  training loss: 0.1608000248670578 Validation loss 0.1579560786485672\n",
      "Epoch 11680/30000  training loss: 0.16066572070121765 Validation loss 0.15782484412193298\n",
      "Epoch 11690/30000  training loss: 0.16053153574466705 Validation loss 0.15769369900226593\n",
      "Epoch 11700/30000  training loss: 0.1603974550962448 Validation loss 0.15756270289421082\n",
      "Epoch 11710/30000  training loss: 0.16026346385478973 Validation loss 0.15743178129196167\n",
      "Epoch 11720/30000  training loss: 0.160129576921463 Validation loss 0.15730100870132446\n",
      "Epoch 11730/30000  training loss: 0.15999580919742584 Validation loss 0.15717031061649323\n",
      "Epoch 11740/30000  training loss: 0.15986214578151703 Validation loss 0.15703973174095154\n",
      "Epoch 11750/30000  training loss: 0.15972861647605896 Validation loss 0.1569092571735382\n",
      "Epoch 11760/30000  training loss: 0.15959514677524567 Validation loss 0.15677890181541443\n",
      "Epoch 11770/30000  training loss: 0.1594618260860443 Validation loss 0.1566486805677414\n",
      "Epoch 11780/30000  training loss: 0.15932857990264893 Validation loss 0.15651850402355194\n",
      "Epoch 11790/30000  training loss: 0.1591954529285431 Validation loss 0.15638849139213562\n",
      "Epoch 11800/30000  training loss: 0.1590624302625656 Validation loss 0.15625858306884766\n",
      "Epoch 11810/30000  training loss: 0.1589295119047165 Validation loss 0.15612876415252686\n",
      "Epoch 11820/30000  training loss: 0.15879672765731812 Validation loss 0.1559990644454956\n",
      "Epoch 11830/30000  training loss: 0.1586640328168869 Validation loss 0.1558694988489151\n",
      "Epoch 11840/30000  training loss: 0.15853144228458405 Validation loss 0.15574003756046295\n",
      "Epoch 11850/30000  training loss: 0.15839897096157074 Validation loss 0.15561068058013916\n",
      "Epoch 11860/30000  training loss: 0.15826661884784698 Validation loss 0.15548144280910492\n",
      "Epoch 11870/30000  training loss: 0.1581343710422516 Validation loss 0.15535230934619904\n",
      "Epoch 11880/30000  training loss: 0.15800222754478455 Validation loss 0.1552233099937439\n",
      "Epoch 11890/30000  training loss: 0.15787020325660706 Validation loss 0.15509441494941711\n",
      "Epoch 11900/30000  training loss: 0.15773828327655792 Validation loss 0.1549656242132187\n",
      "Epoch 11910/30000  training loss: 0.15760648250579834 Validation loss 0.15483695268630981\n",
      "Epoch 11920/30000  training loss: 0.15747478604316711 Validation loss 0.15470841526985168\n",
      "Epoch 11930/30000  training loss: 0.15734319388866425 Validation loss 0.15457996726036072\n",
      "Epoch 11940/30000  training loss: 0.15721173584461212 Validation loss 0.1544516533613205\n",
      "Epoch 11950/30000  training loss: 0.15708038210868835 Validation loss 0.15432344377040863\n",
      "Epoch 11960/30000  training loss: 0.15694914758205414 Validation loss 0.15419535338878632\n",
      "Epoch 11970/30000  training loss: 0.15681800246238708 Validation loss 0.15406739711761475\n",
      "Epoch 11980/30000  training loss: 0.15668699145317078 Validation loss 0.15393953025341034\n",
      "Epoch 11990/30000  training loss: 0.15655608475208282 Validation loss 0.15381179749965668\n",
      "Epoch 12000/30000  training loss: 0.1564253270626068 Validation loss 0.15368415415287018\n",
      "Epoch 12010/30000  training loss: 0.15629464387893677 Validation loss 0.1535566747188568\n",
      "Epoch 12020/30000  training loss: 0.1561639904975891 Validation loss 0.15342918038368225\n",
      "Epoch 12030/30000  training loss: 0.15603342652320862 Validation loss 0.15330179035663605\n",
      "Epoch 12040/30000  training loss: 0.15590296685695648 Validation loss 0.1531745195388794\n",
      "Epoch 12050/30000  training loss: 0.15577268600463867 Validation loss 0.15304742753505707\n",
      "Epoch 12060/30000  training loss: 0.1556425243616104 Validation loss 0.1529204398393631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12070/30000  training loss: 0.1555124968290329 Validation loss 0.15279357135295868\n",
      "Epoch 12080/30000  training loss: 0.15538257360458374 Validation loss 0.1526668518781662\n",
      "Epoch 12090/30000  training loss: 0.15525275468826294 Validation loss 0.15254023671150208\n",
      "Epoch 12100/30000  training loss: 0.1551230549812317 Validation loss 0.15241371095180511\n",
      "Epoch 12110/30000  training loss: 0.15499348938465118 Validation loss 0.15228736400604248\n",
      "Epoch 12120/30000  training loss: 0.15486402809619904 Validation loss 0.152161106467247\n",
      "Epoch 12130/30000  training loss: 0.15473470091819763 Validation loss 0.1520349681377411\n",
      "Epoch 12140/30000  training loss: 0.15460547804832458 Validation loss 0.15190894901752472\n",
      "Epoch 12150/30000  training loss: 0.15447638928890228 Validation loss 0.1517830491065979\n",
      "Epoch 12160/30000  training loss: 0.15434738993644714 Validation loss 0.15165728330612183\n",
      "Epoch 12170/30000  training loss: 0.15421852469444275 Validation loss 0.1515316218137741\n",
      "Epoch 12180/30000  training loss: 0.1540897786617279 Validation loss 0.15140609443187714\n",
      "Epoch 12190/30000  training loss: 0.153961181640625 Validation loss 0.1512807011604309\n",
      "Epoch 12200/30000  training loss: 0.15383267402648926 Validation loss 0.15115542709827423\n",
      "Epoch 12210/30000  training loss: 0.15370428562164307 Validation loss 0.1510302573442459\n",
      "Epoch 12220/30000  training loss: 0.15357601642608643 Validation loss 0.15090523660182953\n",
      "Epoch 12230/30000  training loss: 0.15344789624214172 Validation loss 0.1507803052663803\n",
      "Epoch 12240/30000  training loss: 0.15331986546516418 Validation loss 0.15065555274486542\n",
      "Epoch 12250/30000  training loss: 0.15319205820560455 Validation loss 0.15053096413612366\n",
      "Epoch 12260/30000  training loss: 0.15306434035301208 Validation loss 0.15040650963783264\n",
      "Epoch 12270/30000  training loss: 0.15293675661087036 Validation loss 0.1502821445465088\n",
      "Epoch 12280/30000  training loss: 0.1528092920780182 Validation loss 0.15015791356563568\n",
      "Epoch 12290/30000  training loss: 0.15268194675445557 Validation loss 0.15003381669521332\n",
      "Epoch 12300/30000  training loss: 0.1525547355413437 Validation loss 0.1499098539352417\n",
      "Epoch 12310/30000  training loss: 0.15242764353752136 Validation loss 0.14978599548339844\n",
      "Epoch 12320/30000  training loss: 0.1523006707429886 Validation loss 0.14966228604316711\n",
      "Epoch 12330/30000  training loss: 0.15217380225658417 Validation loss 0.14953868091106415\n",
      "Epoch 12340/30000  training loss: 0.1520470827817917 Validation loss 0.14941522479057312\n",
      "Epoch 12350/30000  training loss: 0.15192049741744995 Validation loss 0.14929185807704926\n",
      "Epoch 12360/30000  training loss: 0.15179400146007538 Validation loss 0.14916865527629852\n",
      "Epoch 12370/30000  training loss: 0.15166766941547394 Validation loss 0.14904555678367615\n",
      "Epoch 12380/30000  training loss: 0.15154142677783966 Validation loss 0.1489226073026657\n",
      "Epoch 12390/30000  training loss: 0.15141531825065613 Validation loss 0.14879979193210602\n",
      "Epoch 12400/30000  training loss: 0.1512894183397293 Validation loss 0.14867712557315826\n",
      "Epoch 12410/30000  training loss: 0.15116363763809204 Validation loss 0.14855460822582245\n",
      "Epoch 12420/30000  training loss: 0.15103797614574432 Validation loss 0.14843222498893738\n",
      "Epoch 12430/30000  training loss: 0.15091243386268616 Validation loss 0.14830997586250305\n",
      "Epoch 12440/30000  training loss: 0.15078704059123993 Validation loss 0.14818784594535828\n",
      "Epoch 12450/30000  training loss: 0.15066175162792206 Validation loss 0.14806583523750305\n",
      "Epoch 12460/30000  training loss: 0.15053658187389374 Validation loss 0.14794395864009857\n",
      "Epoch 12470/30000  training loss: 0.15041156113147736 Validation loss 0.14782223105430603\n",
      "Epoch 12480/30000  training loss: 0.15028665959835052 Validation loss 0.14770060777664185\n",
      "Epoch 12490/30000  training loss: 0.15016189217567444 Validation loss 0.1475791484117508\n",
      "Epoch 12500/30000  training loss: 0.15003728866577148 Validation loss 0.14745785295963287\n",
      "Epoch 12510/30000  training loss: 0.14991290867328644 Validation loss 0.14733676612377167\n",
      "Epoch 12520/30000  training loss: 0.14978867769241333 Validation loss 0.1472157984972\n",
      "Epoch 12530/30000  training loss: 0.14966461062431335 Validation loss 0.14709503948688507\n",
      "Epoch 12540/30000  training loss: 0.14954067766666412 Validation loss 0.14697441458702087\n",
      "Epoch 12550/30000  training loss: 0.14941686391830444 Validation loss 0.14685390889644623\n",
      "Epoch 12560/30000  training loss: 0.1492931991815567 Validation loss 0.14673352241516113\n",
      "Epoch 12570/30000  training loss: 0.14916963875293732 Validation loss 0.14661328494548798\n",
      "Epoch 12580/30000  training loss: 0.14904624223709106 Validation loss 0.14649318158626556\n",
      "Epoch 12590/30000  training loss: 0.14892295002937317 Validation loss 0.1463731825351715\n",
      "Epoch 12600/30000  training loss: 0.1487998068332672 Validation loss 0.14625334739685059\n",
      "Epoch 12610/30000  training loss: 0.14867675304412842 Validation loss 0.1461336314678192\n",
      "Epoch 12620/30000  training loss: 0.14855386316776276 Validation loss 0.1460140347480774\n",
      "Epoch 12630/30000  training loss: 0.14843109250068665 Validation loss 0.14589457213878632\n",
      "Epoch 12640/30000  training loss: 0.14830850064754486 Validation loss 0.14577533304691315\n",
      "Epoch 12650/30000  training loss: 0.14818605780601501 Validation loss 0.14565619826316833\n",
      "Epoch 12660/30000  training loss: 0.1480637639760971 Validation loss 0.14553719758987427\n",
      "Epoch 12670/30000  training loss: 0.14794158935546875 Validation loss 0.14541834592819214\n",
      "Epoch 12680/30000  training loss: 0.14781953394412994 Validation loss 0.14529962837696075\n",
      "Epoch 12690/30000  training loss: 0.14769762754440308 Validation loss 0.14518103003501892\n",
      "Epoch 12700/30000  training loss: 0.14757584035396576 Validation loss 0.14506256580352783\n",
      "Epoch 12710/30000  training loss: 0.1474541872739792 Validation loss 0.1449442356824875\n",
      "Epoch 12720/30000  training loss: 0.14733265340328217 Validation loss 0.1448260396718979\n",
      "Epoch 12730/30000  training loss: 0.14721128344535828 Validation loss 0.14470799267292023\n",
      "Epoch 12740/30000  training loss: 0.14709003269672394 Validation loss 0.1445900946855545\n",
      "Epoch 12750/30000  training loss: 0.14696897566318512 Validation loss 0.14447236061096191\n",
      "Epoch 12760/30000  training loss: 0.14684805274009705 Validation loss 0.14435477554798126\n",
      "Epoch 12770/30000  training loss: 0.1467272788286209 Validation loss 0.14423732459545135\n",
      "Epoch 12780/30000  training loss: 0.14660659432411194 Validation loss 0.1441200077533722\n",
      "Epoch 12790/30000  training loss: 0.1464860588312149 Validation loss 0.14400282502174377\n",
      "Epoch 12800/30000  training loss: 0.146365687251091 Validation loss 0.1438857764005661\n",
      "Epoch 12810/30000  training loss: 0.14624541997909546 Validation loss 0.14376889169216156\n",
      "Epoch 12820/30000  training loss: 0.14612528681755066 Validation loss 0.14365209639072418\n",
      "Epoch 12830/30000  training loss: 0.1460053026676178 Validation loss 0.14353542029857635\n",
      "Epoch 12840/30000  training loss: 0.14588548243045807 Validation loss 0.14341896772384644\n",
      "Epoch 12850/30000  training loss: 0.14576582610607147 Validation loss 0.14330267906188965\n",
      "Epoch 12860/30000  training loss: 0.14564630389213562 Validation loss 0.14318647980690002\n",
      "Epoch 12870/30000  training loss: 0.1455269157886505 Validation loss 0.14307047426700592\n",
      "Epoch 12880/30000  training loss: 0.14540764689445496 Validation loss 0.14295455813407898\n",
      "Epoch 12890/30000  training loss: 0.14528852701187134 Validation loss 0.14283879101276398\n",
      "Epoch 12900/30000  training loss: 0.14516954123973846 Validation loss 0.1427231729030609\n",
      "Epoch 12910/30000  training loss: 0.14505068957805634 Validation loss 0.1426076740026474\n",
      "Epoch 12920/30000  training loss: 0.14493197202682495 Validation loss 0.14249230921268463\n",
      "Epoch 12930/30000  training loss: 0.1448134481906891 Validation loss 0.14237715303897858\n",
      "Epoch 12940/30000  training loss: 0.14469505846500397 Validation loss 0.14226214587688446\n",
      "Epoch 12950/30000  training loss: 0.1445768177509308 Validation loss 0.1421472579240799\n",
      "Epoch 12960/30000  training loss: 0.14445871114730835 Validation loss 0.14203250408172607\n",
      "Epoch 12970/30000  training loss: 0.14434073865413666 Validation loss 0.14191792905330658\n",
      "Epoch 12980/30000  training loss: 0.14422304928302765 Validation loss 0.14180360734462738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12990/30000  training loss: 0.144105464220047 Validation loss 0.14168941974639893\n",
      "Epoch 13000/30000  training loss: 0.14398805797100067 Validation loss 0.14157533645629883\n",
      "Epoch 13010/30000  training loss: 0.1438707858324051 Validation loss 0.14146144688129425\n",
      "Epoch 13020/30000  training loss: 0.14375369250774384 Validation loss 0.1413477212190628\n",
      "Epoch 13030/30000  training loss: 0.14363674819469452 Validation loss 0.1412341445684433\n",
      "Epoch 13040/30000  training loss: 0.14351992309093475 Validation loss 0.14112070202827454\n",
      "Epoch 13050/30000  training loss: 0.14340324699878693 Validation loss 0.14100739359855652\n",
      "Epoch 13060/30000  training loss: 0.14328670501708984 Validation loss 0.14089423418045044\n",
      "Epoch 13070/30000  training loss: 0.1431702971458435 Validation loss 0.1407812088727951\n",
      "Epoch 13080/30000  training loss: 0.1430540233850479 Validation loss 0.14066830277442932\n",
      "Epoch 13090/30000  training loss: 0.14293792843818665 Validation loss 0.14055559039115906\n",
      "Epoch 13100/30000  training loss: 0.1428220123052597 Validation loss 0.14044304192066193\n",
      "Epoch 13110/30000  training loss: 0.14270618557929993 Validation loss 0.14033061265945435\n",
      "Epoch 13120/30000  training loss: 0.14259056746959686 Validation loss 0.14021837711334229\n",
      "Epoch 13130/30000  training loss: 0.14247503876686096 Validation loss 0.1401062309741974\n",
      "Epoch 13140/30000  training loss: 0.142359659075737 Validation loss 0.13999421894550323\n",
      "Epoch 13150/30000  training loss: 0.14224441349506378 Validation loss 0.13988237082958221\n",
      "Epoch 13160/30000  training loss: 0.1421293169260025 Validation loss 0.13977067172527313\n",
      "Epoch 13170/30000  training loss: 0.14201438426971436 Validation loss 0.13965913653373718\n",
      "Epoch 13180/30000  training loss: 0.14189964532852173 Validation loss 0.13954776525497437\n",
      "Epoch 13190/30000  training loss: 0.14178502559661865 Validation loss 0.1394365429878235\n",
      "Epoch 13200/30000  training loss: 0.14167052507400513 Validation loss 0.13932543992996216\n",
      "Epoch 13210/30000  training loss: 0.14155620336532593 Validation loss 0.13921450078487396\n",
      "Epoch 13220/30000  training loss: 0.14144200086593628 Validation loss 0.13910368084907532\n",
      "Epoch 13230/30000  training loss: 0.14132793247699738 Validation loss 0.1389930099248886\n",
      "Epoch 13240/30000  training loss: 0.1412140280008316 Validation loss 0.13888250291347504\n",
      "Epoch 13250/30000  training loss: 0.1411004364490509 Validation loss 0.13877230882644653\n",
      "Epoch 13260/30000  training loss: 0.14098697900772095 Validation loss 0.13866224884986877\n",
      "Epoch 13270/30000  training loss: 0.14087368547916412 Validation loss 0.13855232298374176\n",
      "Epoch 13280/30000  training loss: 0.14076051115989685 Validation loss 0.13844256103038788\n",
      "Epoch 13290/30000  training loss: 0.14064748585224152 Validation loss 0.13833294808864594\n",
      "Epoch 13300/30000  training loss: 0.14053459465503693 Validation loss 0.13822342455387115\n",
      "Epoch 13310/30000  training loss: 0.14042186737060547 Validation loss 0.1381140947341919\n",
      "Epoch 13320/30000  training loss: 0.14030931890010834 Validation loss 0.13800495862960815\n",
      "Epoch 13330/30000  training loss: 0.14019691944122314 Validation loss 0.13789592683315277\n",
      "Epoch 13340/30000  training loss: 0.1400846391916275 Validation loss 0.1377870738506317\n",
      "Epoch 13350/30000  training loss: 0.139972522854805 Validation loss 0.137678325176239\n",
      "Epoch 13360/30000  training loss: 0.13986051082611084 Validation loss 0.13756975531578064\n",
      "Epoch 13370/30000  training loss: 0.13974866271018982 Validation loss 0.137461319565773\n",
      "Epoch 13380/30000  training loss: 0.13963697850704193 Validation loss 0.1373530477285385\n",
      "Epoch 13390/30000  training loss: 0.13952548801898956 Validation loss 0.13724492490291595\n",
      "Epoch 13400/30000  training loss: 0.13941411674022675 Validation loss 0.13713696599006653\n",
      "Epoch 13410/30000  training loss: 0.13930292427539825 Validation loss 0.13702915608882904\n",
      "Epoch 13420/30000  training loss: 0.13919183611869812 Validation loss 0.1369214653968811\n",
      "Epoch 13430/30000  training loss: 0.13908089697360992 Validation loss 0.1368139386177063\n",
      "Epoch 13440/30000  training loss: 0.13897009193897247 Validation loss 0.13670651614665985\n",
      "Epoch 13450/30000  training loss: 0.13885948061943054 Validation loss 0.1365993320941925\n",
      "Epoch 13460/30000  training loss: 0.1387491226196289 Validation loss 0.13649240136146545\n",
      "Epoch 13470/30000  training loss: 0.1386389583349228 Validation loss 0.13638560473918915\n",
      "Epoch 13480/30000  training loss: 0.13852891325950623 Validation loss 0.13627898693084717\n",
      "Epoch 13490/30000  training loss: 0.1384190171957016 Validation loss 0.13617247343063354\n",
      "Epoch 13500/30000  training loss: 0.13830925524234772 Validation loss 0.13606612384319305\n",
      "Epoch 13510/30000  training loss: 0.13819967210292816 Validation loss 0.1359599530696869\n",
      "Epoch 13520/30000  training loss: 0.13809026777744293 Validation loss 0.13585396111011505\n",
      "Epoch 13530/30000  training loss: 0.13798098266124725 Validation loss 0.13574807345867157\n",
      "Epoch 13540/30000  training loss: 0.1378718614578247 Validation loss 0.13564236462116241\n",
      "Epoch 13550/30000  training loss: 0.1377628743648529 Validation loss 0.1355367749929428\n",
      "Epoch 13560/30000  training loss: 0.13765403628349304 Validation loss 0.13543133437633514\n",
      "Epoch 13570/30000  training loss: 0.13754534721374512 Validation loss 0.1353260576725006\n",
      "Epoch 13580/30000  training loss: 0.13743683695793152 Validation loss 0.1352209597826004\n",
      "Epoch 13590/30000  training loss: 0.13732849061489105 Validation loss 0.13511599600315094\n",
      "Epoch 13600/30000  training loss: 0.13722027838230133 Validation loss 0.1350111961364746\n",
      "Epoch 13610/30000  training loss: 0.13711220026016235 Validation loss 0.13490653038024902\n",
      "Epoch 13620/30000  training loss: 0.13700425624847412 Validation loss 0.134801983833313\n",
      "Epoch 13630/30000  training loss: 0.13689647614955902 Validation loss 0.13469761610031128\n",
      "Epoch 13640/30000  training loss: 0.13678893446922302 Validation loss 0.13459350168704987\n",
      "Epoch 13650/30000  training loss: 0.13668163120746613 Validation loss 0.13448959589004517\n",
      "Epoch 13660/30000  training loss: 0.13657446205615997 Validation loss 0.1343858242034912\n",
      "Epoch 13670/30000  training loss: 0.13646742701530457 Validation loss 0.1342822015285492\n",
      "Epoch 13680/30000  training loss: 0.1363605409860611 Validation loss 0.13417872786521912\n",
      "Epoch 13690/30000  training loss: 0.13625380396842957 Validation loss 0.13407538831233978\n",
      "Epoch 13700/30000  training loss: 0.13614726066589355 Validation loss 0.13397227227687836\n",
      "Epoch 13710/30000  training loss: 0.13604088127613068 Validation loss 0.1338692605495453\n",
      "Epoch 13720/30000  training loss: 0.13593460619449615 Validation loss 0.13376642763614655\n",
      "Epoch 13730/30000  training loss: 0.13582848012447357 Validation loss 0.13366371393203735\n",
      "Epoch 13740/30000  training loss: 0.13572251796722412 Validation loss 0.1335611343383789\n",
      "Epoch 13750/30000  training loss: 0.1356167197227478 Validation loss 0.13345873355865479\n",
      "Epoch 13760/30000  training loss: 0.13551108539104462 Validation loss 0.13335652649402618\n",
      "Epoch 13770/30000  training loss: 0.13540560007095337 Validation loss 0.13325445353984833\n",
      "Epoch 13780/30000  training loss: 0.13530024886131287 Validation loss 0.13315249979496002\n",
      "Epoch 13790/30000  training loss: 0.1351950615644455 Validation loss 0.13305069506168365\n",
      "Epoch 13800/30000  training loss: 0.13509000837802887 Validation loss 0.13294905424118042\n",
      "Epoch 13810/30000  training loss: 0.13498526811599731 Validation loss 0.13284772634506226\n",
      "Epoch 13820/30000  training loss: 0.1348806917667389 Validation loss 0.13274654746055603\n",
      "Epoch 13830/30000  training loss: 0.1347762644290924 Validation loss 0.13264551758766174\n",
      "Epoch 13840/30000  training loss: 0.13467194139957428 Validation loss 0.1325446367263794\n",
      "Epoch 13850/30000  training loss: 0.13456779718399048 Validation loss 0.1324438601732254\n",
      "Epoch 13860/30000  training loss: 0.13446378707885742 Validation loss 0.13234326243400574\n",
      "Epoch 13870/30000  training loss: 0.13435997068881989 Validation loss 0.1322428584098816\n",
      "Epoch 13880/30000  training loss: 0.13425631821155548 Validation loss 0.132142573595047\n",
      "Epoch 13890/30000  training loss: 0.13415279984474182 Validation loss 0.13204246759414673\n",
      "Epoch 13900/30000  training loss: 0.1340494155883789 Validation loss 0.131942480802536\n",
      "Epoch 13910/30000  training loss: 0.13394616544246674 Validation loss 0.13184262812137604\n",
      "Epoch 13920/30000  training loss: 0.1338430941104889 Validation loss 0.13174296915531158\n",
      "Epoch 13930/30000  training loss: 0.13374020159244537 Validation loss 0.13164347410202026\n",
      "Epoch 13940/30000  training loss: 0.1336374580860138 Validation loss 0.1315441131591797\n",
      "Epoch 13950/30000  training loss: 0.13353484869003296 Validation loss 0.13144493103027344\n",
      "Epoch 13960/30000  training loss: 0.1334325224161148 Validation loss 0.1313459873199463\n",
      "Epoch 13970/30000  training loss: 0.1333303451538086 Validation loss 0.13124720752239227\n",
      "Epoch 13980/30000  training loss: 0.1332283467054367 Validation loss 0.13114860653877258\n",
      "Epoch 13990/30000  training loss: 0.13312649726867676 Validation loss 0.13105015456676483\n",
      "Epoch 14000/30000  training loss: 0.13302481174468994 Validation loss 0.13095183670520782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14010/30000  training loss: 0.13292323052883148 Validation loss 0.13085366785526276\n",
      "Epoch 14020/30000  training loss: 0.13282179832458496 Validation loss 0.13075564801692963\n",
      "Epoch 14030/30000  training loss: 0.13272058963775635 Validation loss 0.13065779209136963\n",
      "Epoch 14040/30000  training loss: 0.13261950016021729 Validation loss 0.13056011497974396\n",
      "Epoch 14050/30000  training loss: 0.13251855969429016 Validation loss 0.13046255707740784\n",
      "Epoch 14060/30000  training loss: 0.13241775333881378 Validation loss 0.13036514818668365\n",
      "Epoch 14070/30000  training loss: 0.13231709599494934 Validation loss 0.1302678883075714\n",
      "Epoch 14080/30000  training loss: 0.1322166472673416 Validation loss 0.1301708072423935\n",
      "Epoch 14090/30000  training loss: 0.13211634755134583 Validation loss 0.1300739198923111\n",
      "Epoch 14100/30000  training loss: 0.13201630115509033 Validation loss 0.1299772709608078\n",
      "Epoch 14110/30000  training loss: 0.13191640377044678 Validation loss 0.12988075613975525\n",
      "Epoch 14120/30000  training loss: 0.13181665539741516 Validation loss 0.12978437542915344\n",
      "Epoch 14130/30000  training loss: 0.13171708583831787 Validation loss 0.12968818843364716\n",
      "Epoch 14140/30000  training loss: 0.1316176801919937 Validation loss 0.1295921504497528\n",
      "Epoch 14150/30000  training loss: 0.1315183937549591 Validation loss 0.1294962763786316\n",
      "Epoch 14160/30000  training loss: 0.13141925632953644 Validation loss 0.12940052151679993\n",
      "Epoch 14170/30000  training loss: 0.1313202828168869 Validation loss 0.1293049156665802\n",
      "Epoch 14180/30000  training loss: 0.1312214732170105 Validation loss 0.1292094886302948\n",
      "Epoch 14190/30000  training loss: 0.13112281262874603 Validation loss 0.12911421060562134\n",
      "Epoch 14200/30000  training loss: 0.1310243159532547 Validation loss 0.12901908159255981\n",
      "Epoch 14210/30000  training loss: 0.13092593848705292 Validation loss 0.12892410159111023\n",
      "Epoch 14220/30000  training loss: 0.13082775473594666 Validation loss 0.12882930040359497\n",
      "Epoch 14230/30000  training loss: 0.13072985410690308 Validation loss 0.1287347674369812\n",
      "Epoch 14240/30000  training loss: 0.13063210248947144 Validation loss 0.12864038348197937\n",
      "Epoch 14250/30000  training loss: 0.13053448498249054 Validation loss 0.12854614853858948\n",
      "Epoch 14260/30000  training loss: 0.13043701648712158 Validation loss 0.12845206260681152\n",
      "Epoch 14270/30000  training loss: 0.13033969700336456 Validation loss 0.12835808098316193\n",
      "Epoch 14280/30000  training loss: 0.13024254143238068 Validation loss 0.12826430797576904\n",
      "Epoch 14290/30000  training loss: 0.13014556467533112 Validation loss 0.12817071378231049\n",
      "Epoch 14300/30000  training loss: 0.1300487071275711 Validation loss 0.12807722389698029\n",
      "Epoch 14310/30000  training loss: 0.12995201349258423 Validation loss 0.12798388302326202\n",
      "Epoch 14320/30000  training loss: 0.1298554390668869 Validation loss 0.1278906762599945\n",
      "Epoch 14330/30000  training loss: 0.12975908815860748 Validation loss 0.1277976781129837\n",
      "Epoch 14340/30000  training loss: 0.12966284155845642 Validation loss 0.12770481407642365\n",
      "Epoch 14350/30000  training loss: 0.12956687808036804 Validation loss 0.12761220335960388\n",
      "Epoch 14360/30000  training loss: 0.1294710636138916 Validation loss 0.12751975655555725\n",
      "Epoch 14370/30000  training loss: 0.1293753981590271 Validation loss 0.12742744386196136\n",
      "Epoch 14380/30000  training loss: 0.12927991151809692 Validation loss 0.1273353397846222\n",
      "Epoch 14390/30000  training loss: 0.1291845589876175 Validation loss 0.12724334001541138\n",
      "Epoch 14400/30000  training loss: 0.1290893703699112 Validation loss 0.1271515041589737\n",
      "Epoch 14410/30000  training loss: 0.12899433076381683 Validation loss 0.12705980241298676\n",
      "Epoch 14420/30000  training loss: 0.1288994401693344 Validation loss 0.12696827948093414\n",
      "Epoch 14430/30000  training loss: 0.12880472838878632 Validation loss 0.12687690556049347\n",
      "Epoch 14440/30000  training loss: 0.12871013581752777 Validation loss 0.12678566575050354\n",
      "Epoch 14450/30000  training loss: 0.12861569225788116 Validation loss 0.12669458985328674\n",
      "Epoch 14460/30000  training loss: 0.1285214126110077 Validation loss 0.1266036331653595\n",
      "Epoch 14470/30000  training loss: 0.1284274011850357 Validation loss 0.12651298940181732\n",
      "Epoch 14480/30000  training loss: 0.12833356857299805 Validation loss 0.12642250955104828\n",
      "Epoch 14490/30000  training loss: 0.12823987007141113 Validation loss 0.1263321489095688\n",
      "Epoch 14500/30000  training loss: 0.12814633548259735 Validation loss 0.12624193727970123\n",
      "Epoch 14510/30000  training loss: 0.12805292010307312 Validation loss 0.1261518895626068\n",
      "Epoch 14520/30000  training loss: 0.1279596984386444 Validation loss 0.12606200575828552\n",
      "Epoch 14530/30000  training loss: 0.12786662578582764 Validation loss 0.12597225606441498\n",
      "Epoch 14540/30000  training loss: 0.12777367234230042 Validation loss 0.12588265538215637\n",
      "Epoch 14550/30000  training loss: 0.12768088281154633 Validation loss 0.1257931888103485\n",
      "Epoch 14560/30000  training loss: 0.12758825719356537 Validation loss 0.12570388615131378\n",
      "Epoch 14570/30000  training loss: 0.12749579548835754 Validation loss 0.12561476230621338\n",
      "Epoch 14580/30000  training loss: 0.12740352749824524 Validation loss 0.1255258321762085\n",
      "Epoch 14590/30000  training loss: 0.12731145322322845 Validation loss 0.12543711066246033\n",
      "Epoch 14600/30000  training loss: 0.1272195279598236 Validation loss 0.12534849345684052\n",
      "Epoch 14610/30000  training loss: 0.12712781131267548 Validation loss 0.1252601146697998\n",
      "Epoch 14620/30000  training loss: 0.1270362138748169 Validation loss 0.12517184019088745\n",
      "Epoch 14630/30000  training loss: 0.12694476544857025 Validation loss 0.12508371472358704\n",
      "Epoch 14640/30000  training loss: 0.12685345113277435 Validation loss 0.12499570846557617\n",
      "Epoch 14650/30000  training loss: 0.12676231563091278 Validation loss 0.12490790337324142\n",
      "Epoch 14660/30000  training loss: 0.12667134404182434 Validation loss 0.12482022494077682\n",
      "Epoch 14670/30000  training loss: 0.12658049166202545 Validation loss 0.12473271042108536\n",
      "Epoch 14680/30000  training loss: 0.1264897882938385 Validation loss 0.12464531511068344\n",
      "Epoch 14690/30000  training loss: 0.12639927864074707 Validation loss 0.12455813586711884\n",
      "Epoch 14700/30000  training loss: 0.12630903720855713 Validation loss 0.12447120249271393\n",
      "Epoch 14710/30000  training loss: 0.12621892988681793 Validation loss 0.12438439577817917\n",
      "Epoch 14720/30000  training loss: 0.12612895667552948 Validation loss 0.12429773062467575\n",
      "Epoch 14730/30000  training loss: 0.12603913247585297 Validation loss 0.12421119213104248\n",
      "Epoch 14740/30000  training loss: 0.12594947218894958 Validation loss 0.12412484735250473\n",
      "Epoch 14750/30000  training loss: 0.12585996091365814 Validation loss 0.12403865158557892\n",
      "Epoch 14760/30000  training loss: 0.12577058374881744 Validation loss 0.12395257502794266\n",
      "Epoch 14770/30000  training loss: 0.12568135559558868 Validation loss 0.12386664003133774\n",
      "Epoch 14780/30000  training loss: 0.12559227645397186 Validation loss 0.12378086894750595\n",
      "Epoch 14790/30000  training loss: 0.12550337612628937 Validation loss 0.1236952543258667\n",
      "Epoch 14800/30000  training loss: 0.12541472911834717 Validation loss 0.12360990047454834\n",
      "Epoch 14810/30000  training loss: 0.1253262311220169 Validation loss 0.12352469563484192\n",
      "Epoch 14820/30000  training loss: 0.1252378672361374 Validation loss 0.12343961745500565\n",
      "Epoch 14830/30000  training loss: 0.1251496821641922 Validation loss 0.12335473299026489\n",
      "Epoch 14840/30000  training loss: 0.12506164610385895 Validation loss 0.12326996773481369\n",
      "Epoch 14850/30000  training loss: 0.12497374415397644 Validation loss 0.12318535149097443\n",
      "Epoch 14860/30000  training loss: 0.12488597631454468 Validation loss 0.12310086190700531\n",
      "Epoch 14870/30000  training loss: 0.12479837238788605 Validation loss 0.12301655113697052\n",
      "Epoch 14880/30000  training loss: 0.12471094727516174 Validation loss 0.12293239682912827\n",
      "Epoch 14890/30000  training loss: 0.1246236264705658 Validation loss 0.12284836173057556\n",
      "Epoch 14900/30000  training loss: 0.12453651428222656 Validation loss 0.12276452034711838\n",
      "Epoch 14910/30000  training loss: 0.12444961816072464 Validation loss 0.1226809024810791\n",
      "Epoch 14920/30000  training loss: 0.12436291575431824 Validation loss 0.12259745597839355\n",
      "Epoch 14930/30000  training loss: 0.12427633255720139 Validation loss 0.12251414358615875\n",
      "Epoch 14940/30000  training loss: 0.12418986856937408 Validation loss 0.1224309504032135\n",
      "Epoch 14950/30000  training loss: 0.12410356849431992 Validation loss 0.12234792113304138\n",
      "Epoch 14960/30000  training loss: 0.12401743978261948 Validation loss 0.1222650408744812\n",
      "Epoch 14970/30000  training loss: 0.12393146008253098 Validation loss 0.12218231707811356\n",
      "Epoch 14980/30000  training loss: 0.12384558469057083 Validation loss 0.12209970504045486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14990/30000  training loss: 0.12375985831022263 Validation loss 0.12201724201440811\n",
      "Epoch 15000/30000  training loss: 0.12367437034845352 Validation loss 0.12193502485752106\n",
      "Epoch 15010/30000  training loss: 0.12358909845352173 Validation loss 0.12185299396514893\n",
      "Epoch 15020/30000  training loss: 0.12350396066904068 Validation loss 0.12177108973264694\n",
      "Epoch 15030/30000  training loss: 0.12341893464326859 Validation loss 0.1216893196105957\n",
      "Epoch 15040/30000  training loss: 0.12333410978317261 Validation loss 0.12160774320363998\n",
      "Epoch 15050/30000  training loss: 0.12324941903352737 Validation loss 0.12152627855539322\n",
      "Epoch 15060/30000  training loss: 0.12316486239433289 Validation loss 0.12144496291875839\n",
      "Epoch 15070/30000  training loss: 0.12308042496442795 Validation loss 0.12136377394199371\n",
      "Epoch 15080/30000  training loss: 0.12299617379903793 Validation loss 0.12128276377916336\n",
      "Epoch 15090/30000  training loss: 0.12291206419467926 Validation loss 0.12120188772678375\n",
      "Epoch 15100/30000  training loss: 0.1228281706571579 Validation loss 0.12112122774124146\n",
      "Epoch 15110/30000  training loss: 0.12274445593357086 Validation loss 0.12104073166847229\n",
      "Epoch 15120/30000  training loss: 0.12266088277101517 Validation loss 0.12096040695905685\n",
      "Epoch 15130/30000  training loss: 0.1225774884223938 Validation loss 0.12088023126125336\n",
      "Epoch 15140/30000  training loss: 0.12249422818422318 Validation loss 0.12080017477273941\n",
      "Epoch 15150/30000  training loss: 0.1224110797047615 Validation loss 0.1207202672958374\n",
      "Epoch 15160/30000  training loss: 0.12232809513807297 Validation loss 0.12064051628112793\n",
      "Epoch 15170/30000  training loss: 0.12224528193473816 Validation loss 0.1205608993768692\n",
      "Epoch 15180/30000  training loss: 0.1221625879406929 Validation loss 0.12048142403364182\n",
      "Epoch 15190/30000  training loss: 0.12208002805709839 Validation loss 0.12040208280086517\n",
      "Epoch 15200/30000  training loss: 0.12199772894382477 Validation loss 0.12032301723957062\n",
      "Epoch 15210/30000  training loss: 0.12191561609506607 Validation loss 0.1202441081404686\n",
      "Epoch 15220/30000  training loss: 0.12183363735675812 Validation loss 0.12016532570123672\n",
      "Epoch 15230/30000  training loss: 0.12175177782773972 Validation loss 0.1200866550207138\n",
      "Epoch 15240/30000  training loss: 0.12167006731033325 Validation loss 0.12000813335180283\n",
      "Epoch 15250/30000  training loss: 0.12158852070569992 Validation loss 0.11992979049682617\n",
      "Epoch 15260/30000  training loss: 0.12150710821151733 Validation loss 0.11985156685113907\n",
      "Epoch 15270/30000  training loss: 0.12142582982778549 Validation loss 0.1197734922170639\n",
      "Epoch 15280/30000  training loss: 0.1213446781039238 Validation loss 0.1196955218911171\n",
      "Epoch 15290/30000  training loss: 0.12126375734806061 Validation loss 0.11961782723665237\n",
      "Epoch 15300/30000  training loss: 0.12118304520845413 Validation loss 0.11954028904438019\n",
      "Epoch 15310/30000  training loss: 0.12110244482755661 Validation loss 0.11946287751197815\n",
      "Epoch 15320/30000  training loss: 0.12102199345827103 Validation loss 0.11938558518886566\n",
      "Epoch 15330/30000  training loss: 0.12094172835350037 Validation loss 0.11930850148200989\n",
      "Epoch 15340/30000  training loss: 0.12086156010627747 Validation loss 0.11923151463270187\n",
      "Epoch 15350/30000  training loss: 0.1207815557718277 Validation loss 0.1191546767950058\n",
      "Epoch 15360/30000  training loss: 0.12070164829492569 Validation loss 0.11907795071601868\n",
      "Epoch 15370/30000  training loss: 0.1206219419836998 Validation loss 0.11900139600038528\n",
      "Epoch 15380/30000  training loss: 0.12054239213466644 Validation loss 0.11892503499984741\n",
      "Epoch 15390/30000  training loss: 0.1204630583524704 Validation loss 0.11884885281324387\n",
      "Epoch 15400/30000  training loss: 0.1203838586807251 Validation loss 0.11877281963825226\n",
      "Epoch 15410/30000  training loss: 0.12030482292175293 Validation loss 0.11869694292545319\n",
      "Epoch 15420/30000  training loss: 0.1202259212732315 Validation loss 0.11862118542194366\n",
      "Epoch 15430/30000  training loss: 0.12014715373516083 Validation loss 0.11854556202888489\n",
      "Epoch 15440/30000  training loss: 0.1200684979557991 Validation loss 0.11847007274627686\n",
      "Epoch 15450/30000  training loss: 0.1199900433421135 Validation loss 0.11839476227760315\n",
      "Epoch 15460/30000  training loss: 0.11991170793771744 Validation loss 0.1183195635676384\n",
      "Epoch 15470/30000  training loss: 0.11983349174261093 Validation loss 0.11824449896812439\n",
      "Epoch 15480/30000  training loss: 0.11975553631782532 Validation loss 0.11816968768835068\n",
      "Epoch 15490/30000  training loss: 0.11967774480581284 Validation loss 0.1180950403213501\n",
      "Epoch 15500/30000  training loss: 0.1196000874042511 Validation loss 0.11802050471305847\n",
      "Epoch 15510/30000  training loss: 0.11952254921197891 Validation loss 0.11794610321521759\n",
      "Epoch 15520/30000  training loss: 0.11944515258073807 Validation loss 0.11787184327840805\n",
      "Epoch 15530/30000  training loss: 0.11936791986227036 Validation loss 0.11779772490262985\n",
      "Epoch 15540/30000  training loss: 0.1192908063530922 Validation loss 0.1177237331867218\n",
      "Epoch 15550/30000  training loss: 0.11921382695436478 Validation loss 0.1176498681306839\n",
      "Epoch 15560/30000  training loss: 0.1191369891166687 Validation loss 0.11757618188858032\n",
      "Epoch 15570/30000  training loss: 0.11906042695045471 Validation loss 0.11750271916389465\n",
      "Epoch 15580/30000  training loss: 0.11898399889469147 Validation loss 0.11742939800024033\n",
      "Epoch 15590/30000  training loss: 0.11890766769647598 Validation loss 0.11735618859529495\n",
      "Epoch 15600/30000  training loss: 0.11883150786161423 Validation loss 0.11728313565254211\n",
      "Epoch 15610/30000  training loss: 0.11875549703836441 Validation loss 0.11721023917198181\n",
      "Epoch 15620/30000  training loss: 0.11867959797382355 Validation loss 0.11713742464780807\n",
      "Epoch 15630/30000  training loss: 0.11860383301973343 Validation loss 0.11706474423408508\n",
      "Epoch 15640/30000  training loss: 0.11852820962667465 Validation loss 0.11699223518371582\n",
      "Epoch 15650/30000  training loss: 0.1184527799487114 Validation loss 0.11691989749670029\n",
      "Epoch 15660/30000  training loss: 0.11837755888700485 Validation loss 0.11684776842594147\n",
      "Epoch 15670/30000  training loss: 0.11830244213342667 Validation loss 0.1167757511138916\n",
      "Epoch 15680/30000  training loss: 0.1182275041937828 Validation loss 0.11670389771461487\n",
      "Epoch 15690/30000  training loss: 0.1181526929140091 Validation loss 0.11663216352462769\n",
      "Epoch 15700/30000  training loss: 0.11807799339294434 Validation loss 0.11656055599451065\n",
      "Epoch 15710/30000  training loss: 0.11800344288349152 Validation loss 0.11648905277252197\n",
      "Epoch 15720/30000  training loss: 0.11792904883623123 Validation loss 0.11641775071620941\n",
      "Epoch 15730/30000  training loss: 0.1178547739982605 Validation loss 0.11634653806686401\n",
      "Epoch 15740/30000  training loss: 0.11778069287538528 Validation loss 0.11627552658319473\n",
      "Epoch 15750/30000  training loss: 0.1177067682147026 Validation loss 0.11620467901229858\n",
      "Epoch 15760/30000  training loss: 0.11763303726911545 Validation loss 0.11613401025533676\n",
      "Epoch 15770/30000  training loss: 0.11755941808223724 Validation loss 0.1160634458065033\n",
      "Epoch 15780/30000  training loss: 0.11748593300580978 Validation loss 0.11599300801753998\n",
      "Epoch 15790/30000  training loss: 0.11741256713867188 Validation loss 0.11592269688844681\n",
      "Epoch 15800/30000  training loss: 0.1173393577337265 Validation loss 0.11585251986980438\n",
      "Epoch 15810/30000  training loss: 0.11726627498865128 Validation loss 0.11578250676393509\n",
      "Epoch 15820/30000  training loss: 0.117193304002285 Validation loss 0.11571258306503296\n",
      "Epoch 15830/30000  training loss: 0.11712058633565903 Validation loss 0.11564291268587112\n",
      "Epoch 15840/30000  training loss: 0.11704803258180618 Validation loss 0.11557339876890182\n",
      "Epoch 15850/30000  training loss: 0.11697559803724289 Validation loss 0.11550398916006088\n",
      "Epoch 15860/30000  training loss: 0.11690329015254974 Validation loss 0.11543469876050949\n",
      "Epoch 15870/30000  training loss: 0.11683110147714615 Validation loss 0.11536556482315063\n",
      "Epoch 15880/30000  training loss: 0.11675907671451569 Validation loss 0.11529654264450073\n",
      "Epoch 15890/30000  training loss: 0.11668716371059418 Validation loss 0.11522766947746277\n",
      "Epoch 15900/30000  training loss: 0.11661536246538162 Validation loss 0.11515888571739197\n",
      "Epoch 15910/30000  training loss: 0.11654376238584518 Validation loss 0.11509031802415848\n",
      "Epoch 15920/30000  training loss: 0.11647237092256546 Validation loss 0.1150219514966011\n",
      "Epoch 15930/30000  training loss: 0.11640110611915588 Validation loss 0.1149536743760109\n",
      "Epoch 15940/30000  training loss: 0.11632994562387466 Validation loss 0.11488553881645203\n",
      "Epoch 15950/30000  training loss: 0.11625894904136658 Validation loss 0.1148175373673439\n",
      "Epoch 15960/30000  training loss: 0.11618808656930923 Validation loss 0.11474968492984772\n",
      "Epoch 15970/30000  training loss: 0.11611733585596085 Validation loss 0.1146819144487381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15980/30000  training loss: 0.11604667454957962 Validation loss 0.11461427062749863\n",
      "Epoch 15990/30000  training loss: 0.11597621440887451 Validation loss 0.11454680562019348\n",
      "Epoch 16000/30000  training loss: 0.11590595543384552 Validation loss 0.11447954177856445\n",
      "Epoch 16010/30000  training loss: 0.11583583801984787 Validation loss 0.11441241949796677\n",
      "Epoch 16020/30000  training loss: 0.11576584726572037 Validation loss 0.11434540152549744\n",
      "Epoch 16030/30000  training loss: 0.1156960055232048 Validation loss 0.11427854746580124\n",
      "Epoch 16040/30000  training loss: 0.11562629789113998 Validation loss 0.11421181261539459\n",
      "Epoch 16050/30000  training loss: 0.11555667221546173 Validation loss 0.11414516717195511\n",
      "Epoch 16060/30000  training loss: 0.11548720300197601 Validation loss 0.11407867074012756\n",
      "Epoch 16070/30000  training loss: 0.11541788280010223 Validation loss 0.11401232331991196\n",
      "Epoch 16080/30000  training loss: 0.1153487041592598 Validation loss 0.1139461100101471\n",
      "Epoch 16090/30000  training loss: 0.11527971178293228 Validation loss 0.11388009786605835\n",
      "Epoch 16100/30000  training loss: 0.11521089822053909 Validation loss 0.11381421983242035\n",
      "Epoch 16110/30000  training loss: 0.11514218151569366 Validation loss 0.1137484610080719\n",
      "Epoch 16120/30000  training loss: 0.11507359892129898 Validation loss 0.1136828362941742\n",
      "Epoch 16130/30000  training loss: 0.11500509828329086 Validation loss 0.11361727863550186\n",
      "Epoch 16140/30000  training loss: 0.11493679136037827 Validation loss 0.11355191469192505\n",
      "Epoch 16150/30000  training loss: 0.11486858874559402 Validation loss 0.11348666250705719\n",
      "Epoch 16160/30000  training loss: 0.11480051279067993 Validation loss 0.11342152208089828\n",
      "Epoch 16170/30000  training loss: 0.11473265290260315 Validation loss 0.11335662007331848\n",
      "Epoch 16180/30000  training loss: 0.1146649643778801 Validation loss 0.11329183727502823\n",
      "Epoch 16190/30000  training loss: 0.1145973652601242 Validation loss 0.11322718858718872\n",
      "Epoch 16200/30000  training loss: 0.11452990025281906 Validation loss 0.11316263675689697\n",
      "Epoch 16210/30000  training loss: 0.11446254700422287 Validation loss 0.11309822648763657\n",
      "Epoch 16220/30000  training loss: 0.11439534276723862 Validation loss 0.11303394287824631\n",
      "Epoch 16230/30000  training loss: 0.11432825028896332 Validation loss 0.1129697635769844\n",
      "Epoch 16240/30000  training loss: 0.11426126956939697 Validation loss 0.11290568858385086\n",
      "Epoch 16250/30000  training loss: 0.11419451981782913 Validation loss 0.11284185945987701\n",
      "Epoch 16260/30000  training loss: 0.11412793397903442 Validation loss 0.11277817189693451\n",
      "Epoch 16270/30000  training loss: 0.11406144499778748 Validation loss 0.11271460354328156\n",
      "Epoch 16280/30000  training loss: 0.11399506032466888 Validation loss 0.11265110969543457\n",
      "Epoch 16290/30000  training loss: 0.11392886191606522 Validation loss 0.1125878095626831\n",
      "Epoch 16300/30000  training loss: 0.11386274546384811 Validation loss 0.1125245913863182\n",
      "Epoch 16310/30000  training loss: 0.11379674077033997 Validation loss 0.11246148496866226\n",
      "Epoch 16320/30000  training loss: 0.11373087018728256 Validation loss 0.11239850521087646\n",
      "Epoch 16330/30000  training loss: 0.11366520076990128 Validation loss 0.11233572661876678\n",
      "Epoch 16340/30000  training loss: 0.11359971016645432 Validation loss 0.11227310448884964\n",
      "Epoch 16350/30000  training loss: 0.11353430151939392 Validation loss 0.11221057921648026\n",
      "Epoch 16360/30000  training loss: 0.11346903443336487 Validation loss 0.11214819550514221\n",
      "Epoch 16370/30000  training loss: 0.11340391635894775 Validation loss 0.11208593845367432\n",
      "Epoch 16380/30000  training loss: 0.1133388802409172 Validation loss 0.11202377825975418\n",
      "Epoch 16390/30000  training loss: 0.11327396333217621 Validation loss 0.1119617223739624\n",
      "Epoch 16400/30000  training loss: 0.11320918798446655 Validation loss 0.11189980804920197\n",
      "Epoch 16410/30000  training loss: 0.11314459145069122 Validation loss 0.11183807253837585\n",
      "Epoch 16420/30000  training loss: 0.11308015137910843 Validation loss 0.11177650094032288\n",
      "Epoch 16430/30000  training loss: 0.11301582306623459 Validation loss 0.11171502619981766\n",
      "Epoch 16440/30000  training loss: 0.11295165866613388 Validation loss 0.11165370792150497\n",
      "Epoch 16450/30000  training loss: 0.11288758367300034 Validation loss 0.11159248650074005\n",
      "Epoch 16460/30000  training loss: 0.11282363533973694 Validation loss 0.11153136193752289\n",
      "Epoch 16470/30000  training loss: 0.11275980621576309 Validation loss 0.11147037893533707\n",
      "Epoch 16480/30000  training loss: 0.1126960963010788 Validation loss 0.1114095076918602\n",
      "Epoch 16490/30000  training loss: 0.11263255774974823 Validation loss 0.11134878545999527\n",
      "Epoch 16500/30000  training loss: 0.1125691831111908 Validation loss 0.11128823459148407\n",
      "Epoch 16510/30000  training loss: 0.1125059574842453 Validation loss 0.1112278401851654\n",
      "Epoch 16520/30000  training loss: 0.11244283616542816 Validation loss 0.1111675426363945\n",
      "Epoch 16530/30000  training loss: 0.11237981915473938 Validation loss 0.11110735684633255\n",
      "Epoch 16540/30000  training loss: 0.11231691390275955 Validation loss 0.11104728281497955\n",
      "Epoch 16550/30000  training loss: 0.11225415766239166 Validation loss 0.11098731309175491\n",
      "Epoch 16560/30000  training loss: 0.11219152063131332 Validation loss 0.11092747747898102\n",
      "Epoch 16570/30000  training loss: 0.11212898045778275 Validation loss 0.11086776852607727\n",
      "Epoch 16580/30000  training loss: 0.11206667125225067 Validation loss 0.11080826073884964\n",
      "Epoch 16590/30000  training loss: 0.11200449615716934 Validation loss 0.11074887961149216\n",
      "Epoch 16600/30000  training loss: 0.11194241791963577 Validation loss 0.11068960279226303\n",
      "Epoch 16610/30000  training loss: 0.11188045889139175 Validation loss 0.11063041538000107\n",
      "Epoch 16620/30000  training loss: 0.11181860417127609 Validation loss 0.11057135462760925\n",
      "Epoch 16630/30000  training loss: 0.11175689101219177 Validation loss 0.11051242798566818\n",
      "Epoch 16640/30000  training loss: 0.11169527471065521 Validation loss 0.11045359075069427\n",
      "Epoch 16650/30000  training loss: 0.1116337701678276 Validation loss 0.11039487272500992\n",
      "Epoch 16660/30000  training loss: 0.1115725189447403 Validation loss 0.11033639311790466\n",
      "Epoch 16670/30000  training loss: 0.11151137202978134 Validation loss 0.11027800291776657\n",
      "Epoch 16680/30000  training loss: 0.11145032197237015 Validation loss 0.11021973192691803\n",
      "Epoch 16690/30000  training loss: 0.11138937622308731 Validation loss 0.11016155779361725\n",
      "Epoch 16700/30000  training loss: 0.11132857948541641 Validation loss 0.11010351777076721\n",
      "Epoch 16710/30000  training loss: 0.11126787960529327 Validation loss 0.11004557460546494\n",
      "Epoch 16720/30000  training loss: 0.11120729148387909 Validation loss 0.10998772084712982\n",
      "Epoch 16730/30000  training loss: 0.11114681512117386 Validation loss 0.10993001610040665\n",
      "Epoch 16740/30000  training loss: 0.11108657717704773 Validation loss 0.10987251251935959\n",
      "Epoch 16750/30000  training loss: 0.11102643609046936 Validation loss 0.10981512069702148\n",
      "Epoch 16760/30000  training loss: 0.11096639931201935 Validation loss 0.10975782573223114\n",
      "Epoch 16770/30000  training loss: 0.11090649664402008 Validation loss 0.10970064997673035\n",
      "Epoch 16780/30000  training loss: 0.11084670573472977 Validation loss 0.1096435934305191\n",
      "Epoch 16790/30000  training loss: 0.11078699678182602 Validation loss 0.10958661139011383\n",
      "Epoch 16800/30000  training loss: 0.11072740703821182 Validation loss 0.1095297634601593\n",
      "Epoch 16810/30000  training loss: 0.11066795885562897 Validation loss 0.10947303473949432\n",
      "Epoch 16820/30000  training loss: 0.11060872673988342 Validation loss 0.10941652208566666\n",
      "Epoch 16830/30000  training loss: 0.11054957658052444 Validation loss 0.10936009138822556\n",
      "Epoch 16840/30000  training loss: 0.11049055308103561 Validation loss 0.10930376499891281\n",
      "Epoch 16850/30000  training loss: 0.11043165624141693 Validation loss 0.10924758017063141\n",
      "Epoch 16860/30000  training loss: 0.11037284135818481 Validation loss 0.10919146239757538\n",
      "Epoch 16870/30000  training loss: 0.11031413078308105 Validation loss 0.1091354638338089\n",
      "Epoch 16880/30000  training loss: 0.11025554686784744 Validation loss 0.10907958447933197\n",
      "Epoch 16890/30000  training loss: 0.11019710451364517 Validation loss 0.10902383178472519\n",
      "Epoch 16900/30000  training loss: 0.11013885587453842 Validation loss 0.10896827280521393\n",
      "Epoch 16910/30000  training loss: 0.11008068919181824 Validation loss 0.10891278833150864\n",
      "Epoch 16920/30000  training loss: 0.1100226640701294 Validation loss 0.10885744541883469\n",
      "Epoch 16930/30000  training loss: 0.10996473580598831 Validation loss 0.1088021993637085\n",
      "Epoch 16940/30000  training loss: 0.10990691184997559 Validation loss 0.10874704271554947\n",
      "Epoch 16950/30000  training loss: 0.10984918475151062 Validation loss 0.1086919978260994\n",
      "Epoch 16960/30000  training loss: 0.1097915917634964 Validation loss 0.10863707214593887\n",
      "Epoch 16970/30000  training loss: 0.10973411053419113 Validation loss 0.1085822582244873\n",
      "Epoch 16980/30000  training loss: 0.10967681556940079 Validation loss 0.10852763056755066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16990/30000  training loss: 0.10961965471506119 Validation loss 0.10847312211990356\n",
      "Epoch 17000/30000  training loss: 0.10956258326768875 Validation loss 0.10841871052980423\n",
      "Epoch 17010/30000  training loss: 0.10950561612844467 Validation loss 0.10836439579725266\n",
      "Epoch 17020/30000  training loss: 0.10944873094558716 Validation loss 0.10831016302108765\n",
      "Epoch 17030/30000  training loss: 0.10939200967550278 Validation loss 0.10825609415769577\n",
      "Epoch 17040/30000  training loss: 0.10933535546064377 Validation loss 0.10820208489894867\n",
      "Epoch 17050/30000  training loss: 0.1092788502573967 Validation loss 0.10814821720123291\n",
      "Epoch 17060/30000  training loss: 0.10922251641750336 Validation loss 0.10809451341629028\n",
      "Epoch 17070/30000  training loss: 0.10916630923748016 Validation loss 0.10804092884063721\n",
      "Epoch 17080/30000  training loss: 0.10911020636558533 Validation loss 0.10798744857311249\n",
      "Epoch 17090/30000  training loss: 0.10905417054891586 Validation loss 0.10793404281139374\n",
      "Epoch 17100/30000  training loss: 0.10899828374385834 Validation loss 0.10788076370954514\n",
      "Epoch 17110/30000  training loss: 0.10894247889518738 Validation loss 0.1078275814652443\n",
      "Epoch 17120/30000  training loss: 0.10888678580522537 Validation loss 0.10777448862791061\n",
      "Epoch 17130/30000  training loss: 0.10883121192455292 Validation loss 0.10772155225276947\n",
      "Epoch 17140/30000  training loss: 0.10877584666013718 Validation loss 0.10766877979040146\n",
      "Epoch 17150/30000  training loss: 0.10872058570384979 Validation loss 0.1076161116361618\n",
      "Epoch 17160/30000  training loss: 0.10866539925336838 Validation loss 0.10756351798772812\n",
      "Epoch 17170/30000  training loss: 0.10861031711101532 Validation loss 0.10751103609800339\n",
      "Epoch 17180/30000  training loss: 0.10855536162853241 Validation loss 0.10745866596698761\n",
      "Epoch 17190/30000  training loss: 0.10850049555301666 Validation loss 0.10740639269351959\n",
      "Epoch 17200/30000  training loss: 0.10844571143388748 Validation loss 0.10735419392585754\n",
      "Epoch 17210/30000  training loss: 0.10839109867811203 Validation loss 0.10730218142271042\n",
      "Epoch 17220/30000  training loss: 0.10833665728569031 Validation loss 0.10725030303001404\n",
      "Epoch 17230/30000  training loss: 0.10828229784965515 Validation loss 0.10719852149486542\n",
      "Epoch 17240/30000  training loss: 0.10822803527116776 Validation loss 0.10714681446552277\n",
      "Epoch 17250/30000  training loss: 0.10817389190196991 Validation loss 0.10709524154663086\n",
      "Epoch 17260/30000  training loss: 0.10811985284090042 Validation loss 0.10704375803470612\n",
      "Epoch 17270/30000  training loss: 0.1080658882856369 Validation loss 0.10699235647916794\n",
      "Epoch 17280/30000  training loss: 0.10801202058792114 Validation loss 0.10694103688001633\n",
      "Epoch 17290/30000  training loss: 0.1079583466053009 Validation loss 0.10688993334770203\n",
      "Epoch 17300/30000  training loss: 0.1079048216342926 Validation loss 0.10683894157409668\n",
      "Epoch 17310/30000  training loss: 0.10785137861967087 Validation loss 0.1067880317568779\n",
      "Epoch 17320/30000  training loss: 0.1077980175614357 Validation loss 0.10673722624778748\n",
      "Epoch 17330/30000  training loss: 0.10774479061365128 Validation loss 0.10668652504682541\n",
      "Epoch 17340/30000  training loss: 0.10769163817167282 Validation loss 0.10663590580224991\n",
      "Epoch 17350/30000  training loss: 0.10763858258724213 Validation loss 0.10658536851406097\n",
      "Epoch 17360/30000  training loss: 0.10758563131093979 Validation loss 0.10653495043516159\n",
      "Epoch 17370/30000  training loss: 0.10753286629915237 Validation loss 0.10648472607135773\n",
      "Epoch 17380/30000  training loss: 0.10748020559549332 Validation loss 0.10643459111452103\n",
      "Epoch 17390/30000  training loss: 0.10742765665054321 Validation loss 0.10638453811407089\n",
      "Epoch 17400/30000  training loss: 0.10737521946430206 Validation loss 0.1063346117734909\n",
      "Epoch 17410/30000  training loss: 0.10732285678386688 Validation loss 0.10628476738929749\n",
      "Epoch 17420/30000  training loss: 0.10727060586214066 Validation loss 0.10623500496149063\n",
      "Epoch 17430/30000  training loss: 0.1072184145450592 Validation loss 0.10618532449007034\n",
      "Epoch 17440/30000  training loss: 0.10716637969017029 Validation loss 0.1061357706785202\n",
      "Epoch 17450/30000  training loss: 0.10711448639631271 Validation loss 0.10608638823032379\n",
      "Epoch 17460/30000  training loss: 0.10706272721290588 Validation loss 0.10603710263967514\n",
      "Epoch 17470/30000  training loss: 0.10701104998588562 Validation loss 0.10598791390657425\n",
      "Epoch 17480/30000  training loss: 0.1069594994187355 Validation loss 0.10593884438276291\n",
      "Epoch 17490/30000  training loss: 0.10690800845623016 Validation loss 0.10588983446359634\n",
      "Epoch 17500/30000  training loss: 0.10685660690069199 Validation loss 0.10584091395139694\n",
      "Epoch 17510/30000  training loss: 0.10680532455444336 Validation loss 0.1057921051979065\n",
      "Epoch 17520/30000  training loss: 0.10675414651632309 Validation loss 0.10574338585138321\n",
      "Epoch 17530/30000  training loss: 0.10670311748981476 Validation loss 0.10569483041763306\n",
      "Epoch 17540/30000  training loss: 0.10665219277143478 Validation loss 0.10564637184143066\n",
      "Epoch 17550/30000  training loss: 0.10660140961408615 Validation loss 0.10559803992509842\n",
      "Epoch 17560/30000  training loss: 0.10655070096254349 Validation loss 0.10554978251457214\n",
      "Epoch 17570/30000  training loss: 0.10650007426738739 Validation loss 0.10550160706043243\n",
      "Epoch 17580/30000  training loss: 0.10644952207803726 Validation loss 0.1054535061120987\n",
      "Epoch 17590/30000  training loss: 0.10639911890029907 Validation loss 0.1054055318236351\n",
      "Epoch 17600/30000  training loss: 0.10634877532720566 Validation loss 0.10535762459039688\n",
      "Epoch 17610/30000  training loss: 0.10629861801862717 Validation loss 0.10530990362167358\n",
      "Epoch 17620/30000  training loss: 0.10624855756759644 Validation loss 0.10526227951049805\n",
      "Epoch 17630/30000  training loss: 0.10619860887527466 Validation loss 0.10521475225687027\n",
      "Epoch 17640/30000  training loss: 0.10614874958992004 Validation loss 0.10516730695962906\n",
      "Epoch 17650/30000  training loss: 0.1060989573597908 Validation loss 0.10511993616819382\n",
      "Epoch 17660/30000  training loss: 0.10604926198720932 Validation loss 0.10507269203662872\n",
      "Epoch 17670/30000  training loss: 0.10599967837333679 Validation loss 0.10502549260854721\n",
      "Epoch 17680/30000  training loss: 0.10595018416643143 Validation loss 0.10497841984033585\n",
      "Epoch 17690/30000  training loss: 0.1059008538722992 Validation loss 0.10493148863315582\n",
      "Epoch 17700/30000  training loss: 0.1058516576886177 Validation loss 0.10488470643758774\n",
      "Epoch 17710/30000  training loss: 0.10580252856016159 Validation loss 0.10483796894550323\n",
      "Epoch 17720/30000  training loss: 0.10575349628925323 Validation loss 0.1047913208603859\n",
      "Epoch 17730/30000  training loss: 0.10570453107357025 Validation loss 0.10474474728107452\n",
      "Epoch 17740/30000  training loss: 0.10565569251775742 Validation loss 0.1046983003616333\n",
      "Epoch 17750/30000  training loss: 0.10560692846775055 Validation loss 0.10465192049741745\n",
      "Epoch 17760/30000  training loss: 0.10555825382471085 Validation loss 0.10460563004016876\n",
      "Epoch 17770/30000  training loss: 0.10550976544618607 Validation loss 0.1045595183968544\n",
      "Epoch 17780/30000  training loss: 0.10546137392520905 Validation loss 0.1045134961605072\n",
      "Epoch 17790/30000  training loss: 0.10541307181119919 Validation loss 0.10446755588054657\n",
      "Epoch 17800/30000  training loss: 0.1053648442029953 Validation loss 0.1044216901063919\n",
      "Epoch 17810/30000  training loss: 0.10531671345233917 Validation loss 0.104375921189785\n",
      "Epoch 17820/30000  training loss: 0.10526867210865021 Validation loss 0.10433024168014526\n",
      "Epoch 17830/30000  training loss: 0.1052207201719284 Validation loss 0.1042846292257309\n",
      "Epoch 17840/30000  training loss: 0.10517282783985138 Validation loss 0.10423911362886429\n",
      "Epoch 17850/30000  training loss: 0.10512516647577286 Validation loss 0.1041937917470932\n",
      "Epoch 17860/30000  training loss: 0.10507757216691971 Validation loss 0.10414854437112808\n",
      "Epoch 17870/30000  training loss: 0.10503007471561432 Validation loss 0.10410337895154953\n",
      "Epoch 17880/30000  training loss: 0.1049826368689537 Validation loss 0.10405828803777695\n",
      "Epoch 17890/30000  training loss: 0.10493533313274384 Validation loss 0.10401330888271332\n",
      "Epoch 17900/30000  training loss: 0.10488806664943695 Validation loss 0.10396838188171387\n",
      "Epoch 17910/30000  training loss: 0.10484090447425842 Validation loss 0.10392353683710098\n",
      "Epoch 17920/30000  training loss: 0.10479383170604706 Validation loss 0.10387880355119705\n",
      "Epoch 17930/30000  training loss: 0.10474696010351181 Validation loss 0.10383424907922745\n",
      "Epoch 17940/30000  training loss: 0.10470017790794373 Validation loss 0.10378974676132202\n",
      "Epoch 17950/30000  training loss: 0.10465344041585922 Validation loss 0.10374532639980316\n",
      "Epoch 17960/30000  training loss: 0.10460681468248367 Validation loss 0.10370100289583206\n",
      "Epoch 17970/30000  training loss: 0.10456027090549469 Validation loss 0.10365678369998932\n",
      "Epoch 17980/30000  training loss: 0.10451380163431168 Validation loss 0.10361260175704956\n",
      "Epoch 17990/30000  training loss: 0.10446741431951523 Validation loss 0.10356849431991577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18000/30000  training loss: 0.10442113876342773 Validation loss 0.10352452844381332\n",
      "Epoch 18010/30000  training loss: 0.10437502712011337 Validation loss 0.10348070412874222\n",
      "Epoch 18020/30000  training loss: 0.10432899743318558 Validation loss 0.10343696177005768\n",
      "Epoch 18030/30000  training loss: 0.10428304225206375 Validation loss 0.10339327901601791\n",
      "Epoch 18040/30000  training loss: 0.10423717647790909 Validation loss 0.1033497080206871\n",
      "Epoch 18050/30000  training loss: 0.10419141501188278 Validation loss 0.10330621153116226\n",
      "Epoch 18060/30000  training loss: 0.10414571315050125 Validation loss 0.10326278209686279\n",
      "Epoch 18070/30000  training loss: 0.1041000708937645 Validation loss 0.1032194122672081\n",
      "Epoch 18080/30000  training loss: 0.10405458509922028 Validation loss 0.10317618399858475\n",
      "Epoch 18090/30000  training loss: 0.1040092259645462 Validation loss 0.10313308984041214\n",
      "Epoch 18100/30000  training loss: 0.10396397113800049 Validation loss 0.1030900850892067\n",
      "Epoch 18110/30000  training loss: 0.10391876846551895 Validation loss 0.10304712504148483\n",
      "Epoch 18120/30000  training loss: 0.10387367010116577 Validation loss 0.10300429165363312\n",
      "Epoch 18130/30000  training loss: 0.10382864624261856 Validation loss 0.10296149551868439\n",
      "Epoch 18140/30000  training loss: 0.10378368943929672 Validation loss 0.10291879624128342\n",
      "Epoch 18150/30000  training loss: 0.10373881459236145 Validation loss 0.10287614911794662\n",
      "Epoch 18160/30000  training loss: 0.10369407385587692 Validation loss 0.10283364355564117\n",
      "Epoch 18170/30000  training loss: 0.10364945232868195 Validation loss 0.10279126465320587\n",
      "Epoch 18180/30000  training loss: 0.10360493510961533 Validation loss 0.10274896025657654\n",
      "Epoch 18190/30000  training loss: 0.10356048494577408 Validation loss 0.10270673036575317\n",
      "Epoch 18200/30000  training loss: 0.10351613909006119 Validation loss 0.10266461968421936\n",
      "Epoch 18210/30000  training loss: 0.10347184538841248 Validation loss 0.10262253135442734\n",
      "Epoch 18220/30000  training loss: 0.10342762619256973 Validation loss 0.10258052498102188\n",
      "Epoch 18230/30000  training loss: 0.10338350385427475 Validation loss 0.10253860801458359\n",
      "Epoch 18240/30000  training loss: 0.10333947092294693 Validation loss 0.10249679535627365\n",
      "Epoch 18250/30000  training loss: 0.10329560190439224 Validation loss 0.10245510935783386\n",
      "Epoch 18260/30000  training loss: 0.10325179249048233 Validation loss 0.10241350531578064\n",
      "Epoch 18270/30000  training loss: 0.10320808738470078 Validation loss 0.10237199068069458\n",
      "Epoch 18280/30000  training loss: 0.10316445678472519 Validation loss 0.10233055800199509\n",
      "Epoch 18290/30000  training loss: 0.10312089323997498 Validation loss 0.10228916257619858\n",
      "Epoch 18300/30000  training loss: 0.10307739675045013 Validation loss 0.10224786400794983\n",
      "Epoch 18310/30000  training loss: 0.10303398966789246 Validation loss 0.10220663994550705\n",
      "Epoch 18320/30000  training loss: 0.10299067944288254 Validation loss 0.10216551274061203\n",
      "Epoch 18330/30000  training loss: 0.10294751077890396 Validation loss 0.10212450474500656\n",
      "Epoch 18340/30000  training loss: 0.10290443152189255 Validation loss 0.10208359360694885\n",
      "Epoch 18350/30000  training loss: 0.1028614416718483 Validation loss 0.1020427718758583\n",
      "Epoch 18360/30000  training loss: 0.10281852632761002 Validation loss 0.10200200974941254\n",
      "Epoch 18370/30000  training loss: 0.10277566313743591 Validation loss 0.10196130722761154\n",
      "Epoch 18380/30000  training loss: 0.10273287445306778 Validation loss 0.10192067921161652\n",
      "Epoch 18390/30000  training loss: 0.1026901826262474 Validation loss 0.10188013315200806\n",
      "Epoch 18400/30000  training loss: 0.1026475727558136 Validation loss 0.10183967649936676\n",
      "Epoch 18410/30000  training loss: 0.10260510444641113 Validation loss 0.10179935395717621\n",
      "Epoch 18420/30000  training loss: 0.10256271809339523 Validation loss 0.10175911337137222\n",
      "Epoch 18430/30000  training loss: 0.10252043604850769 Validation loss 0.1017189621925354\n",
      "Epoch 18440/30000  training loss: 0.10247822105884552 Validation loss 0.10167887806892395\n",
      "Epoch 18450/30000  training loss: 0.10243605822324753 Validation loss 0.10163883119821548\n",
      "Epoch 18460/30000  training loss: 0.1023939698934555 Validation loss 0.10159887373447418\n",
      "Epoch 18470/30000  training loss: 0.10235197842121124 Validation loss 0.10155900567770004\n",
      "Epoch 18480/30000  training loss: 0.10231003910303116 Validation loss 0.10151919722557068\n",
      "Epoch 18490/30000  training loss: 0.1022682636976242 Validation loss 0.10147953033447266\n",
      "Epoch 18500/30000  training loss: 0.10222657024860382 Validation loss 0.1014399528503418\n",
      "Epoch 18510/30000  training loss: 0.1021849736571312 Validation loss 0.1014004498720169\n",
      "Epoch 18520/30000  training loss: 0.10214343667030334 Validation loss 0.10136102139949799\n",
      "Epoch 18530/30000  training loss: 0.10210196673870087 Validation loss 0.10132164508104324\n",
      "Epoch 18540/30000  training loss: 0.10206055641174316 Validation loss 0.10128235071897507\n",
      "Epoch 18550/30000  training loss: 0.10201924294233322 Validation loss 0.10124311596155167\n",
      "Epoch 18560/30000  training loss: 0.10197798907756805 Validation loss 0.10120396316051483\n",
      "Epoch 18570/30000  training loss: 0.10193686932325363 Validation loss 0.10116492956876755\n",
      "Epoch 18580/30000  training loss: 0.10189585387706757 Validation loss 0.10112599283456802\n",
      "Epoch 18590/30000  training loss: 0.10185492783784866 Validation loss 0.10108715295791626\n",
      "Epoch 18600/30000  training loss: 0.10181405395269394 Validation loss 0.10104834288358688\n",
      "Epoch 18610/30000  training loss: 0.10177325457334518 Validation loss 0.10100961476564407\n",
      "Epoch 18620/30000  training loss: 0.1017325296998024 Validation loss 0.10097094625234604\n",
      "Epoch 18630/30000  training loss: 0.10169187188148499 Validation loss 0.10093236714601517\n",
      "Epoch 18640/30000  training loss: 0.10165128856897354 Validation loss 0.10089384019374847\n",
      "Epoch 18650/30000  training loss: 0.10161080956459045 Validation loss 0.10085542500019073\n",
      "Epoch 18660/30000  training loss: 0.10157044231891632 Validation loss 0.10081711411476135\n",
      "Epoch 18670/30000  training loss: 0.10153017193078995 Validation loss 0.10077891498804092\n",
      "Epoch 18680/30000  training loss: 0.10148996859788895 Validation loss 0.10074073821306229\n",
      "Epoch 18690/30000  training loss: 0.10144983232021332 Validation loss 0.10070263594388962\n",
      "Epoch 18700/30000  training loss: 0.10140974074602127 Validation loss 0.10066459327936172\n",
      "Epoch 18710/30000  training loss: 0.10136976093053818 Validation loss 0.10062665492296219\n",
      "Epoch 18720/30000  training loss: 0.10132981836795807 Validation loss 0.10058873146772385\n",
      "Epoch 18730/30000  training loss: 0.10128995031118393 Validation loss 0.10055090487003326\n",
      "Epoch 18740/30000  training loss: 0.10125023871660233 Validation loss 0.10051321983337402\n",
      "Epoch 18750/30000  training loss: 0.10121062397956848 Validation loss 0.10047563165426254\n",
      "Epoch 18760/30000  training loss: 0.10117105394601822 Validation loss 0.10043808817863464\n",
      "Epoch 18770/30000  training loss: 0.10113155096769333 Validation loss 0.10040059685707092\n",
      "Epoch 18780/30000  training loss: 0.10109211504459381 Validation loss 0.10036317259073257\n",
      "Epoch 18790/30000  training loss: 0.10105276107788086 Validation loss 0.1003258228302002\n",
      "Epoch 18800/30000  training loss: 0.10101347416639328 Validation loss 0.100288525223732\n",
      "Epoch 18810/30000  training loss: 0.10097422450780869 Validation loss 0.10025128722190857\n",
      "Epoch 18820/30000  training loss: 0.10093514621257782 Validation loss 0.10021421313285828\n",
      "Epoch 18830/30000  training loss: 0.10089614242315292 Validation loss 0.10017721354961395\n",
      "Epoch 18840/30000  training loss: 0.100857213139534 Validation loss 0.1001402735710144\n",
      "Epoch 18850/30000  training loss: 0.10081833600997925 Validation loss 0.10010337829589844\n",
      "Epoch 18860/30000  training loss: 0.10077954083681107 Validation loss 0.10006655752658844\n",
      "Epoch 18870/30000  training loss: 0.10074079781770706 Validation loss 0.10002979636192322\n",
      "Epoch 18880/30000  training loss: 0.10070213675498962 Validation loss 0.09999310970306396\n",
      "Epoch 18890/30000  training loss: 0.10066352039575577 Validation loss 0.0999564677476883\n",
      "Epoch 18900/30000  training loss: 0.10062502324581146 Validation loss 0.09991993755102158\n",
      "Epoch 18910/30000  training loss: 0.1005866602063179 Validation loss 0.0998835414648056\n",
      "Epoch 18920/30000  training loss: 0.10054833441972733 Validation loss 0.09984719008207321\n",
      "Epoch 18930/30000  training loss: 0.10051009058952332 Validation loss 0.099810890853405\n",
      "Epoch 18940/30000  training loss: 0.10047188401222229 Validation loss 0.09977464377880096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18950/30000  training loss: 0.10043377429246902 Validation loss 0.09973849356174469\n",
      "Epoch 18960/30000  training loss: 0.10039571672677994 Validation loss 0.0997023805975914\n",
      "Epoch 18970/30000  training loss: 0.10035772621631622 Validation loss 0.09966632723808289\n",
      "Epoch 18980/30000  training loss: 0.1003197729587555 Validation loss 0.09963033348321915\n",
      "Epoch 18990/30000  training loss: 0.10028201341629028 Validation loss 0.09959452599287033\n",
      "Epoch 19000/30000  training loss: 0.10024431347846985 Validation loss 0.0995587408542633\n",
      "Epoch 19010/30000  training loss: 0.10020666569471359 Validation loss 0.09952302277088165\n",
      "Epoch 19020/30000  training loss: 0.10016907006502151 Validation loss 0.09948734939098358\n",
      "Epoch 19030/30000  training loss: 0.1001315638422966 Validation loss 0.09945176541805267\n",
      "Epoch 19040/30000  training loss: 0.10009410232305527 Validation loss 0.09941623359918594\n",
      "Epoch 19050/30000  training loss: 0.1000567078590393 Validation loss 0.09938075393438339\n",
      "Epoch 19060/30000  training loss: 0.10001935064792633 Validation loss 0.09934530407190323\n",
      "Epoch 19070/30000  training loss: 0.09998214989900589 Validation loss 0.0993100255727768\n",
      "Epoch 19080/30000  training loss: 0.09994504600763321 Validation loss 0.09927482903003693\n",
      "Epoch 19090/30000  training loss: 0.09990797936916351 Validation loss 0.09923966974020004\n",
      "Epoch 19100/30000  training loss: 0.09987097233533859 Validation loss 0.09920457005500793\n",
      "Epoch 19110/30000  training loss: 0.09983404725790024 Validation loss 0.0991695299744606\n",
      "Epoch 19120/30000  training loss: 0.09979718923568726 Validation loss 0.09913455694913864\n",
      "Epoch 19130/30000  training loss: 0.09976036846637726 Validation loss 0.09909962117671967\n",
      "Epoch 19140/30000  training loss: 0.09972360730171204 Validation loss 0.09906475991010666\n",
      "Epoch 19150/30000  training loss: 0.09968692064285278 Validation loss 0.09902998059988022\n",
      "Epoch 19160/30000  training loss: 0.09965039789676666 Validation loss 0.09899532794952393\n",
      "Epoch 19170/30000  training loss: 0.09961392730474472 Validation loss 0.09896073490381241\n",
      "Epoch 19180/30000  training loss: 0.09957749396562576 Validation loss 0.09892616420984268\n",
      "Epoch 19190/30000  training loss: 0.09954114258289337 Validation loss 0.09889167547225952\n",
      "Epoch 19200/30000  training loss: 0.09950485080480576 Validation loss 0.09885724633932114\n",
      "Epoch 19210/30000  training loss: 0.09946861863136292 Validation loss 0.09882288426160812\n",
      "Epoch 19220/30000  training loss: 0.09943242371082306 Validation loss 0.0987885519862175\n",
      "Epoch 19230/30000  training loss: 0.09939630329608917 Validation loss 0.09875427931547165\n",
      "Epoch 19240/30000  training loss: 0.09936030209064484 Validation loss 0.09872014820575714\n",
      "Epoch 19250/30000  training loss: 0.09932440519332886 Validation loss 0.0986860916018486\n",
      "Epoch 19260/30000  training loss: 0.09928854554891586 Validation loss 0.09865207970142365\n",
      "Epoch 19270/30000  training loss: 0.09925273060798645 Validation loss 0.09861811250448227\n",
      "Epoch 19280/30000  training loss: 0.09921702742576599 Validation loss 0.09858423471450806\n",
      "Epoch 19290/30000  training loss: 0.09918135404586792 Validation loss 0.09855039417743683\n",
      "Epoch 19300/30000  training loss: 0.09914573282003403 Validation loss 0.09851661324501038\n",
      "Epoch 19310/30000  training loss: 0.09911014139652252 Validation loss 0.09848284721374512\n",
      "Epoch 19320/30000  training loss: 0.09907464683055878 Validation loss 0.09844918549060822\n",
      "Epoch 19330/30000  training loss: 0.09903930127620697 Validation loss 0.09841568022966385\n",
      "Epoch 19340/30000  training loss: 0.09900400042533875 Validation loss 0.09838218986988068\n",
      "Epoch 19350/30000  training loss: 0.0989687591791153 Validation loss 0.09834875166416168\n",
      "Epoch 19360/30000  training loss: 0.09893356263637543 Validation loss 0.09831538051366806\n",
      "Epoch 19370/30000  training loss: 0.09889845550060272 Validation loss 0.09828206896781921\n",
      "Epoch 19380/30000  training loss: 0.098863385617733 Validation loss 0.09824880212545395\n",
      "Epoch 19390/30000  training loss: 0.09882836043834686 Validation loss 0.09821557998657227\n",
      "Epoch 19400/30000  training loss: 0.0987933948636055 Validation loss 0.09818241000175476\n",
      "Epoch 19410/30000  training loss: 0.09875853359699249 Validation loss 0.09814934432506561\n",
      "Epoch 19420/30000  training loss: 0.09872378408908844 Validation loss 0.09811639040708542\n",
      "Epoch 19430/30000  training loss: 0.09868906438350677 Validation loss 0.09808347374200821\n",
      "Epoch 19440/30000  training loss: 0.09865441918373108 Validation loss 0.09805060178041458\n",
      "Epoch 19450/30000  training loss: 0.09861984103918076 Validation loss 0.09801780432462692\n",
      "Epoch 19460/30000  training loss: 0.09858531504869461 Validation loss 0.09798504412174225\n",
      "Epoch 19470/30000  training loss: 0.09855082631111145 Validation loss 0.09795233607292175\n",
      "Epoch 19480/30000  training loss: 0.09851637482643127 Validation loss 0.09791966527700424\n",
      "Epoch 19490/30000  training loss: 0.09848202019929886 Validation loss 0.0978870689868927\n",
      "Epoch 19500/30000  training loss: 0.09844774752855301 Validation loss 0.09785458445549011\n",
      "Epoch 19510/30000  training loss: 0.09841358661651611 Validation loss 0.0978221669793129\n",
      "Epoch 19520/30000  training loss: 0.0983794555068016 Validation loss 0.09778979420661926\n",
      "Epoch 19530/30000  training loss: 0.09834539145231247 Validation loss 0.0977574959397316\n",
      "Epoch 19540/30000  training loss: 0.09831138700246811 Validation loss 0.09772525727748871\n",
      "Epoch 19550/30000  training loss: 0.09827743470668793 Validation loss 0.09769303351640701\n",
      "Epoch 19560/30000  training loss: 0.09824352711439133 Validation loss 0.09766087681055069\n",
      "Epoch 19570/30000  training loss: 0.09820964932441711 Validation loss 0.09762874245643616\n",
      "Epoch 19580/30000  training loss: 0.09817587584257126 Validation loss 0.09759669750928879\n",
      "Epoch 19590/30000  training loss: 0.09814219176769257 Validation loss 0.09756476432085037\n",
      "Epoch 19600/30000  training loss: 0.09810860455036163 Validation loss 0.09753289818763733\n",
      "Epoch 19610/30000  training loss: 0.0980750322341919 Validation loss 0.09750105440616608\n",
      "Epoch 19620/30000  training loss: 0.09804154932498932 Validation loss 0.09746930003166199\n",
      "Epoch 19630/30000  training loss: 0.09800811856985092 Validation loss 0.09743758291006088\n",
      "Epoch 19640/30000  training loss: 0.09797471761703491 Validation loss 0.09740591049194336\n",
      "Epoch 19650/30000  training loss: 0.09794136881828308 Validation loss 0.09737428277730942\n",
      "Epoch 19660/30000  training loss: 0.09790806472301483 Validation loss 0.09734269976615906\n",
      "Epoch 19670/30000  training loss: 0.09787485003471375 Validation loss 0.09731117635965347\n",
      "Epoch 19680/30000  training loss: 0.09784174710512161 Validation loss 0.09727978706359863\n",
      "Epoch 19690/30000  training loss: 0.09780868887901306 Validation loss 0.09724844247102737\n",
      "Epoch 19700/30000  training loss: 0.09777569025754929 Validation loss 0.0972171351313591\n",
      "Epoch 19710/30000  training loss: 0.09774276614189148 Validation loss 0.0971858873963356\n",
      "Epoch 19720/30000  training loss: 0.09770987182855606 Validation loss 0.09715471416711807\n",
      "Epoch 19730/30000  training loss: 0.09767702966928482 Validation loss 0.09712354838848114\n",
      "Epoch 19740/30000  training loss: 0.09764420986175537 Validation loss 0.09709243476390839\n",
      "Epoch 19750/30000  training loss: 0.09761148691177368 Validation loss 0.09706137329339981\n",
      "Epoch 19760/30000  training loss: 0.09757879376411438 Validation loss 0.09703037142753601\n",
      "Epoch 19770/30000  training loss: 0.09754623472690582 Validation loss 0.09699950367212296\n",
      "Epoch 19780/30000  training loss: 0.09751373529434204 Validation loss 0.09696865826845169\n",
      "Epoch 19790/30000  training loss: 0.09748126566410065 Validation loss 0.0969378650188446\n",
      "Epoch 19800/30000  training loss: 0.09744887799024582 Validation loss 0.09690716117620468\n",
      "Epoch 19810/30000  training loss: 0.09741653501987457 Validation loss 0.09687644988298416\n",
      "Epoch 19820/30000  training loss: 0.09738422185182571 Validation loss 0.09684580564498901\n",
      "Epoch 19830/30000  training loss: 0.09735194593667984 Validation loss 0.09681519865989685\n",
      "Epoch 19840/30000  training loss: 0.09731975197792053 Validation loss 0.09678465127944946\n",
      "Epoch 19850/30000  training loss: 0.09728759527206421 Validation loss 0.09675414860248566\n",
      "Epoch 19860/30000  training loss: 0.09725555777549744 Validation loss 0.0967237576842308\n",
      "Epoch 19870/30000  training loss: 0.09722357988357544 Validation loss 0.09669344127178192\n",
      "Epoch 19880/30000  training loss: 0.09719164669513702 Validation loss 0.09666313976049423\n",
      "Epoch 19890/30000  training loss: 0.09715979546308517 Validation loss 0.09663292020559311\n",
      "Epoch 19900/30000  training loss: 0.09712796658277512 Validation loss 0.09660273045301437\n",
      "Epoch 19910/30000  training loss: 0.09709617495536804 Validation loss 0.09657256305217743\n",
      "Epoch 19920/30000  training loss: 0.09706442058086395 Validation loss 0.09654244780540466\n",
      "Epoch 19930/30000  training loss: 0.09703274816274643 Validation loss 0.09651239216327667\n",
      "Epoch 19940/30000  training loss: 0.09700111299753189 Validation loss 0.09648238122463226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19950/30000  training loss: 0.0969695970416069 Validation loss 0.096452496945858\n",
      "Epoch 19960/30000  training loss: 0.0969381183385849 Validation loss 0.09642264246940613\n",
      "Epoch 19970/30000  training loss: 0.09690670669078827 Validation loss 0.09639285504817963\n",
      "Epoch 19980/30000  training loss: 0.09687536209821701 Validation loss 0.09636310487985611\n",
      "Epoch 19990/30000  training loss: 0.09684404730796814 Validation loss 0.09633339941501617\n",
      "Epoch 20000/30000  training loss: 0.09681277722120285 Validation loss 0.09630371630191803\n",
      "Epoch 20010/30000  training loss: 0.09678152203559875 Validation loss 0.09627407044172287\n",
      "Epoch 20020/30000  training loss: 0.09675035625696182 Validation loss 0.09624449908733368\n",
      "Epoch 20030/30000  training loss: 0.09671923518180847 Validation loss 0.09621497243642807\n",
      "Epoch 20040/30000  training loss: 0.09668819606304169 Validation loss 0.09618555009365082\n",
      "Epoch 20050/30000  training loss: 0.09665723145008087 Validation loss 0.09615615755319595\n",
      "Epoch 20060/30000  training loss: 0.09662631899118423 Validation loss 0.09612683206796646\n",
      "Epoch 20070/30000  training loss: 0.09659547358751297 Validation loss 0.09609757363796234\n",
      "Epoch 20080/30000  training loss: 0.09656465798616409 Validation loss 0.09606832265853882\n",
      "Epoch 20090/30000  training loss: 0.0965338870882988 Validation loss 0.09603912383317947\n",
      "Epoch 20100/30000  training loss: 0.09650314599275589 Validation loss 0.09600996226072311\n",
      "Epoch 20110/30000  training loss: 0.09647245705127716 Validation loss 0.09598084539175034\n",
      "Epoch 20120/30000  training loss: 0.0964418277144432 Validation loss 0.09595178812742233\n",
      "Epoch 20130/30000  training loss: 0.09641125798225403 Validation loss 0.0959227979183197\n",
      "Epoch 20140/30000  training loss: 0.09638078510761261 Validation loss 0.09589387476444244\n",
      "Epoch 20150/30000  training loss: 0.09635034948587418 Validation loss 0.09586500376462936\n",
      "Epoch 20160/30000  training loss: 0.09631999582052231 Validation loss 0.09583620727062225\n",
      "Epoch 20170/30000  training loss: 0.09628967940807343 Validation loss 0.09580743312835693\n",
      "Epoch 20180/30000  training loss: 0.09625938534736633 Validation loss 0.09577867388725281\n",
      "Epoch 20190/30000  training loss: 0.09622912108898163 Validation loss 0.09574996680021286\n",
      "Epoch 20200/30000  training loss: 0.0961989238858223 Validation loss 0.09572131186723709\n",
      "Epoch 20210/30000  training loss: 0.09616877138614655 Validation loss 0.0956926941871643\n",
      "Epoch 20220/30000  training loss: 0.09613866358995438 Validation loss 0.09566410630941391\n",
      "Epoch 20230/30000  training loss: 0.09610866755247116 Validation loss 0.09563565999269485\n",
      "Epoch 20240/30000  training loss: 0.09607871621847153 Validation loss 0.09560723602771759\n",
      "Epoch 20250/30000  training loss: 0.09604882448911667 Validation loss 0.0955788865685463\n",
      "Epoch 20260/30000  training loss: 0.09601897746324539 Validation loss 0.0955505445599556\n",
      "Epoch 20270/30000  training loss: 0.0959891602396965 Validation loss 0.09552225470542908\n",
      "Epoch 20280/30000  training loss: 0.0959593877196312 Validation loss 0.09549398720264435\n",
      "Epoch 20290/30000  training loss: 0.09592963755130768 Validation loss 0.0954657718539238\n",
      "Epoch 20300/30000  training loss: 0.09589995443820953 Validation loss 0.09543760865926743\n",
      "Epoch 20310/30000  training loss: 0.09587032347917557 Validation loss 0.09540946036577225\n",
      "Epoch 20320/30000  training loss: 0.09584074467420578 Validation loss 0.09538140892982483\n",
      "Epoch 20330/30000  training loss: 0.09581126272678375 Validation loss 0.09535342454910278\n",
      "Epoch 20340/30000  training loss: 0.0957818329334259 Validation loss 0.09532549977302551\n",
      "Epoch 20350/30000  training loss: 0.09575245529413223 Validation loss 0.09529760479927063\n",
      "Epoch 20360/30000  training loss: 0.09572310745716095 Validation loss 0.09526974707841873\n",
      "Epoch 20370/30000  training loss: 0.09569378942251205 Validation loss 0.09524191915988922\n",
      "Epoch 20380/30000  training loss: 0.09566449373960495 Validation loss 0.0952141284942627\n",
      "Epoch 20390/30000  training loss: 0.09563526511192322 Validation loss 0.09518638253211975\n",
      "Epoch 20400/30000  training loss: 0.09560608863830566 Validation loss 0.09515868127346039\n",
      "Epoch 20410/30000  training loss: 0.0955769419670105 Validation loss 0.09513100981712341\n",
      "Epoch 20420/30000  training loss: 0.09554789960384369 Validation loss 0.09510345757007599\n",
      "Epoch 20430/30000  training loss: 0.09551891684532166 Validation loss 0.09507593512535095\n",
      "Epoch 20440/30000  training loss: 0.0954899936914444 Validation loss 0.09504848718643188\n",
      "Epoch 20450/30000  training loss: 0.09546109288930893 Validation loss 0.09502104669809341\n",
      "Epoch 20460/30000  training loss: 0.09543222934007645 Validation loss 0.09499364346265793\n",
      "Epoch 20470/30000  training loss: 0.09540339559316635 Validation loss 0.09496627002954483\n",
      "Epoch 20480/30000  training loss: 0.09537459909915924 Validation loss 0.09493894875049591\n",
      "Epoch 20490/30000  training loss: 0.0953458696603775 Validation loss 0.09491166472434998\n",
      "Epoch 20500/30000  training loss: 0.09531717002391815 Validation loss 0.09488441050052643\n",
      "Epoch 20510/30000  training loss: 0.09528850764036179 Validation loss 0.09485720843076706\n",
      "Epoch 20520/30000  training loss: 0.09525997191667557 Validation loss 0.09483011066913605\n",
      "Epoch 20530/30000  training loss: 0.09523145109415054 Validation loss 0.09480305761098862\n",
      "Epoch 20540/30000  training loss: 0.09520301222801208 Validation loss 0.09477603435516357\n",
      "Epoch 20550/30000  training loss: 0.09517459571361542 Validation loss 0.09474906325340271\n",
      "Epoch 20560/30000  training loss: 0.09514619410037994 Validation loss 0.09472210705280304\n",
      "Epoch 20570/30000  training loss: 0.09511784464120865 Validation loss 0.09469517320394516\n",
      "Epoch 20580/30000  training loss: 0.09508953243494034 Validation loss 0.09466828405857086\n",
      "Epoch 20590/30000  training loss: 0.09506125748157501 Validation loss 0.09464143961668015\n",
      "Epoch 20600/30000  training loss: 0.09503303468227386 Validation loss 0.09461464732885361\n",
      "Epoch 20610/30000  training loss: 0.09500487893819809 Validation loss 0.09458790719509125\n",
      "Epoch 20620/30000  training loss: 0.09497678279876709 Validation loss 0.09456124156713486\n",
      "Epoch 20630/30000  training loss: 0.09494875371456146 Validation loss 0.09453462809324265\n",
      "Epoch 20640/30000  training loss: 0.09492077678442001 Validation loss 0.09450805932283401\n",
      "Epoch 20650/30000  training loss: 0.09489281475543976 Validation loss 0.09448151290416718\n",
      "Epoch 20660/30000  training loss: 0.09486488997936249 Validation loss 0.09445498138666153\n",
      "Epoch 20670/30000  training loss: 0.09483698010444641 Validation loss 0.09442847222089767\n",
      "Epoch 20680/30000  training loss: 0.0948091372847557 Validation loss 0.09440203756093979\n",
      "Epoch 20690/30000  training loss: 0.09478134661912918 Validation loss 0.09437563270330429\n",
      "Epoch 20700/30000  training loss: 0.09475354850292206 Validation loss 0.09434923529624939\n",
      "Epoch 20710/30000  training loss: 0.09472586959600449 Validation loss 0.09432294964790344\n",
      "Epoch 20720/30000  training loss: 0.0946982353925705 Validation loss 0.09429670125246048\n",
      "Epoch 20730/30000  training loss: 0.09467066079378128 Validation loss 0.09427052736282349\n",
      "Epoch 20740/30000  training loss: 0.09464313089847565 Validation loss 0.09424436092376709\n",
      "Epoch 20750/30000  training loss: 0.09461561590433121 Validation loss 0.09421823918819427\n",
      "Epoch 20760/30000  training loss: 0.09458813816308975 Validation loss 0.09419213235378265\n",
      "Epoch 20770/30000  training loss: 0.09456068277359009 Validation loss 0.09416605532169342\n",
      "Epoch 20780/30000  training loss: 0.0945332944393158 Validation loss 0.09414002299308777\n",
      "Epoch 20790/30000  training loss: 0.09450594335794449 Validation loss 0.0941140353679657\n",
      "Epoch 20800/30000  training loss: 0.09447860717773438 Validation loss 0.09408807009458542\n",
      "Epoch 20810/30000  training loss: 0.09445135295391083 Validation loss 0.0940621942281723\n",
      "Epoch 20820/30000  training loss: 0.09442415833473206 Validation loss 0.09403636306524277\n",
      "Epoch 20830/30000  training loss: 0.09439703822135925 Validation loss 0.09401058405637741\n",
      "Epoch 20840/30000  training loss: 0.09436993300914764 Validation loss 0.09398483484983444\n",
      "Epoch 20850/30000  training loss: 0.09434287250041962 Validation loss 0.09395910799503326\n",
      "Epoch 20860/30000  training loss: 0.09431581944227219 Validation loss 0.09393342584371567\n",
      "Epoch 20870/30000  training loss: 0.09428880363702774 Validation loss 0.09390774369239807\n",
      "Epoch 20880/30000  training loss: 0.09426184743642807 Validation loss 0.09388214349746704\n",
      "Epoch 20890/30000  training loss: 0.09423492103815079 Validation loss 0.0938565582036972\n",
      "Epoch 20900/30000  training loss: 0.0942080169916153 Validation loss 0.09383099526166916\n",
      "Epoch 20910/30000  training loss: 0.09418118745088577 Validation loss 0.09380550682544708\n",
      "Epoch 20920/30000  training loss: 0.09415442496538162 Validation loss 0.09378007054328918\n",
      "Epoch 20930/30000  training loss: 0.09412772953510284 Validation loss 0.09375470131635666\n",
      "Epoch 20940/30000  training loss: 0.09410107135772705 Validation loss 0.09372936189174652\n",
      "Epoch 20950/30000  training loss: 0.09407442063093185 Validation loss 0.09370402991771698\n",
      "Epoch 20960/30000  training loss: 0.09404779970645905 Validation loss 0.09367873519659042\n",
      "Epoch 20970/30000  training loss: 0.09402121603488922 Validation loss 0.09365346282720566\n",
      "Epoch 20980/30000  training loss: 0.09399466961622238 Validation loss 0.09362824261188507\n",
      "Epoch 20990/30000  training loss: 0.09396816045045853 Validation loss 0.09360304474830627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21000/30000  training loss: 0.09394168853759766 Validation loss 0.09357789158821106\n",
      "Epoch 21010/30000  training loss: 0.09391526132822037 Validation loss 0.09355276823043823\n",
      "Epoch 21020/30000  training loss: 0.09388891607522964 Validation loss 0.09352772682905197\n",
      "Epoch 21030/30000  training loss: 0.0938626155257225 Validation loss 0.09350273758172989\n",
      "Epoch 21040/30000  training loss: 0.09383636713027954 Validation loss 0.0934777781367302\n",
      "Epoch 21050/30000  training loss: 0.09381014108657837 Validation loss 0.09345285594463348\n",
      "Epoch 21060/30000  training loss: 0.09378394484519958 Validation loss 0.09342794120311737\n",
      "Epoch 21070/30000  training loss: 0.093757763504982 Validation loss 0.09340304136276245\n",
      "Epoch 21080/30000  training loss: 0.09373162686824799 Validation loss 0.09337820112705231\n",
      "Epoch 21090/30000  training loss: 0.09370553493499756 Validation loss 0.09335339814424515\n",
      "Epoch 21100/30000  training loss: 0.09367945790290833 Validation loss 0.09332861006259918\n",
      "Epoch 21110/30000  training loss: 0.09365341812372208 Validation loss 0.0933038517832756\n",
      "Epoch 21120/30000  training loss: 0.09362748265266418 Validation loss 0.09327919036149979\n",
      "Epoch 21130/30000  training loss: 0.09360157698392868 Validation loss 0.09325457364320755\n",
      "Epoch 21140/30000  training loss: 0.09357573091983795 Validation loss 0.0932299867272377\n",
      "Epoch 21150/30000  training loss: 0.09354991465806961 Validation loss 0.09320543706417084\n",
      "Epoch 21160/30000  training loss: 0.09352410584688187 Validation loss 0.09318089485168457\n",
      "Epoch 21170/30000  training loss: 0.0934983342885971 Validation loss 0.09315638989210129\n",
      "Epoch 21180/30000  training loss: 0.09347258508205414 Validation loss 0.0931318998336792\n",
      "Epoch 21190/30000  training loss: 0.09344689548015594 Validation loss 0.09310746192932129\n",
      "Epoch 21200/30000  training loss: 0.09342122077941895 Validation loss 0.09308305382728577\n",
      "Epoch 21210/30000  training loss: 0.09339559078216553 Validation loss 0.09305866807699203\n",
      "Epoch 21220/30000  training loss: 0.0933699980378151 Validation loss 0.09303433448076248\n",
      "Epoch 21230/30000  training loss: 0.09334447979927063 Validation loss 0.0930100753903389\n",
      "Epoch 21240/30000  training loss: 0.09331902116537094 Validation loss 0.0929858535528183\n",
      "Epoch 21250/30000  training loss: 0.09329359978437424 Validation loss 0.09296166151762009\n",
      "Epoch 21260/30000  training loss: 0.09326818585395813 Validation loss 0.09293750673532486\n",
      "Epoch 21270/30000  training loss: 0.09324280172586441 Validation loss 0.09291335940361023\n",
      "Epoch 21280/30000  training loss: 0.09321743994951248 Validation loss 0.0928892269730568\n",
      "Epoch 21290/30000  training loss: 0.09319212287664413 Validation loss 0.09286513179540634\n",
      "Epoch 21300/30000  training loss: 0.09316685050725937 Validation loss 0.09284108877182007\n",
      "Epoch 21310/30000  training loss: 0.0931415930390358 Validation loss 0.09281706064939499\n",
      "Epoch 21320/30000  training loss: 0.09311635792255402 Validation loss 0.0927930548787117\n",
      "Epoch 21330/30000  training loss: 0.09309122711420059 Validation loss 0.09276913106441498\n",
      "Epoch 21340/30000  training loss: 0.09306611865758896 Validation loss 0.09274524450302124\n",
      "Epoch 21350/30000  training loss: 0.0930410698056221 Validation loss 0.09272143244743347\n",
      "Epoch 21360/30000  training loss: 0.09301605820655823 Validation loss 0.0926976203918457\n",
      "Epoch 21370/30000  training loss: 0.09299104660749435 Validation loss 0.09267383068799973\n",
      "Epoch 21380/30000  training loss: 0.09296607226133347 Validation loss 0.09265006333589554\n",
      "Epoch 21390/30000  training loss: 0.09294111281633377 Validation loss 0.09262630343437195\n",
      "Epoch 21400/30000  training loss: 0.09291621297597885 Validation loss 0.09260260313749313\n",
      "Epoch 21410/30000  training loss: 0.09289132803678513 Validation loss 0.09257892519235611\n",
      "Epoch 21420/30000  training loss: 0.09286648035049438 Validation loss 0.09255526959896088\n",
      "Epoch 21430/30000  training loss: 0.09284164011478424 Validation loss 0.09253164380788803\n",
      "Epoch 21440/30000  training loss: 0.09281690418720245 Validation loss 0.09250809997320175\n",
      "Epoch 21450/30000  training loss: 0.09279221296310425 Validation loss 0.09248460829257965\n",
      "Epoch 21460/30000  training loss: 0.09276756644248962 Validation loss 0.09246113896369934\n",
      "Epoch 21470/30000  training loss: 0.09274294227361679 Validation loss 0.09243769943714142\n",
      "Epoch 21480/30000  training loss: 0.09271833300590515 Validation loss 0.09241426736116409\n",
      "Epoch 21490/30000  training loss: 0.0926937535405159 Validation loss 0.09239086508750916\n",
      "Epoch 21500/30000  training loss: 0.09266918897628784 Validation loss 0.09236747771501541\n",
      "Epoch 21510/30000  training loss: 0.09264467656612396 Validation loss 0.09234414994716644\n",
      "Epoch 21520/30000  training loss: 0.09262019395828247 Validation loss 0.09232082962989807\n",
      "Epoch 21530/30000  training loss: 0.09259572625160217 Validation loss 0.09229753166437149\n",
      "Epoch 21540/30000  training loss: 0.09257127344608307 Validation loss 0.0922742709517479\n",
      "Epoch 21550/30000  training loss: 0.09254694730043411 Validation loss 0.09225109964609146\n",
      "Epoch 21560/30000  training loss: 0.09252264350652695 Validation loss 0.09222795814275742\n",
      "Epoch 21570/30000  training loss: 0.09249837696552277 Validation loss 0.09220484644174576\n",
      "Epoch 21580/30000  training loss: 0.09247414022684097 Validation loss 0.09218175709247589\n",
      "Epoch 21590/30000  training loss: 0.09244991093873978 Validation loss 0.09215869009494781\n",
      "Epoch 21600/30000  training loss: 0.09242570400238037 Validation loss 0.09213563799858093\n",
      "Epoch 21610/30000  training loss: 0.09240151941776276 Validation loss 0.09211260825395584\n",
      "Epoch 21620/30000  training loss: 0.09237740188837051 Validation loss 0.09208962321281433\n",
      "Epoch 21630/30000  training loss: 0.09235329180955887 Validation loss 0.09206665307283401\n",
      "Epoch 21640/30000  training loss: 0.09232919663190842 Validation loss 0.09204372018575668\n",
      "Epoch 21650/30000  training loss: 0.09230512380599976 Validation loss 0.09202078729867935\n",
      "Epoch 21660/30000  training loss: 0.09228115528821945 Validation loss 0.09199795871973038\n",
      "Epoch 21670/30000  training loss: 0.09225723147392273 Validation loss 0.09197516739368439\n",
      "Epoch 21680/30000  training loss: 0.0922333374619484 Validation loss 0.09195241332054138\n",
      "Epoch 21690/30000  training loss: 0.09220946580171585 Validation loss 0.09192965924739838\n",
      "Epoch 21700/30000  training loss: 0.0921856164932251 Validation loss 0.09190694242715836\n",
      "Epoch 21710/30000  training loss: 0.09216178953647614 Validation loss 0.09188423305749893\n",
      "Epoch 21720/30000  training loss: 0.09213797003030777 Validation loss 0.0918615311384201\n",
      "Epoch 21730/30000  training loss: 0.09211420267820358 Validation loss 0.09183889627456665\n",
      "Epoch 21740/30000  training loss: 0.09209045767784119 Validation loss 0.09181626886129379\n",
      "Epoch 21750/30000  training loss: 0.09206673502922058 Validation loss 0.09179367125034332\n",
      "Epoch 21760/30000  training loss: 0.09204303473234177 Validation loss 0.09177107363939285\n",
      "Epoch 21770/30000  training loss: 0.09201941639184952 Validation loss 0.09174857288599014\n",
      "Epoch 21780/30000  training loss: 0.09199584275484085 Validation loss 0.09172610938549042\n",
      "Epoch 21790/30000  training loss: 0.09197231382131577 Validation loss 0.09170369058847427\n",
      "Epoch 21800/30000  training loss: 0.09194880723953247 Validation loss 0.09168127924203873\n",
      "Epoch 21810/30000  training loss: 0.09192531555891037 Validation loss 0.09165887534618378\n",
      "Epoch 21820/30000  training loss: 0.09190184623003006 Validation loss 0.09163650870323181\n",
      "Epoch 21830/30000  training loss: 0.09187839180231094 Validation loss 0.09161414206027985\n",
      "Epoch 21840/30000  training loss: 0.09185496717691422 Validation loss 0.09159181267023087\n",
      "Epoch 21850/30000  training loss: 0.09183157980442047 Validation loss 0.09156953543424606\n",
      "Epoch 21860/30000  training loss: 0.09180820733308792 Validation loss 0.09154725074768066\n",
      "Epoch 21870/30000  training loss: 0.09178487211465836 Validation loss 0.09152499586343765\n",
      "Epoch 21880/30000  training loss: 0.09176157414913177 Validation loss 0.09150278568267822\n",
      "Epoch 21890/30000  training loss: 0.09173835068941116 Validation loss 0.09148064255714417\n",
      "Epoch 21900/30000  training loss: 0.09171517193317413 Validation loss 0.09145854413509369\n",
      "Epoch 21910/30000  training loss: 0.0916920080780983 Validation loss 0.0914364606142044\n",
      "Epoch 21920/30000  training loss: 0.09166887402534485 Validation loss 0.09141439199447632\n",
      "Epoch 21930/30000  training loss: 0.091645747423172 Validation loss 0.09139233827590942\n",
      "Epoch 21940/30000  training loss: 0.09162263572216034 Validation loss 0.09137029200792313\n",
      "Epoch 21950/30000  training loss: 0.09159955382347107 Validation loss 0.09134828299283981\n",
      "Epoch 21960/30000  training loss: 0.09157652407884598 Validation loss 0.09132631123065948\n",
      "Epoch 21970/30000  training loss: 0.09155350178480148 Validation loss 0.09130435436964035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21980/30000  training loss: 0.09153048694133759 Validation loss 0.09128241240978241\n",
      "Epoch 21990/30000  training loss: 0.09150752425193787 Validation loss 0.09126047790050507\n",
      "Epoch 22000/30000  training loss: 0.09148461371660233 Validation loss 0.09123865514993668\n",
      "Epoch 22010/30000  training loss: 0.09146174788475037 Validation loss 0.09121683239936829\n",
      "Epoch 22020/30000  training loss: 0.09143893420696259 Validation loss 0.09119507670402527\n",
      "Epoch 22030/30000  training loss: 0.091416135430336 Validation loss 0.09117331355810165\n",
      "Epoch 22040/30000  training loss: 0.0913933515548706 Validation loss 0.09115158021450043\n",
      "Epoch 22050/30000  training loss: 0.0913705825805664 Validation loss 0.0911298617720604\n",
      "Epoch 22060/30000  training loss: 0.0913478434085846 Validation loss 0.09110815078020096\n",
      "Epoch 22070/30000  training loss: 0.09132511913776398 Validation loss 0.09108646214008331\n",
      "Epoch 22080/30000  training loss: 0.09130243957042694 Validation loss 0.09106481075286865\n",
      "Epoch 22090/30000  training loss: 0.0912797749042511 Validation loss 0.09104319661855698\n",
      "Epoch 22100/30000  training loss: 0.09125712513923645 Validation loss 0.0910215899348259\n",
      "Epoch 22110/30000  training loss: 0.091234490275383 Validation loss 0.09099998325109482\n",
      "Epoch 22120/30000  training loss: 0.0912119597196579 Validation loss 0.0909784808754921\n",
      "Epoch 22130/30000  training loss: 0.091189444065094 Validation loss 0.09095700085163116\n",
      "Epoch 22140/30000  training loss: 0.09116698056459427 Validation loss 0.09093555063009262\n",
      "Epoch 22150/30000  training loss: 0.09114453196525574 Validation loss 0.09091411530971527\n",
      "Epoch 22160/30000  training loss: 0.0911220982670784 Validation loss 0.09089270979166031\n",
      "Epoch 22170/30000  training loss: 0.09109967947006226 Validation loss 0.09087130427360535\n",
      "Epoch 22180/30000  training loss: 0.0910772755742073 Validation loss 0.09084991365671158\n",
      "Epoch 22190/30000  training loss: 0.09105489403009415 Validation loss 0.0908285453915596\n",
      "Epoch 22200/30000  training loss: 0.09103256464004517 Validation loss 0.0908072218298912\n",
      "Epoch 22210/30000  training loss: 0.09101024270057678 Validation loss 0.090785913169384\n",
      "Epoch 22220/30000  training loss: 0.09098793566226959 Validation loss 0.0907646045088768\n",
      "Epoch 22230/30000  training loss: 0.09096565842628479 Validation loss 0.09074333310127258\n",
      "Epoch 22240/30000  training loss: 0.09094345569610596 Validation loss 0.09072213619947433\n",
      "Epoch 22250/30000  training loss: 0.09092128276824951 Validation loss 0.09070096909999847\n",
      "Epoch 22260/30000  training loss: 0.09089916199445724 Validation loss 0.0906798392534256\n",
      "Epoch 22270/30000  training loss: 0.09087704867124557 Validation loss 0.09065871685743332\n",
      "Epoch 22280/30000  training loss: 0.0908549576997757 Validation loss 0.09063760936260223\n",
      "Epoch 22290/30000  training loss: 0.09083286672830582 Validation loss 0.09061650931835175\n",
      "Epoch 22300/30000  training loss: 0.09081080555915833 Validation loss 0.09059543162584305\n",
      "Epoch 22310/30000  training loss: 0.09078875184059143 Validation loss 0.09057436138391495\n",
      "Epoch 22320/30000  training loss: 0.09076675027608871 Validation loss 0.09055335074663162\n",
      "Epoch 22330/30000  training loss: 0.09074476361274719 Validation loss 0.0905323401093483\n",
      "Epoch 22340/30000  training loss: 0.09072278439998627 Validation loss 0.09051135182380676\n",
      "Epoch 22350/30000  training loss: 0.09070083498954773 Validation loss 0.09049035608768463\n",
      "Epoch 22360/30000  training loss: 0.09067895263433456 Validation loss 0.09046946465969086\n",
      "Epoch 22370/30000  training loss: 0.09065711498260498 Validation loss 0.09044858813285828\n",
      "Epoch 22380/30000  training loss: 0.09063531458377838 Validation loss 0.09042776376008987\n",
      "Epoch 22390/30000  training loss: 0.09061353653669357 Validation loss 0.09040693938732147\n",
      "Epoch 22400/30000  training loss: 0.09059175848960876 Validation loss 0.09038613736629486\n",
      "Epoch 22410/30000  training loss: 0.09057000279426575 Validation loss 0.09036534279584885\n",
      "Epoch 22420/30000  training loss: 0.09054826200008392 Validation loss 0.09034455567598343\n",
      "Epoch 22430/30000  training loss: 0.0905265361070633 Validation loss 0.0903237909078598\n",
      "Epoch 22440/30000  training loss: 0.09050484001636505 Validation loss 0.09030305594205856\n",
      "Epoch 22450/30000  training loss: 0.0904831811785698 Validation loss 0.09028235077857971\n",
      "Epoch 22460/30000  training loss: 0.09046153724193573 Validation loss 0.09026165306568146\n",
      "Epoch 22470/30000  training loss: 0.09043989330530167 Validation loss 0.0902409628033638\n",
      "Epoch 22480/30000  training loss: 0.09041830897331238 Validation loss 0.09022034704685211\n",
      "Epoch 22490/30000  training loss: 0.09039679169654846 Validation loss 0.09019974619150162\n",
      "Epoch 22500/30000  training loss: 0.09037528932094574 Validation loss 0.0901792049407959\n",
      "Epoch 22510/30000  training loss: 0.0903538167476654 Validation loss 0.09015865623950958\n",
      "Epoch 22520/30000  training loss: 0.09033235907554626 Validation loss 0.09013814479112625\n",
      "Epoch 22530/30000  training loss: 0.09031093120574951 Validation loss 0.09011764824390411\n",
      "Epoch 22540/30000  training loss: 0.09028949588537216 Validation loss 0.09009715169668198\n",
      "Epoch 22550/30000  training loss: 0.0902680829167366 Validation loss 0.09007667005062103\n",
      "Epoch 22560/30000  training loss: 0.09024668484926224 Validation loss 0.09005620330572128\n",
      "Epoch 22570/30000  training loss: 0.09022533893585205 Validation loss 0.09003578126430511\n",
      "Epoch 22580/30000  training loss: 0.09020399302244186 Validation loss 0.09001536667346954\n",
      "Epoch 22590/30000  training loss: 0.09018267691135406 Validation loss 0.08999496698379517\n",
      "Epoch 22600/30000  training loss: 0.09016136825084686 Validation loss 0.08997458964586258\n",
      "Epoch 22610/30000  training loss: 0.09014014154672623 Validation loss 0.08995428681373596\n",
      "Epoch 22620/30000  training loss: 0.09011893719434738 Validation loss 0.08993399888277054\n",
      "Epoch 22630/30000  training loss: 0.09009777009487152 Validation loss 0.0899137482047081\n",
      "Epoch 22640/30000  training loss: 0.09007661789655685 Validation loss 0.08989351242780685\n",
      "Epoch 22650/30000  training loss: 0.09005550295114517 Validation loss 0.0898732841014862\n",
      "Epoch 22660/30000  training loss: 0.0900343656539917 Validation loss 0.08985307812690735\n",
      "Epoch 22670/30000  training loss: 0.09001326560974121 Validation loss 0.0898328572511673\n",
      "Epoch 22680/30000  training loss: 0.08999217301607132 Validation loss 0.08981268107891083\n",
      "Epoch 22690/30000  training loss: 0.08997109532356262 Validation loss 0.08979251980781555\n",
      "Epoch 22700/30000  training loss: 0.0899500623345375 Validation loss 0.08977237343788147\n",
      "Epoch 22710/30000  training loss: 0.08992903679609299 Validation loss 0.08975224196910858\n",
      "Epoch 22720/30000  training loss: 0.08990802615880966 Validation loss 0.08973214775323868\n",
      "Epoch 22730/30000  training loss: 0.08988704532384872 Validation loss 0.08971206843852997\n",
      "Epoch 22740/30000  training loss: 0.08986614644527435 Validation loss 0.08969204872846603\n",
      "Epoch 22750/30000  training loss: 0.08984524011611938 Validation loss 0.0896720439195633\n",
      "Epoch 22760/30000  training loss: 0.08982440084218979 Validation loss 0.08965208381414413\n",
      "Epoch 22770/30000  training loss: 0.0898035541176796 Validation loss 0.08963212370872498\n",
      "Epoch 22780/30000  training loss: 0.08978273719549179 Validation loss 0.08961217850446701\n",
      "Epoch 22790/30000  training loss: 0.08976191282272339 Validation loss 0.08959224820137024\n",
      "Epoch 22800/30000  training loss: 0.08974111825227737 Validation loss 0.08957233279943466\n",
      "Epoch 22810/30000  training loss: 0.08972033858299255 Validation loss 0.08955243229866028\n",
      "Epoch 22820/30000  training loss: 0.08969957381486893 Validation loss 0.0895325317978859\n",
      "Epoch 22830/30000  training loss: 0.08967884629964828 Validation loss 0.08951268345117569\n",
      "Epoch 22840/30000  training loss: 0.08965813368558884 Validation loss 0.08949284255504608\n",
      "Epoch 22850/30000  training loss: 0.08963742107152939 Validation loss 0.08947300165891647\n",
      "Epoch 22860/30000  training loss: 0.08961673825979233 Validation loss 0.08945319056510925\n",
      "Epoch 22870/30000  training loss: 0.08959612995386124 Validation loss 0.0894334614276886\n",
      "Epoch 22880/30000  training loss: 0.08957554399967194 Validation loss 0.08941373229026794\n",
      "Epoch 22890/30000  training loss: 0.08955499529838562 Validation loss 0.08939404785633087\n",
      "Epoch 22900/30000  training loss: 0.0895344614982605 Validation loss 0.0893743634223938\n",
      "Epoch 22910/30000  training loss: 0.08951393514871597 Validation loss 0.08935468643903732\n",
      "Epoch 22920/30000  training loss: 0.08949342370033264 Validation loss 0.08933502435684204\n",
      "Epoch 22930/30000  training loss: 0.0894729271531105 Validation loss 0.08931539207696915\n",
      "Epoch 22940/30000  training loss: 0.08945243060588837 Validation loss 0.08929573744535446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22950/30000  training loss: 0.08943194150924683 Validation loss 0.08927612751722336\n",
      "Epoch 22960/30000  training loss: 0.08941152691841125 Validation loss 0.08925653249025345\n",
      "Epoch 22970/30000  training loss: 0.08939111232757568 Validation loss 0.08923696726560593\n",
      "Epoch 22980/30000  training loss: 0.08937069773674011 Validation loss 0.08921739459037781\n",
      "Epoch 22990/30000  training loss: 0.08935029059648514 Validation loss 0.08919783681631088\n",
      "Epoch 23000/30000  training loss: 0.08932997286319733 Validation loss 0.08917837589979172\n",
      "Epoch 23010/30000  training loss: 0.0893096849322319 Validation loss 0.08915890753269196\n",
      "Epoch 23020/30000  training loss: 0.08928939700126648 Validation loss 0.08913946151733398\n",
      "Epoch 23030/30000  training loss: 0.08926916122436523 Validation loss 0.0891200602054596\n",
      "Epoch 23040/30000  training loss: 0.08924892544746399 Validation loss 0.08910065144300461\n",
      "Epoch 23050/30000  training loss: 0.08922869712114334 Validation loss 0.08908125758171082\n",
      "Epoch 23060/30000  training loss: 0.08920849859714508 Validation loss 0.08906187117099762\n",
      "Epoch 23070/30000  training loss: 0.08918827772140503 Validation loss 0.08904249966144562\n",
      "Epoch 23080/30000  training loss: 0.08916810154914856 Validation loss 0.08902312815189362\n",
      "Epoch 23090/30000  training loss: 0.08914794027805328 Validation loss 0.0890038013458252\n",
      "Epoch 23100/30000  training loss: 0.0891278013586998 Validation loss 0.08898448199033737\n",
      "Epoch 23110/30000  training loss: 0.08910767734050751 Validation loss 0.08896517008543015\n",
      "Epoch 23120/30000  training loss: 0.08908756077289581 Validation loss 0.08894587308168411\n",
      "Epoch 23130/30000  training loss: 0.0890674889087677 Validation loss 0.08892662823200226\n",
      "Epoch 23140/30000  training loss: 0.08904747664928436 Validation loss 0.08890742063522339\n",
      "Epoch 23150/30000  training loss: 0.08902747184038162 Validation loss 0.08888822793960571\n",
      "Epoch 23160/30000  training loss: 0.08900749683380127 Validation loss 0.08886905759572983\n",
      "Epoch 23170/30000  training loss: 0.08898754417896271 Validation loss 0.08884990215301514\n",
      "Epoch 23180/30000  training loss: 0.08896760642528534 Validation loss 0.08883077651262283\n",
      "Epoch 23190/30000  training loss: 0.08894766867160797 Validation loss 0.08881165087223053\n",
      "Epoch 23200/30000  training loss: 0.0889277532696724 Validation loss 0.08879252523183823\n",
      "Epoch 23210/30000  training loss: 0.08890783786773682 Validation loss 0.08877340704202652\n",
      "Epoch 23220/30000  training loss: 0.08888794481754303 Validation loss 0.0887543186545372\n",
      "Epoch 23230/30000  training loss: 0.08886807411909103 Validation loss 0.08873524516820908\n",
      "Epoch 23240/30000  training loss: 0.08884822577238083 Validation loss 0.08871617913246155\n",
      "Epoch 23250/30000  training loss: 0.08882838487625122 Validation loss 0.088697150349617\n",
      "Epoch 23260/30000  training loss: 0.0888085663318634 Validation loss 0.08867811411619186\n",
      "Epoch 23270/30000  training loss: 0.08878879249095917 Validation loss 0.0886591300368309\n",
      "Epoch 23280/30000  training loss: 0.08876906335353851 Validation loss 0.08864017575979233\n",
      "Epoch 23290/30000  training loss: 0.08874934166669846 Validation loss 0.08862123638391495\n",
      "Epoch 23300/30000  training loss: 0.08872964233160019 Validation loss 0.08860234171152115\n",
      "Epoch 23310/30000  training loss: 0.0887099876999855 Validation loss 0.08858343958854675\n",
      "Epoch 23320/30000  training loss: 0.08869032561779022 Validation loss 0.08856455981731415\n",
      "Epoch 23330/30000  training loss: 0.08867067098617554 Validation loss 0.08854568749666214\n",
      "Epoch 23340/30000  training loss: 0.08865103125572205 Validation loss 0.08852682262659073\n",
      "Epoch 23350/30000  training loss: 0.08863139897584915 Validation loss 0.08850795775651932\n",
      "Epoch 23360/30000  training loss: 0.08861178159713745 Validation loss 0.0884891226887703\n",
      "Epoch 23370/30000  training loss: 0.08859218657016754 Validation loss 0.08847030252218246\n",
      "Epoch 23380/30000  training loss: 0.08857262134552002 Validation loss 0.08845148980617523\n",
      "Epoch 23390/30000  training loss: 0.0885530561208725 Validation loss 0.0884326919913292\n",
      "Epoch 23400/30000  training loss: 0.08853350579738617 Validation loss 0.08841390907764435\n",
      "Epoch 23410/30000  training loss: 0.08851400017738342 Validation loss 0.08839517086744308\n",
      "Epoch 23420/30000  training loss: 0.08849454671144485 Validation loss 0.0883764699101448\n",
      "Epoch 23430/30000  training loss: 0.08847510814666748 Validation loss 0.08835777640342712\n",
      "Epoch 23440/30000  training loss: 0.0884556770324707 Validation loss 0.08833912014961243\n",
      "Epoch 23450/30000  training loss: 0.08843628317117691 Validation loss 0.08832048624753952\n",
      "Epoch 23460/30000  training loss: 0.08841689676046371 Validation loss 0.08830184489488602\n",
      "Epoch 23470/30000  training loss: 0.08839751780033112 Validation loss 0.08828321844339371\n",
      "Epoch 23480/30000  training loss: 0.08837815374135971 Validation loss 0.0882645919919014\n",
      "Epoch 23490/30000  training loss: 0.0883587971329689 Validation loss 0.08824598044157028\n",
      "Epoch 23500/30000  training loss: 0.0883394405245781 Validation loss 0.08822736889123917\n",
      "Epoch 23510/30000  training loss: 0.08832010626792908 Validation loss 0.08820878714323044\n",
      "Epoch 23520/30000  training loss: 0.08830080181360245 Validation loss 0.0881902202963829\n",
      "Epoch 23530/30000  training loss: 0.08828151226043701 Validation loss 0.08817167580127716\n",
      "Epoch 23540/30000  training loss: 0.08826222270727158 Validation loss 0.08815313130617142\n",
      "Epoch 23550/30000  training loss: 0.08824297785758972 Validation loss 0.08813460916280746\n",
      "Epoch 23560/30000  training loss: 0.08822377771139145 Validation loss 0.08811616897583008\n",
      "Epoch 23570/30000  training loss: 0.08820460736751556 Validation loss 0.0880977138876915\n",
      "Epoch 23580/30000  training loss: 0.08818543702363968 Validation loss 0.08807926625013351\n",
      "Epoch 23590/30000  training loss: 0.08816629648208618 Validation loss 0.08806087076663971\n",
      "Epoch 23600/30000  training loss: 0.08814717084169388 Validation loss 0.08804246783256531\n",
      "Epoch 23610/30000  training loss: 0.08812805265188217 Validation loss 0.0880240797996521\n",
      "Epoch 23620/30000  training loss: 0.08810894936323166 Validation loss 0.08800570666790009\n",
      "Epoch 23630/30000  training loss: 0.08808986097574234 Validation loss 0.08798731863498688\n",
      "Epoch 23640/30000  training loss: 0.08807077258825302 Validation loss 0.08796894550323486\n",
      "Epoch 23650/30000  training loss: 0.0880516842007637 Validation loss 0.08795059472322464\n",
      "Epoch 23660/30000  training loss: 0.08803262561559677 Validation loss 0.08793225884437561\n",
      "Epoch 23670/30000  training loss: 0.08801359683275223 Validation loss 0.08791393041610718\n",
      "Epoch 23680/30000  training loss: 0.08799456804990768 Validation loss 0.08789562433958054\n",
      "Epoch 23690/30000  training loss: 0.08797555416822433 Validation loss 0.0878773108124733\n",
      "Epoch 23700/30000  training loss: 0.08795658499002457 Validation loss 0.08785905689001083\n",
      "Epoch 23710/30000  training loss: 0.08793766796588898 Validation loss 0.08784084767103195\n",
      "Epoch 23720/30000  training loss: 0.08791875094175339 Validation loss 0.08782263100147247\n",
      "Epoch 23730/30000  training loss: 0.0878998413681984 Validation loss 0.08780442923307419\n",
      "Epoch 23740/30000  training loss: 0.08788096904754639 Validation loss 0.08778627216815948\n",
      "Epoch 23750/30000  training loss: 0.08786210417747498 Validation loss 0.08776810020208359\n",
      "Epoch 23760/30000  training loss: 0.08784326165914536 Validation loss 0.08774995803833008\n",
      "Epoch 23770/30000  training loss: 0.08782441914081573 Validation loss 0.08773180842399597\n",
      "Epoch 23780/30000  training loss: 0.08780557662248611 Validation loss 0.08771365880966187\n",
      "Epoch 23790/30000  training loss: 0.08778674900531769 Validation loss 0.08769553899765015\n",
      "Epoch 23800/30000  training loss: 0.08776793628931046 Validation loss 0.08767740428447723\n",
      "Epoch 23810/30000  training loss: 0.08774913102388382 Validation loss 0.08765929937362671\n",
      "Epoch 23820/30000  training loss: 0.08773034811019897 Validation loss 0.08764122426509857\n",
      "Epoch 23830/30000  training loss: 0.08771159499883652 Validation loss 0.08762312680482864\n",
      "Epoch 23840/30000  training loss: 0.08769282698631287 Validation loss 0.0876050665974617\n",
      "Epoch 23850/30000  training loss: 0.08767412602901459 Validation loss 0.08758705109357834\n",
      "Epoch 23860/30000  training loss: 0.0876554623246193 Validation loss 0.08756905049085617\n",
      "Epoch 23870/30000  training loss: 0.087636798620224 Validation loss 0.08755107969045639\n",
      "Epoch 23880/30000  training loss: 0.0876181498169899 Validation loss 0.08753310143947601\n",
      "Epoch 23890/30000  training loss: 0.08759951591491699 Validation loss 0.08751515299081802\n",
      "Epoch 23900/30000  training loss: 0.08758091181516647 Validation loss 0.08749721944332123\n",
      "Epoch 23910/30000  training loss: 0.08756232261657715 Validation loss 0.08747930824756622\n",
      "Epoch 23920/30000  training loss: 0.08754372596740723 Validation loss 0.08746137470006943\n",
      "Epoch 23930/30000  training loss: 0.0875251367688179 Validation loss 0.08744347095489502\n",
      "Epoch 23940/30000  training loss: 0.08750656992197037 Validation loss 0.08742555975914001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23950/30000  training loss: 0.08748800307512283 Validation loss 0.0874076634645462\n",
      "Epoch 23960/30000  training loss: 0.0874694362282753 Validation loss 0.08738977462053299\n",
      "Epoch 23970/30000  training loss: 0.08745090663433075 Validation loss 0.08737190812826157\n",
      "Epoch 23980/30000  training loss: 0.0874323919415474 Validation loss 0.08735404908657074\n",
      "Epoch 23990/30000  training loss: 0.08741389214992523 Validation loss 0.0873362123966217\n",
      "Epoch 24000/30000  training loss: 0.08739539980888367 Validation loss 0.08731838315725327\n",
      "Epoch 24010/30000  training loss: 0.08737697452306747 Validation loss 0.0873006209731102\n",
      "Epoch 24020/30000  training loss: 0.08735856413841248 Validation loss 0.08728285878896713\n",
      "Epoch 24030/30000  training loss: 0.08734015375375748 Validation loss 0.08726511150598526\n",
      "Epoch 24040/30000  training loss: 0.08732175081968307 Validation loss 0.08724736422300339\n",
      "Epoch 24050/30000  training loss: 0.08730339258909225 Validation loss 0.0872296616435051\n",
      "Epoch 24060/30000  training loss: 0.08728504925966263 Validation loss 0.08721195161342621\n",
      "Epoch 24070/30000  training loss: 0.087266705930233 Validation loss 0.08719425648450851\n",
      "Epoch 24080/30000  training loss: 0.08724836260080338 Validation loss 0.08717655390501022\n",
      "Epoch 24090/30000  training loss: 0.08723003417253494 Validation loss 0.08715886622667313\n",
      "Epoch 24100/30000  training loss: 0.08721170574426651 Validation loss 0.08714118599891663\n",
      "Epoch 24110/30000  training loss: 0.08719339221715927 Validation loss 0.08712350577116013\n",
      "Epoch 24120/30000  training loss: 0.08717507869005203 Validation loss 0.08710583299398422\n",
      "Epoch 24130/30000  training loss: 0.08715680241584778 Validation loss 0.0870881900191307\n",
      "Epoch 24140/30000  training loss: 0.08713853359222412 Validation loss 0.08707056939601898\n",
      "Epoch 24150/30000  training loss: 0.08712027221918106 Validation loss 0.08705295622348785\n",
      "Epoch 24160/30000  training loss: 0.08710204809904099 Validation loss 0.08703537285327911\n",
      "Epoch 24170/30000  training loss: 0.08708387613296509 Validation loss 0.08701782673597336\n",
      "Epoch 24180/30000  training loss: 0.08706571161746979 Validation loss 0.0870002880692482\n",
      "Epoch 24190/30000  training loss: 0.08704754710197449 Validation loss 0.08698274195194244\n",
      "Epoch 24200/30000  training loss: 0.08702940493822098 Validation loss 0.08696522563695908\n",
      "Epoch 24210/30000  training loss: 0.08701128512620926 Validation loss 0.0869477391242981\n",
      "Epoch 24220/30000  training loss: 0.08699317276477814 Validation loss 0.08693024516105652\n",
      "Epoch 24230/30000  training loss: 0.08697506785392761 Validation loss 0.08691277354955673\n",
      "Epoch 24240/30000  training loss: 0.08695697784423828 Validation loss 0.08689528703689575\n",
      "Epoch 24250/30000  training loss: 0.08693888038396835 Validation loss 0.08687782287597656\n",
      "Epoch 24260/30000  training loss: 0.08692080527544022 Validation loss 0.08686035126447678\n",
      "Epoch 24270/30000  training loss: 0.08690273016691208 Validation loss 0.08684289455413818\n",
      "Epoch 24280/30000  training loss: 0.08688464760780334 Validation loss 0.08682543784379959\n",
      "Epoch 24290/30000  training loss: 0.08686660975217819 Validation loss 0.08680801838636398\n",
      "Epoch 24300/30000  training loss: 0.08684857934713364 Validation loss 0.08679059147834778\n",
      "Epoch 24310/30000  training loss: 0.08683055639266968 Validation loss 0.08677318692207336\n",
      "Epoch 24320/30000  training loss: 0.0868125706911087 Validation loss 0.08675579726696014\n",
      "Epoch 24330/30000  training loss: 0.08679463714361191 Validation loss 0.0867384746670723\n",
      "Epoch 24340/30000  training loss: 0.08677670359611511 Validation loss 0.08672114461660385\n",
      "Epoch 24350/30000  training loss: 0.08675877749919891 Validation loss 0.0867038294672966\n",
      "Epoch 24360/30000  training loss: 0.08674086630344391 Validation loss 0.08668651431798935\n",
      "Epoch 24370/30000  training loss: 0.0867229700088501 Validation loss 0.08666922152042389\n",
      "Epoch 24380/30000  training loss: 0.08670509606599808 Validation loss 0.08665195107460022\n",
      "Epoch 24390/30000  training loss: 0.08668722957372665 Validation loss 0.08663466572761536\n",
      "Epoch 24400/30000  training loss: 0.08666937053203583 Validation loss 0.08661740273237228\n",
      "Epoch 24410/30000  training loss: 0.0866515189409256 Validation loss 0.08660013973712921\n",
      "Epoch 24420/30000  training loss: 0.08663365989923477 Validation loss 0.08658288419246674\n",
      "Epoch 24430/30000  training loss: 0.08661581575870514 Validation loss 0.08656562119722366\n",
      "Epoch 24440/30000  training loss: 0.0865979790687561 Validation loss 0.08654838800430298\n",
      "Epoch 24450/30000  training loss: 0.08658014237880707 Validation loss 0.0865311473608017\n",
      "Epoch 24460/30000  training loss: 0.08656235784292221 Validation loss 0.0865139365196228\n",
      "Epoch 24470/30000  training loss: 0.08654456585645676 Validation loss 0.0864967331290245\n",
      "Epoch 24480/30000  training loss: 0.0865267738699913 Validation loss 0.08647952973842621\n",
      "Epoch 24490/30000  training loss: 0.08650904893875122 Validation loss 0.08646238595247269\n",
      "Epoch 24500/30000  training loss: 0.08649135380983353 Validation loss 0.08644527941942215\n",
      "Epoch 24510/30000  training loss: 0.08647365123033524 Validation loss 0.08642815798521042\n",
      "Epoch 24520/30000  training loss: 0.08645597100257874 Validation loss 0.08641105145215988\n",
      "Epoch 24530/30000  training loss: 0.08643828332424164 Validation loss 0.08639393001794815\n",
      "Epoch 24540/30000  training loss: 0.08642063289880753 Validation loss 0.08637685328722\n",
      "Epoch 24550/30000  training loss: 0.08640298247337341 Validation loss 0.08635978400707245\n",
      "Epoch 24560/30000  training loss: 0.08638535439968109 Validation loss 0.08634272962808609\n",
      "Epoch 24570/30000  training loss: 0.08636772632598877 Validation loss 0.08632566034793854\n",
      "Epoch 24580/30000  training loss: 0.08635009080171585 Validation loss 0.08630859851837158\n",
      "Epoch 24590/30000  training loss: 0.08633247762918472 Validation loss 0.08629154413938522\n",
      "Epoch 24600/30000  training loss: 0.086314857006073 Validation loss 0.08627449721097946\n",
      "Epoch 24610/30000  training loss: 0.08629724383354187 Validation loss 0.0862574577331543\n",
      "Epoch 24620/30000  training loss: 0.08627964556217194 Validation loss 0.08624042570590973\n",
      "Epoch 24630/30000  training loss: 0.08626208454370499 Validation loss 0.08622342348098755\n",
      "Epoch 24640/30000  training loss: 0.08624452352523804 Validation loss 0.08620642125606537\n",
      "Epoch 24650/30000  training loss: 0.08622696250677109 Validation loss 0.08618941903114319\n",
      "Epoch 24660/30000  training loss: 0.08620947599411011 Validation loss 0.08617249876260757\n",
      "Epoch 24670/30000  training loss: 0.08619200438261032 Validation loss 0.08615557849407196\n",
      "Epoch 24680/30000  training loss: 0.08617453277111053 Validation loss 0.08613866567611694\n",
      "Epoch 24690/30000  training loss: 0.08615707606077194 Validation loss 0.08612175285816193\n",
      "Epoch 24700/30000  training loss: 0.08613961189985275 Validation loss 0.0861048474907875\n",
      "Epoch 24710/30000  training loss: 0.08612217754125595 Validation loss 0.08608794957399368\n",
      "Epoch 24720/30000  training loss: 0.08610475063323975 Validation loss 0.08607108891010284\n",
      "Epoch 24730/30000  training loss: 0.08608733862638474 Validation loss 0.086054228246212\n",
      "Epoch 24740/30000  training loss: 0.08606993407011032 Validation loss 0.08603736013174057\n",
      "Epoch 24750/30000  training loss: 0.0860525369644165 Validation loss 0.08602049201726913\n",
      "Epoch 24760/30000  training loss: 0.08603513240814209 Validation loss 0.08600364625453949\n",
      "Epoch 24770/30000  training loss: 0.08601774275302887 Validation loss 0.08598679304122925\n",
      "Epoch 24780/30000  training loss: 0.08600036054849625 Validation loss 0.0859699472784996\n",
      "Epoch 24790/30000  training loss: 0.08598297089338303 Validation loss 0.08595310151576996\n",
      "Epoch 24800/30000  training loss: 0.0859656110405922 Validation loss 0.08593627065420151\n",
      "Epoch 24810/30000  training loss: 0.08594827353954315 Validation loss 0.08591948449611664\n",
      "Epoch 24820/30000  training loss: 0.08593093603849411 Validation loss 0.08590267598628998\n",
      "Epoch 24830/30000  training loss: 0.08591364324092865 Validation loss 0.0858859196305275\n",
      "Epoch 24840/30000  training loss: 0.08589639514684677 Validation loss 0.085869200527668\n",
      "Epoch 24850/30000  training loss: 0.0858791321516037 Validation loss 0.0858524814248085\n",
      "Epoch 24860/30000  training loss: 0.08586189150810242 Validation loss 0.08583574742078781\n",
      "Epoch 24870/30000  training loss: 0.08584465086460114 Validation loss 0.08581904321908951\n",
      "Epoch 24880/30000  training loss: 0.08582741022109985 Validation loss 0.08580233156681061\n",
      "Epoch 24890/30000  training loss: 0.08581019937992096 Validation loss 0.0857856422662735\n",
      "Epoch 24900/30000  training loss: 0.08579300343990326 Validation loss 0.08576897531747818\n",
      "Epoch 24910/30000  training loss: 0.08577581495046616 Validation loss 0.08575230836868286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24920/30000  training loss: 0.08575862646102905 Validation loss 0.08573563396930695\n",
      "Epoch 24930/30000  training loss: 0.08574144542217255 Validation loss 0.08571897447109222\n",
      "Epoch 24940/30000  training loss: 0.08572427183389664 Validation loss 0.0857023075222969\n",
      "Epoch 24950/30000  training loss: 0.08570708334445953 Validation loss 0.08568565547466278\n",
      "Epoch 24960/30000  training loss: 0.08568991720676422 Validation loss 0.08566900342702866\n",
      "Epoch 24970/30000  training loss: 0.08567275106906891 Validation loss 0.08565234392881393\n",
      "Epoch 24980/30000  training loss: 0.08565561473369598 Validation loss 0.0856357291340828\n",
      "Epoch 24990/30000  training loss: 0.08563849329948425 Validation loss 0.08561911433935165\n",
      "Epoch 25000/30000  training loss: 0.08562136441469193 Validation loss 0.08560249954462051\n",
      "Epoch 25010/30000  training loss: 0.08560430258512497 Validation loss 0.08558595925569534\n",
      "Epoch 25020/30000  training loss: 0.0855872705578804 Validation loss 0.08556941896677017\n",
      "Epoch 25030/30000  training loss: 0.08557023853063583 Validation loss 0.085552878677845\n",
      "Epoch 25040/30000  training loss: 0.08555319905281067 Validation loss 0.08553635329008102\n",
      "Epoch 25050/30000  training loss: 0.0855361744761467 Validation loss 0.08551983535289764\n",
      "Epoch 25060/30000  training loss: 0.08551914244890213 Validation loss 0.08550331741571426\n",
      "Epoch 25070/30000  training loss: 0.08550214767456055 Validation loss 0.08548679947853088\n",
      "Epoch 25080/30000  training loss: 0.08548516035079956 Validation loss 0.08547032624483109\n",
      "Epoch 25090/30000  training loss: 0.08546818047761917 Validation loss 0.08545383810997009\n",
      "Epoch 25100/30000  training loss: 0.08545120060443878 Validation loss 0.0854373648762703\n",
      "Epoch 25110/30000  training loss: 0.08543423563241959 Validation loss 0.0854208692908287\n",
      "Epoch 25120/30000  training loss: 0.08541727066040039 Validation loss 0.0854044035077095\n",
      "Epoch 25130/30000  training loss: 0.0854003056883812 Validation loss 0.0853879302740097\n",
      "Epoch 25140/30000  training loss: 0.0853833481669426 Validation loss 0.0853714719414711\n",
      "Epoch 25150/30000  training loss: 0.0853663831949234 Validation loss 0.0853550061583519\n",
      "Epoch 25160/30000  training loss: 0.08534944802522659 Validation loss 0.0853385478258133\n",
      "Epoch 25170/30000  training loss: 0.08533252775669098 Validation loss 0.08532212674617767\n",
      "Epoch 25180/30000  training loss: 0.08531562238931656 Validation loss 0.08530569821596146\n",
      "Epoch 25190/30000  training loss: 0.08529873937368393 Validation loss 0.08528931438922882\n",
      "Epoch 25200/30000  training loss: 0.08528190106153488 Validation loss 0.08527295291423798\n",
      "Epoch 25210/30000  training loss: 0.08526507019996643 Validation loss 0.08525661379098892\n",
      "Epoch 25220/30000  training loss: 0.08524824678897858 Validation loss 0.08524026721715927\n",
      "Epoch 25230/30000  training loss: 0.08523143082857132 Validation loss 0.08522390574216843\n",
      "Epoch 25240/30000  training loss: 0.08521461486816406 Validation loss 0.08520757406949997\n",
      "Epoch 25250/30000  training loss: 0.0851977989077568 Validation loss 0.08519124984741211\n",
      "Epoch 25260/30000  training loss: 0.08518100529909134 Validation loss 0.08517493307590485\n",
      "Epoch 25270/30000  training loss: 0.08516423404216766 Validation loss 0.08515863120555878\n",
      "Epoch 25280/30000  training loss: 0.08514747023582458 Validation loss 0.0851423367857933\n",
      "Epoch 25290/30000  training loss: 0.08513069897890091 Validation loss 0.08512602746486664\n",
      "Epoch 25300/30000  training loss: 0.08511393517255783 Validation loss 0.08510974794626236\n",
      "Epoch 25310/30000  training loss: 0.08509717136621475 Validation loss 0.08509345352649689\n",
      "Epoch 25320/30000  training loss: 0.08508041501045227 Validation loss 0.08507716655731201\n",
      "Epoch 25330/30000  training loss: 0.08506366610527039 Validation loss 0.08506087958812714\n",
      "Epoch 25340/30000  training loss: 0.0850469097495079 Validation loss 0.08504459261894226\n",
      "Epoch 25350/30000  training loss: 0.08503017574548721 Validation loss 0.08502832055091858\n",
      "Epoch 25360/30000  training loss: 0.08501346409320831 Validation loss 0.08501207828521729\n",
      "Epoch 25370/30000  training loss: 0.08499675244092941 Validation loss 0.0849958211183548\n",
      "Epoch 25380/30000  training loss: 0.08498010039329529 Validation loss 0.08497963845729828\n",
      "Epoch 25390/30000  training loss: 0.08496345579624176 Validation loss 0.08496346324682236\n",
      "Epoch 25400/30000  training loss: 0.08494684100151062 Validation loss 0.08494728803634644\n",
      "Epoch 25410/30000  training loss: 0.08493022620677948 Validation loss 0.08493112027645111\n",
      "Epoch 25420/30000  training loss: 0.08491359651088715 Validation loss 0.08491495996713638\n",
      "Epoch 25430/30000  training loss: 0.0848969891667366 Validation loss 0.08489879220724106\n",
      "Epoch 25440/30000  training loss: 0.08488038182258606 Validation loss 0.08488263189792633\n",
      "Epoch 25450/30000  training loss: 0.08486378192901611 Validation loss 0.08486650884151459\n",
      "Epoch 25460/30000  training loss: 0.08484720438718796 Validation loss 0.08485037833452225\n",
      "Epoch 25470/30000  training loss: 0.0848306342959404 Validation loss 0.08483424782752991\n",
      "Epoch 25480/30000  training loss: 0.08481407165527344 Validation loss 0.08481813967227936\n",
      "Epoch 25490/30000  training loss: 0.08479750901460648 Validation loss 0.08480201661586761\n",
      "Epoch 25500/30000  training loss: 0.08478094637393951 Validation loss 0.08478590101003647\n",
      "Epoch 25510/30000  training loss: 0.08476439118385315 Validation loss 0.08476979285478592\n",
      "Epoch 25520/30000  training loss: 0.08474783599376678 Validation loss 0.08475369215011597\n",
      "Epoch 25530/30000  training loss: 0.08473129570484161 Validation loss 0.08473758399486542\n",
      "Epoch 25540/30000  training loss: 0.08471474796533585 Validation loss 0.08472146838903427\n",
      "Epoch 25550/30000  training loss: 0.08469821512699127 Validation loss 0.08470539003610611\n",
      "Epoch 25560/30000  training loss: 0.08468171209096909 Validation loss 0.08468931913375854\n",
      "Epoch 25570/30000  training loss: 0.0846652165055275 Validation loss 0.08467327058315277\n",
      "Epoch 25580/30000  training loss: 0.08464878797531128 Validation loss 0.08465727418661118\n",
      "Epoch 25590/30000  training loss: 0.08463235199451447 Validation loss 0.08464127779006958\n",
      "Epoch 25600/30000  training loss: 0.08461593091487885 Validation loss 0.08462528139352798\n",
      "Epoch 25610/30000  training loss: 0.08459950983524323 Validation loss 0.08460929989814758\n",
      "Epoch 25620/30000  training loss: 0.0845830887556076 Validation loss 0.08459329605102539\n",
      "Epoch 25630/30000  training loss: 0.08456667512655258 Validation loss 0.08457731455564499\n",
      "Epoch 25640/30000  training loss: 0.08455026149749756 Validation loss 0.08456134051084518\n",
      "Epoch 25650/30000  training loss: 0.08453387022018433 Validation loss 0.08454537391662598\n",
      "Epoch 25660/30000  training loss: 0.08451748639345169 Validation loss 0.08452942967414856\n",
      "Epoch 25670/30000  training loss: 0.08450112491846085 Validation loss 0.08451346307992935\n",
      "Epoch 25680/30000  training loss: 0.08448474854230881 Validation loss 0.08449751883745193\n",
      "Epoch 25690/30000  training loss: 0.08446837961673737 Validation loss 0.08448157459497452\n",
      "Epoch 25700/30000  training loss: 0.08445201814174652 Validation loss 0.0844656378030777\n",
      "Epoch 25710/30000  training loss: 0.08443566411733627 Validation loss 0.08444969356060028\n",
      "Epoch 25720/30000  training loss: 0.08441930264234543 Validation loss 0.08443375676870346\n",
      "Epoch 25730/30000  training loss: 0.08440295606851578 Validation loss 0.08441781997680664\n",
      "Epoch 25740/30000  training loss: 0.08438660204410553 Validation loss 0.08440189808607101\n",
      "Epoch 25750/30000  training loss: 0.08437026292085648 Validation loss 0.08438597619533539\n",
      "Epoch 25760/30000  training loss: 0.08435394614934921 Validation loss 0.08437006920576096\n",
      "Epoch 25770/30000  training loss: 0.08433764427900314 Validation loss 0.08435419201850891\n",
      "Epoch 25780/30000  training loss: 0.08432140946388245 Validation loss 0.08433837443590164\n",
      "Epoch 25790/30000  training loss: 0.08430517464876175 Validation loss 0.08432254195213318\n",
      "Epoch 25800/30000  training loss: 0.08428893983364105 Validation loss 0.08430672436952591\n",
      "Epoch 25810/30000  training loss: 0.08427270501852036 Validation loss 0.08429089188575745\n",
      "Epoch 25820/30000  training loss: 0.08425647765398026 Validation loss 0.08427507430315018\n",
      "Epoch 25830/30000  training loss: 0.08424025774002075 Validation loss 0.08425925672054291\n",
      "Epoch 25840/30000  training loss: 0.08422403782606125 Validation loss 0.08424343913793564\n",
      "Epoch 25850/30000  training loss: 0.08420782536268234 Validation loss 0.08422762155532837\n",
      "Epoch 25860/30000  training loss: 0.08419162780046463 Validation loss 0.08421184122562408\n",
      "Epoch 25870/30000  training loss: 0.08417544513940811 Validation loss 0.0841960683465004\n",
      "Epoch 25880/30000  training loss: 0.08415926992893219 Validation loss 0.08418027311563492\n",
      "Epoch 25890/30000  training loss: 0.08414308726787567 Validation loss 0.08416450023651123\n",
      "Epoch 25900/30000  training loss: 0.08412691205739975 Validation loss 0.08414872735738754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25910/30000  training loss: 0.08411074429750443 Validation loss 0.08413295447826385\n",
      "Epoch 25920/30000  training loss: 0.0840945765376091 Validation loss 0.08411717414855957\n",
      "Epoch 25930/30000  training loss: 0.08407841622829437 Validation loss 0.08410141617059708\n",
      "Epoch 25940/30000  training loss: 0.08406224846839905 Validation loss 0.08408564329147339\n",
      "Epoch 25950/30000  training loss: 0.08404608815908432 Validation loss 0.0840698853135109\n",
      "Epoch 25960/30000  training loss: 0.08402994275093079 Validation loss 0.084054134786129\n",
      "Epoch 25970/30000  training loss: 0.08401382714509964 Validation loss 0.0840383917093277\n",
      "Epoch 25980/30000  training loss: 0.08399773389101028 Validation loss 0.08402270823717117\n",
      "Epoch 25990/30000  training loss: 0.08398168534040451 Validation loss 0.08400703221559525\n",
      "Epoch 26000/30000  training loss: 0.08396563678979874 Validation loss 0.08399137109518051\n",
      "Epoch 26010/30000  training loss: 0.08394959568977356 Validation loss 0.08397572487592697\n",
      "Epoch 26020/30000  training loss: 0.08393355458974838 Validation loss 0.08396005630493164\n",
      "Epoch 26030/30000  training loss: 0.0839175209403038 Validation loss 0.0839444100856781\n",
      "Epoch 26040/30000  training loss: 0.08390147238969803 Validation loss 0.08392874151468277\n",
      "Epoch 26050/30000  training loss: 0.08388544619083405 Validation loss 0.08391309529542923\n",
      "Epoch 26060/30000  training loss: 0.08386940509080887 Validation loss 0.08389744907617569\n",
      "Epoch 26070/30000  training loss: 0.08385339379310608 Validation loss 0.08388182520866394\n",
      "Epoch 26080/30000  training loss: 0.08383741229772568 Validation loss 0.08386620134115219\n",
      "Epoch 26090/30000  training loss: 0.08382141590118408 Validation loss 0.08385058492422104\n",
      "Epoch 26100/30000  training loss: 0.08380541950464249 Validation loss 0.08383497595787048\n",
      "Epoch 26110/30000  training loss: 0.08378943055868149 Validation loss 0.08381935209035873\n",
      "Epoch 26120/30000  training loss: 0.08377344161272049 Validation loss 0.08380375057458878\n",
      "Epoch 26130/30000  training loss: 0.08375746011734009 Validation loss 0.08378813415765762\n",
      "Epoch 26140/30000  training loss: 0.08374147862195969 Validation loss 0.08377252519130707\n",
      "Epoch 26150/30000  training loss: 0.08372550457715988 Validation loss 0.0837569311261177\n",
      "Epoch 26160/30000  training loss: 0.08370952308177948 Validation loss 0.08374132961034775\n",
      "Epoch 26170/30000  training loss: 0.08369355648756027 Validation loss 0.08372572064399719\n",
      "Epoch 26180/30000  training loss: 0.08367761224508286 Validation loss 0.08371014147996902\n",
      "Epoch 26190/30000  training loss: 0.08366168290376663 Validation loss 0.08369458466768265\n",
      "Epoch 26200/30000  training loss: 0.08364581316709518 Validation loss 0.08367908746004105\n",
      "Epoch 26210/30000  training loss: 0.08362995088100433 Validation loss 0.08366359025239944\n",
      "Epoch 26220/30000  training loss: 0.08361408859491348 Validation loss 0.08364809304475784\n",
      "Epoch 26230/30000  training loss: 0.08359822630882263 Validation loss 0.08363258838653564\n",
      "Epoch 26240/30000  training loss: 0.08358237147331238 Validation loss 0.08361707627773285\n",
      "Epoch 26250/30000  training loss: 0.08356651663780212 Validation loss 0.08360158652067184\n",
      "Epoch 26260/30000  training loss: 0.08355066180229187 Validation loss 0.08358608186244965\n",
      "Epoch 26270/30000  training loss: 0.08353480696678162 Validation loss 0.08357059955596924\n",
      "Epoch 26280/30000  training loss: 0.08351896703243256 Validation loss 0.08355510234832764\n",
      "Epoch 26290/30000  training loss: 0.08350314199924469 Validation loss 0.08353964239358902\n",
      "Epoch 26300/30000  training loss: 0.08348733186721802 Validation loss 0.083524189889431\n",
      "Epoch 26310/30000  training loss: 0.08347152173519135 Validation loss 0.08350872993469238\n",
      "Epoch 26320/30000  training loss: 0.08345571160316467 Validation loss 0.08349326997995377\n",
      "Epoch 26330/30000  training loss: 0.0834398940205574 Validation loss 0.08347781002521515\n",
      "Epoch 26340/30000  training loss: 0.08342409133911133 Validation loss 0.08346236497163773\n",
      "Epoch 26350/30000  training loss: 0.08340829610824585 Validation loss 0.08344690501689911\n",
      "Epoch 26360/30000  training loss: 0.08339249342679977 Validation loss 0.08343145996332169\n",
      "Epoch 26370/30000  training loss: 0.0833766907453537 Validation loss 0.08341600745916367\n",
      "Epoch 26380/30000  training loss: 0.08336090296506882 Validation loss 0.08340056985616684\n",
      "Epoch 26390/30000  training loss: 0.08334510773420334 Validation loss 0.08338511735200882\n",
      "Epoch 26400/30000  training loss: 0.08332933485507965 Validation loss 0.08336969465017319\n",
      "Epoch 26410/30000  training loss: 0.08331359177827835 Validation loss 0.08335429430007935\n",
      "Epoch 26420/30000  training loss: 0.08329789340496063 Validation loss 0.08333894610404968\n",
      "Epoch 26430/30000  training loss: 0.08328220993280411 Validation loss 0.08332359790802002\n",
      "Epoch 26440/30000  training loss: 0.08326652646064758 Validation loss 0.08330825716257095\n",
      "Epoch 26450/30000  training loss: 0.08325085043907166 Validation loss 0.08329291641712189\n",
      "Epoch 26460/30000  training loss: 0.08323515951633453 Validation loss 0.08327756822109222\n",
      "Epoch 26470/30000  training loss: 0.0832194834947586 Validation loss 0.08326222747564316\n",
      "Epoch 26480/30000  training loss: 0.08320380747318268 Validation loss 0.0832468792796135\n",
      "Epoch 26490/30000  training loss: 0.08318813145160675 Validation loss 0.08323154598474503\n",
      "Epoch 26500/30000  training loss: 0.08317246288061142 Validation loss 0.08321620523929596\n",
      "Epoch 26510/30000  training loss: 0.08315679430961609 Validation loss 0.08320087194442749\n",
      "Epoch 26520/30000  training loss: 0.08314115554094315 Validation loss 0.083185575902462\n",
      "Epoch 26530/30000  training loss: 0.0831255167722702 Validation loss 0.08317027986049652\n",
      "Epoch 26540/30000  training loss: 0.08310987800359726 Validation loss 0.08315496146678925\n",
      "Epoch 26550/30000  training loss: 0.08309425413608551 Validation loss 0.08313967287540436\n",
      "Epoch 26560/30000  training loss: 0.08307862281799316 Validation loss 0.08312435448169708\n",
      "Epoch 26570/30000  training loss: 0.08306299895048141 Validation loss 0.0831090658903122\n",
      "Epoch 26580/30000  training loss: 0.08304736763238907 Validation loss 0.08309376239776611\n",
      "Epoch 26590/30000  training loss: 0.08303175121545792 Validation loss 0.08307847380638123\n",
      "Epoch 26600/30000  training loss: 0.08301613479852676 Validation loss 0.08306317776441574\n",
      "Epoch 26610/30000  training loss: 0.08300051093101501 Validation loss 0.08304788172245026\n",
      "Epoch 26620/30000  training loss: 0.08298489451408386 Validation loss 0.08303259313106537\n",
      "Epoch 26630/30000  training loss: 0.0829692929983139 Validation loss 0.08301731199026108\n",
      "Epoch 26640/30000  training loss: 0.08295372873544693 Validation loss 0.08300208300352097\n",
      "Epoch 26650/30000  training loss: 0.08293821662664413 Validation loss 0.08298688381910324\n",
      "Epoch 26660/30000  training loss: 0.08292270451784134 Validation loss 0.08297169953584671\n",
      "Epoch 26670/30000  training loss: 0.08290719240903854 Validation loss 0.08295648545026779\n",
      "Epoch 26680/30000  training loss: 0.08289168775081635 Validation loss 0.08294130861759186\n",
      "Epoch 26690/30000  training loss: 0.08287618309259415 Validation loss 0.08292611688375473\n",
      "Epoch 26700/30000  training loss: 0.08286067098379135 Validation loss 0.082910917699337\n",
      "Epoch 26710/30000  training loss: 0.08284517377614975 Validation loss 0.08289573341608047\n",
      "Epoch 26720/30000  training loss: 0.08282966911792755 Validation loss 0.08288053423166275\n",
      "Epoch 26730/30000  training loss: 0.08281417191028595 Validation loss 0.08286534994840622\n",
      "Epoch 26740/30000  training loss: 0.08279867470264435 Validation loss 0.08285017311573029\n",
      "Epoch 26750/30000  training loss: 0.08278319239616394 Validation loss 0.08283500373363495\n",
      "Epoch 26760/30000  training loss: 0.08276773244142532 Validation loss 0.0828198492527008\n",
      "Epoch 26770/30000  training loss: 0.08275226503610611 Validation loss 0.08280469477176666\n",
      "Epoch 26780/30000  training loss: 0.08273681253194809 Validation loss 0.08278954029083252\n",
      "Epoch 26790/30000  training loss: 0.08272135257720947 Validation loss 0.08277438580989838\n",
      "Epoch 26800/30000  training loss: 0.08270589262247086 Validation loss 0.08275923132896423\n",
      "Epoch 26810/30000  training loss: 0.08269044011831284 Validation loss 0.08274409174919128\n",
      "Epoch 26820/30000  training loss: 0.08267498761415482 Validation loss 0.08272894471883774\n",
      "Epoch 26830/30000  training loss: 0.08265954256057739 Validation loss 0.08271379768848419\n",
      "Epoch 26840/30000  training loss: 0.08264408260583878 Validation loss 0.08269864320755005\n",
      "Epoch 26850/30000  training loss: 0.08262863755226135 Validation loss 0.0826835036277771\n",
      "Epoch 26860/30000  training loss: 0.08261319249868393 Validation loss 0.08266835659742355\n",
      "Epoch 26870/30000  training loss: 0.0825977623462677 Validation loss 0.0826532319188118\n",
      "Epoch 26880/30000  training loss: 0.08258239924907684 Validation loss 0.08263818174600601\n",
      "Epoch 26890/30000  training loss: 0.08256705850362778 Validation loss 0.08262313157320023\n",
      "Epoch 26900/30000  training loss: 0.08255171030759811 Validation loss 0.08260808140039444\n",
      "Epoch 26910/30000  training loss: 0.08253637701272964 Validation loss 0.08259303122758865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26920/30000  training loss: 0.08252103626728058 Validation loss 0.08257798105478287\n",
      "Epoch 26930/30000  training loss: 0.08250570297241211 Validation loss 0.08256293833255768\n",
      "Epoch 26940/30000  training loss: 0.08249036222696304 Validation loss 0.08254789561033249\n",
      "Epoch 26950/30000  training loss: 0.08247502893209457 Validation loss 0.0825328528881073\n",
      "Epoch 26960/30000  training loss: 0.0824596956372261 Validation loss 0.0825178250670433\n",
      "Epoch 26970/30000  training loss: 0.08244437724351883 Validation loss 0.08250278234481812\n",
      "Epoch 26980/30000  training loss: 0.08242904394865036 Validation loss 0.08248773962259293\n",
      "Epoch 26990/30000  training loss: 0.08241371810436249 Validation loss 0.08247269690036774\n",
      "Epoch 27000/30000  training loss: 0.0823984146118164 Validation loss 0.08245769143104553\n",
      "Epoch 27010/30000  training loss: 0.08238312602043152 Validation loss 0.08244268596172333\n",
      "Epoch 27020/30000  training loss: 0.08236782252788544 Validation loss 0.08242767304182053\n",
      "Epoch 27030/30000  training loss: 0.08235254138708115 Validation loss 0.08241267502307892\n",
      "Epoch 27040/30000  training loss: 0.08233724534511566 Validation loss 0.08239766210317612\n",
      "Epoch 27050/30000  training loss: 0.08232196420431137 Validation loss 0.08238266408443451\n",
      "Epoch 27060/30000  training loss: 0.08230667561292648 Validation loss 0.08236765116453171\n",
      "Epoch 27070/30000  training loss: 0.08229139447212219 Validation loss 0.0823526605963707\n",
      "Epoch 27080/30000  training loss: 0.0822761058807373 Validation loss 0.08233766257762909\n",
      "Epoch 27090/30000  training loss: 0.08226082473993301 Validation loss 0.08232265710830688\n",
      "Epoch 27100/30000  training loss: 0.08224555104970932 Validation loss 0.08230765163898468\n",
      "Epoch 27110/30000  training loss: 0.08223026990890503 Validation loss 0.08229266107082367\n",
      "Epoch 27120/30000  training loss: 0.08221501857042313 Validation loss 0.08227770030498505\n",
      "Epoch 27130/30000  training loss: 0.08219984173774719 Validation loss 0.0822627916932106\n",
      "Epoch 27140/30000  training loss: 0.08218466490507126 Validation loss 0.08224788308143616\n",
      "Epoch 27150/30000  training loss: 0.08216949552297592 Validation loss 0.0822329893708229\n",
      "Epoch 27160/30000  training loss: 0.08215431869029999 Validation loss 0.08221808820962906\n",
      "Epoch 27170/30000  training loss: 0.08213914930820465 Validation loss 0.08220317959785461\n",
      "Epoch 27180/30000  training loss: 0.08212397247552872 Validation loss 0.08218827843666077\n",
      "Epoch 27190/30000  training loss: 0.08210879564285278 Validation loss 0.08217337727546692\n",
      "Epoch 27200/30000  training loss: 0.08209363371133804 Validation loss 0.08215847611427307\n",
      "Epoch 27210/30000  training loss: 0.0820784643292427 Validation loss 0.08214357495307922\n",
      "Epoch 27220/30000  training loss: 0.08206330239772797 Validation loss 0.08212868869304657\n",
      "Epoch 27230/30000  training loss: 0.08204813301563263 Validation loss 0.08211378753185272\n",
      "Epoch 27240/30000  training loss: 0.08203297853469849 Validation loss 0.08209890127182007\n",
      "Epoch 27250/30000  training loss: 0.08201783895492554 Validation loss 0.08208401501178741\n",
      "Epoch 27260/30000  training loss: 0.08200270682573318 Validation loss 0.08206915855407715\n",
      "Epoch 27270/30000  training loss: 0.08198758214712143 Validation loss 0.08205429464578629\n",
      "Epoch 27280/30000  training loss: 0.08197245001792908 Validation loss 0.08203943073749542\n",
      "Epoch 27290/30000  training loss: 0.08195732533931732 Validation loss 0.08202455192804337\n",
      "Epoch 27300/30000  training loss: 0.08194219321012497 Validation loss 0.0820097029209137\n",
      "Epoch 27310/30000  training loss: 0.08192706853151321 Validation loss 0.08199483156204224\n",
      "Epoch 27320/30000  training loss: 0.08191195130348206 Validation loss 0.08197998255491257\n",
      "Epoch 27330/30000  training loss: 0.0818968266248703 Validation loss 0.0819651186466217\n",
      "Epoch 27340/30000  training loss: 0.08188171684741974 Validation loss 0.08195025473833084\n",
      "Epoch 27350/30000  training loss: 0.08186660706996918 Validation loss 0.08193540573120117\n",
      "Epoch 27360/30000  training loss: 0.08185149729251862 Validation loss 0.08192054182291031\n",
      "Epoch 27370/30000  training loss: 0.08183637261390686 Validation loss 0.08190568536520004\n",
      "Epoch 27380/30000  training loss: 0.08182132244110107 Validation loss 0.08189091086387634\n",
      "Epoch 27390/30000  training loss: 0.08180630207061768 Validation loss 0.08187612891197205\n",
      "Epoch 27400/30000  training loss: 0.08179128915071487 Validation loss 0.08186136931180954\n",
      "Epoch 27410/30000  training loss: 0.08177627623081207 Validation loss 0.08184661716222763\n",
      "Epoch 27420/30000  training loss: 0.08176126331090927 Validation loss 0.08183185756206512\n",
      "Epoch 27430/30000  training loss: 0.08174625784158707 Validation loss 0.08181709051132202\n",
      "Epoch 27440/30000  training loss: 0.08173124492168427 Validation loss 0.08180233091115952\n",
      "Epoch 27450/30000  training loss: 0.08171623945236206 Validation loss 0.08178757131099701\n",
      "Epoch 27460/30000  training loss: 0.08170122653245926 Validation loss 0.0817728042602539\n",
      "Epoch 27470/30000  training loss: 0.08168622106313705 Validation loss 0.0817580446600914\n",
      "Epoch 27480/30000  training loss: 0.08167122304439545 Validation loss 0.08174329251050949\n",
      "Epoch 27490/30000  training loss: 0.08165621012449265 Validation loss 0.08172854036092758\n",
      "Epoch 27500/30000  training loss: 0.08164121210575104 Validation loss 0.08171377331018448\n",
      "Epoch 27510/30000  training loss: 0.08162622153759003 Validation loss 0.08169903606176376\n",
      "Epoch 27520/30000  training loss: 0.0816112533211708 Validation loss 0.08168429881334305\n",
      "Epoch 27530/30000  training loss: 0.08159629255533218 Validation loss 0.08166958391666412\n",
      "Epoch 27540/30000  training loss: 0.08158130943775177 Validation loss 0.0816548615694046\n",
      "Epoch 27550/30000  training loss: 0.08156634867191315 Validation loss 0.08164013177156448\n",
      "Epoch 27560/30000  training loss: 0.08155138790607452 Validation loss 0.08162540197372437\n",
      "Epoch 27570/30000  training loss: 0.0815364271402359 Validation loss 0.08161068707704544\n",
      "Epoch 27580/30000  training loss: 0.08152145892381668 Validation loss 0.08159594982862473\n",
      "Epoch 27590/30000  training loss: 0.08150649815797806 Validation loss 0.0815812349319458\n",
      "Epoch 27600/30000  training loss: 0.08149153739213943 Validation loss 0.08156651258468628\n",
      "Epoch 27610/30000  training loss: 0.08147657662630081 Validation loss 0.08155180513858795\n",
      "Epoch 27620/30000  training loss: 0.08146162331104279 Validation loss 0.08153707534074783\n",
      "Epoch 27630/30000  training loss: 0.08144666999578476 Validation loss 0.08152236044406891\n",
      "Epoch 27640/30000  training loss: 0.08143173158168793 Validation loss 0.08150766789913177\n",
      "Epoch 27650/30000  training loss: 0.08141686767339706 Validation loss 0.08149302750825882\n",
      "Epoch 27660/30000  training loss: 0.0814020037651062 Validation loss 0.08147839456796646\n",
      "Epoch 27670/30000  training loss: 0.08138714730739594 Validation loss 0.0814637616276741\n",
      "Epoch 27680/30000  training loss: 0.08137229084968567 Validation loss 0.08144913613796234\n",
      "Epoch 27690/30000  training loss: 0.0813574343919754 Validation loss 0.08143451064825058\n",
      "Epoch 27700/30000  training loss: 0.08134257793426514 Validation loss 0.08141988515853882\n",
      "Epoch 27710/30000  training loss: 0.08132772892713547 Validation loss 0.08140526711940765\n",
      "Epoch 27720/30000  training loss: 0.0813128799200058 Validation loss 0.08139064162969589\n",
      "Epoch 27730/30000  training loss: 0.08129803091287613 Validation loss 0.08137603104114532\n",
      "Epoch 27740/30000  training loss: 0.08128318190574646 Validation loss 0.08136139065027237\n",
      "Epoch 27750/30000  training loss: 0.08126834034919739 Validation loss 0.0813467875123024\n",
      "Epoch 27760/30000  training loss: 0.08125349134206772 Validation loss 0.08133216202259064\n",
      "Epoch 27770/30000  training loss: 0.08123864233493805 Validation loss 0.08131753653287888\n",
      "Epoch 27780/30000  training loss: 0.08122380077838898 Validation loss 0.08130291104316711\n",
      "Epoch 27790/30000  training loss: 0.0812089741230011 Validation loss 0.08128832280635834\n",
      "Epoch 27800/30000  training loss: 0.08119416981935501 Validation loss 0.08127373456954956\n",
      "Epoch 27810/30000  training loss: 0.08117935061454773 Validation loss 0.08125913888216019\n",
      "Epoch 27820/30000  training loss: 0.08116455376148224 Validation loss 0.08124454319477081\n",
      "Epoch 27830/30000  training loss: 0.08114974200725555 Validation loss 0.08122996240854263\n",
      "Epoch 27840/30000  training loss: 0.08113493770360947 Validation loss 0.08121537417173386\n",
      "Epoch 27850/30000  training loss: 0.08112012594938278 Validation loss 0.08120079338550568\n",
      "Epoch 27860/30000  training loss: 0.0811053216457367 Validation loss 0.0811861976981163\n",
      "Epoch 27870/30000  training loss: 0.0810905173420906 Validation loss 0.08117160946130753\n",
      "Epoch 27880/30000  training loss: 0.08107571303844452 Validation loss 0.08115702122449875\n",
      "Epoch 27890/30000  training loss: 0.08106090873479843 Validation loss 0.08114244043827057\n",
      "Epoch 27900/30000  training loss: 0.08104610443115234 Validation loss 0.08112785965204239\n",
      "Epoch 27910/30000  training loss: 0.08103131502866745 Validation loss 0.08111326396465302\n",
      "Epoch 27920/30000  training loss: 0.08101655542850494 Validation loss 0.08109872788190842\n",
      "Epoch 27930/30000  training loss: 0.08100184053182602 Validation loss 0.081084243953228\n",
      "Epoch 27940/30000  training loss: 0.08098714053630829 Validation loss 0.08106973767280579\n",
      "Epoch 27950/30000  training loss: 0.08097244054079056 Validation loss 0.08105524629354477\n",
      "Epoch 27960/30000  training loss: 0.08095774054527283 Validation loss 0.08104075491428375\n",
      "Epoch 27970/30000  training loss: 0.0809430405497551 Validation loss 0.08102625608444214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27980/30000  training loss: 0.08092834055423737 Validation loss 0.08101177215576172\n",
      "Epoch 27990/30000  training loss: 0.08091363310813904 Validation loss 0.0809972733259201\n",
      "Epoch 28000/30000  training loss: 0.08089893311262131 Validation loss 0.08098278939723969\n",
      "Epoch 28010/30000  training loss: 0.08088423311710358 Validation loss 0.08096828311681747\n",
      "Epoch 28020/30000  training loss: 0.08086954057216644 Validation loss 0.08095380663871765\n",
      "Epoch 28030/30000  training loss: 0.0808548554778099 Validation loss 0.08093932271003723\n",
      "Epoch 28040/30000  training loss: 0.08084017038345337 Validation loss 0.08092484623193741\n",
      "Epoch 28050/30000  training loss: 0.08082547783851624 Validation loss 0.08091036230325699\n",
      "Epoch 28060/30000  training loss: 0.0808107927441597 Validation loss 0.08089586347341537\n",
      "Epoch 28070/30000  training loss: 0.08079610764980316 Validation loss 0.08088138699531555\n",
      "Epoch 28080/30000  training loss: 0.08078144490718842 Validation loss 0.08086691051721573\n",
      "Epoch 28090/30000  training loss: 0.08076678216457367 Validation loss 0.0808524563908577\n",
      "Epoch 28100/30000  training loss: 0.08075211942195892 Validation loss 0.08083800226449966\n",
      "Epoch 28110/30000  training loss: 0.08073747158050537 Validation loss 0.08082354068756104\n",
      "Epoch 28120/30000  training loss: 0.08072280883789062 Validation loss 0.080809086561203\n",
      "Epoch 28130/30000  training loss: 0.08070815354585648 Validation loss 0.08079463243484497\n",
      "Epoch 28140/30000  training loss: 0.08069350570440292 Validation loss 0.08078017830848694\n",
      "Epoch 28150/30000  training loss: 0.08067885786294937 Validation loss 0.0807657316327095\n",
      "Epoch 28160/30000  training loss: 0.08066421002149582 Validation loss 0.08075127005577087\n",
      "Epoch 28170/30000  training loss: 0.08064956218004227 Validation loss 0.08073682337999344\n",
      "Epoch 28180/30000  training loss: 0.08063492178916931 Validation loss 0.080722376704216\n",
      "Epoch 28190/30000  training loss: 0.08062026649713516 Validation loss 0.08070792257785797\n",
      "Epoch 28200/30000  training loss: 0.08060561865568161 Validation loss 0.08069346100091934\n",
      "Epoch 28210/30000  training loss: 0.08059102296829224 Validation loss 0.08067906647920609\n",
      "Epoch 28220/30000  training loss: 0.08057644963264465 Validation loss 0.08066467940807343\n",
      "Epoch 28230/30000  training loss: 0.08056189119815826 Validation loss 0.08065032213926315\n",
      "Epoch 28240/30000  training loss: 0.08054734021425247 Validation loss 0.08063595741987228\n",
      "Epoch 28250/30000  training loss: 0.08053279668092728 Validation loss 0.08062160760164261\n",
      "Epoch 28260/30000  training loss: 0.08051825314760208 Validation loss 0.08060724288225174\n",
      "Epoch 28270/30000  training loss: 0.08050370961427689 Validation loss 0.08059289306402206\n",
      "Epoch 28280/30000  training loss: 0.08048916608095169 Validation loss 0.0805785208940506\n",
      "Epoch 28290/30000  training loss: 0.0804746225476265 Validation loss 0.08056416362524033\n",
      "Epoch 28300/30000  training loss: 0.0804600715637207 Validation loss 0.08054980635643005\n",
      "Epoch 28310/30000  training loss: 0.08044552803039551 Validation loss 0.08053544908761978\n",
      "Epoch 28320/30000  training loss: 0.08043098449707031 Validation loss 0.08052107691764832\n",
      "Epoch 28330/30000  training loss: 0.08041644096374512 Validation loss 0.08050673454999924\n",
      "Epoch 28340/30000  training loss: 0.08040189743041992 Validation loss 0.08049236238002777\n",
      "Epoch 28350/30000  training loss: 0.08038736134767532 Validation loss 0.0804780125617981\n",
      "Epoch 28360/30000  training loss: 0.08037282526493073 Validation loss 0.08046366274356842\n",
      "Epoch 28370/30000  training loss: 0.08035828918218613 Validation loss 0.08044932037591934\n",
      "Epoch 28380/30000  training loss: 0.08034379035234451 Validation loss 0.08043498545885086\n",
      "Epoch 28390/30000  training loss: 0.0803292766213417 Validation loss 0.08042065799236298\n",
      "Epoch 28400/30000  training loss: 0.08031477779150009 Validation loss 0.08040633797645569\n",
      "Epoch 28410/30000  training loss: 0.08030027151107788 Validation loss 0.08039200305938721\n",
      "Epoch 28420/30000  training loss: 0.08028575778007507 Validation loss 0.08037768304347992\n",
      "Epoch 28430/30000  training loss: 0.08027125895023346 Validation loss 0.08036335557699203\n",
      "Epoch 28440/30000  training loss: 0.08025674521923065 Validation loss 0.08034902811050415\n",
      "Epoch 28450/30000  training loss: 0.08024224638938904 Validation loss 0.08033470064401627\n",
      "Epoch 28460/30000  training loss: 0.08022774755954742 Validation loss 0.08032037317752838\n",
      "Epoch 28470/30000  training loss: 0.08021325618028641 Validation loss 0.0803060531616211\n",
      "Epoch 28480/30000  training loss: 0.0801987573504448 Validation loss 0.0802917331457138\n",
      "Epoch 28490/30000  training loss: 0.08018425852060318 Validation loss 0.08027741312980652\n",
      "Epoch 28500/30000  training loss: 0.08016975969076157 Validation loss 0.08026310056447983\n",
      "Epoch 28510/30000  training loss: 0.08015532046556473 Validation loss 0.08024882525205612\n",
      "Epoch 28520/30000  training loss: 0.08014088124036789 Validation loss 0.0802345722913742\n",
      "Epoch 28530/30000  training loss: 0.08012647181749344 Validation loss 0.08022033423185349\n",
      "Epoch 28540/30000  training loss: 0.08011207729578018 Validation loss 0.08020609617233276\n",
      "Epoch 28550/30000  training loss: 0.08009766787290573 Validation loss 0.08019185066223145\n",
      "Epoch 28560/30000  training loss: 0.08008326590061188 Validation loss 0.08017762750387192\n",
      "Epoch 28570/30000  training loss: 0.08006887137889862 Validation loss 0.0801633819937706\n",
      "Epoch 28580/30000  training loss: 0.08005446940660477 Validation loss 0.08014915138483047\n",
      "Epoch 28590/30000  training loss: 0.08004006743431091 Validation loss 0.08013492077589035\n",
      "Epoch 28600/30000  training loss: 0.08002567291259766 Validation loss 0.08012069761753082\n",
      "Epoch 28610/30000  training loss: 0.0800112783908844 Validation loss 0.0801064595580101\n",
      "Epoch 28620/30000  training loss: 0.07999688386917114 Validation loss 0.08009224385023117\n",
      "Epoch 28630/30000  training loss: 0.07998249679803848 Validation loss 0.08007802069187164\n",
      "Epoch 28640/30000  training loss: 0.07996810227632523 Validation loss 0.08006377518177032\n",
      "Epoch 28650/30000  training loss: 0.07995370775461197 Validation loss 0.0800495520234108\n",
      "Epoch 28660/30000  training loss: 0.0799393281340599 Validation loss 0.08003532886505127\n",
      "Epoch 28670/30000  training loss: 0.07992494106292725 Validation loss 0.08002110570669174\n",
      "Epoch 28680/30000  training loss: 0.07991054654121399 Validation loss 0.08000687509775162\n",
      "Epoch 28690/30000  training loss: 0.07989618182182312 Validation loss 0.07999265938997269\n",
      "Epoch 28700/30000  training loss: 0.07988181710243225 Validation loss 0.07997847348451614\n",
      "Epoch 28710/30000  training loss: 0.07986745983362198 Validation loss 0.07996426522731781\n",
      "Epoch 28720/30000  training loss: 0.07985309511423111 Validation loss 0.07995006442070007\n",
      "Epoch 28730/30000  training loss: 0.07983874529600143 Validation loss 0.07993587106466293\n",
      "Epoch 28740/30000  training loss: 0.07982439547777176 Validation loss 0.07992168515920639\n",
      "Epoch 28750/30000  training loss: 0.07981004565954208 Validation loss 0.07990748435258865\n",
      "Epoch 28760/30000  training loss: 0.07979568094015121 Validation loss 0.07989328354597092\n",
      "Epoch 28770/30000  training loss: 0.07978133112192154 Validation loss 0.07987909764051437\n",
      "Epoch 28780/30000  training loss: 0.07976698130369186 Validation loss 0.07986490428447723\n",
      "Epoch 28790/30000  training loss: 0.07975263148546219 Validation loss 0.0798506960272789\n",
      "Epoch 28800/30000  training loss: 0.07973828166723251 Validation loss 0.07983651757240295\n",
      "Epoch 28810/30000  training loss: 0.07972393184900284 Validation loss 0.07982231676578522\n",
      "Epoch 28820/30000  training loss: 0.07970958203077316 Validation loss 0.07980812340974808\n",
      "Epoch 28830/30000  training loss: 0.07969530671834946 Validation loss 0.07979399710893631\n",
      "Epoch 28840/30000  training loss: 0.07968101650476456 Validation loss 0.07977986335754395\n",
      "Epoch 28850/30000  training loss: 0.07966674864292145 Validation loss 0.07976575195789337\n",
      "Epoch 28860/30000  training loss: 0.07965249568223953 Validation loss 0.0797516405582428\n",
      "Epoch 28870/30000  training loss: 0.07963824272155762 Validation loss 0.07973753660917282\n",
      "Epoch 28880/30000  training loss: 0.0796239897608757 Validation loss 0.07972344011068344\n",
      "Epoch 28890/30000  training loss: 0.07960973680019379 Validation loss 0.07970932871103287\n",
      "Epoch 28900/30000  training loss: 0.07959549129009247 Validation loss 0.0796952173113823\n",
      "Epoch 28910/30000  training loss: 0.07958123087882996 Validation loss 0.07968112826347351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28920/30000  training loss: 0.07956698536872864 Validation loss 0.07966702431440353\n",
      "Epoch 28930/30000  training loss: 0.07955274730920792 Validation loss 0.07965292036533356\n",
      "Epoch 28940/30000  training loss: 0.0795384868979454 Validation loss 0.07963880896568298\n",
      "Epoch 28950/30000  training loss: 0.07952424138784409 Validation loss 0.0796247124671936\n",
      "Epoch 28960/30000  training loss: 0.07950999587774277 Validation loss 0.07961060106754303\n",
      "Epoch 28970/30000  training loss: 0.07949575036764145 Validation loss 0.07959651201963425\n",
      "Epoch 28980/30000  training loss: 0.07948149740695953 Validation loss 0.07958240061998367\n",
      "Epoch 28990/30000  training loss: 0.07946726679801941 Validation loss 0.0795683041214943\n",
      "Epoch 29000/30000  training loss: 0.07945302873849869 Validation loss 0.07955421507358551\n",
      "Epoch 29010/30000  training loss: 0.07943878322839737 Validation loss 0.07954011112451553\n",
      "Epoch 29020/30000  training loss: 0.07942456752061844 Validation loss 0.07952604442834854\n",
      "Epoch 29030/30000  training loss: 0.07941035181283951 Validation loss 0.07951196283102036\n",
      "Epoch 29040/30000  training loss: 0.07939613610506058 Validation loss 0.07949788868427277\n",
      "Epoch 29050/30000  training loss: 0.07938192784786224 Validation loss 0.07948382198810577\n",
      "Epoch 29060/30000  training loss: 0.07936771214008331 Validation loss 0.07946974784135818\n",
      "Epoch 29070/30000  training loss: 0.07935351133346558 Validation loss 0.0794556736946106\n",
      "Epoch 29080/30000  training loss: 0.07933930307626724 Validation loss 0.0794416069984436\n",
      "Epoch 29090/30000  training loss: 0.07932509481906891 Validation loss 0.07942754775285721\n",
      "Epoch 29100/30000  training loss: 0.07931088656187057 Validation loss 0.07941346615552902\n",
      "Epoch 29110/30000  training loss: 0.07929667830467224 Validation loss 0.07939940690994263\n",
      "Epoch 29120/30000  training loss: 0.0792824774980545 Validation loss 0.07938533276319504\n",
      "Epoch 29130/30000  training loss: 0.07926826924085617 Validation loss 0.07937127351760864\n",
      "Epoch 29140/30000  training loss: 0.07925406843423843 Validation loss 0.07935720682144165\n",
      "Epoch 29150/30000  training loss: 0.0792398676276207 Validation loss 0.07934314757585526\n",
      "Epoch 29160/30000  training loss: 0.07922572642564774 Validation loss 0.07932912558317184\n",
      "Epoch 29170/30000  training loss: 0.07921158522367477 Validation loss 0.07931511849164963\n",
      "Epoch 29180/30000  training loss: 0.07919744402170181 Validation loss 0.07930111140012741\n",
      "Epoch 29190/30000  training loss: 0.07918333262205124 Validation loss 0.07928714156150818\n",
      "Epoch 29200/30000  training loss: 0.07916922122240067 Validation loss 0.07927315682172775\n",
      "Epoch 29210/30000  training loss: 0.07915510982275009 Validation loss 0.07925917208194733\n",
      "Epoch 29220/30000  training loss: 0.07914101332426071 Validation loss 0.0792451947927475\n",
      "Epoch 29230/30000  training loss: 0.07912690192461014 Validation loss 0.07923122495412827\n",
      "Epoch 29240/30000  training loss: 0.07911279052495956 Validation loss 0.07921724021434784\n",
      "Epoch 29250/30000  training loss: 0.07909869402647018 Validation loss 0.07920325547456741\n",
      "Epoch 29260/30000  training loss: 0.07908458262681961 Validation loss 0.07918927818536758\n",
      "Epoch 29270/30000  training loss: 0.07907047867774963 Validation loss 0.07917530834674835\n",
      "Epoch 29280/30000  training loss: 0.07905638217926025 Validation loss 0.07916133105754852\n",
      "Epoch 29290/30000  training loss: 0.07904227823019028 Validation loss 0.07914736121892929\n",
      "Epoch 29300/30000  training loss: 0.0790281742811203 Validation loss 0.07913338392972946\n",
      "Epoch 29310/30000  training loss: 0.07901407033205032 Validation loss 0.07911940664052963\n",
      "Epoch 29320/30000  training loss: 0.07899997383356094 Validation loss 0.0791054293513298\n",
      "Epoch 29330/30000  training loss: 0.07898586988449097 Validation loss 0.07909145206212997\n",
      "Epoch 29340/30000  training loss: 0.07897178083658218 Validation loss 0.07907746732234955\n",
      "Epoch 29350/30000  training loss: 0.0789576843380928 Validation loss 0.07906351238489151\n",
      "Epoch 29360/30000  training loss: 0.07894360274076462 Validation loss 0.07904954254627228\n",
      "Epoch 29370/30000  training loss: 0.07892952859401703 Validation loss 0.07903559505939484\n",
      "Epoch 29380/30000  training loss: 0.07891546189785004 Validation loss 0.07902165502309799\n",
      "Epoch 29390/30000  training loss: 0.07890139520168304 Validation loss 0.07900771498680115\n",
      "Epoch 29400/30000  training loss: 0.07888732850551605 Validation loss 0.0789937674999237\n",
      "Epoch 29410/30000  training loss: 0.07887326180934906 Validation loss 0.07897982001304626\n",
      "Epoch 29420/30000  training loss: 0.07885919511318207 Validation loss 0.07896587252616882\n",
      "Epoch 29430/30000  training loss: 0.07884514331817627 Validation loss 0.07895193994045258\n",
      "Epoch 29440/30000  training loss: 0.07883106917142868 Validation loss 0.07893799245357513\n",
      "Epoch 29450/30000  training loss: 0.07881700992584229 Validation loss 0.07892404496669769\n",
      "Epoch 29460/30000  training loss: 0.07880295068025589 Validation loss 0.07891010493040085\n",
      "Epoch 29470/30000  training loss: 0.0787888839840889 Validation loss 0.07889614999294281\n",
      "Epoch 29480/30000  training loss: 0.0787748172879219 Validation loss 0.07888221740722656\n",
      "Epoch 29490/30000  training loss: 0.07876075804233551 Validation loss 0.07886826246976852\n",
      "Epoch 29500/30000  training loss: 0.07874671369791031 Validation loss 0.07885434478521347\n",
      "Epoch 29510/30000  training loss: 0.07873272150754929 Validation loss 0.0788404643535614\n",
      "Epoch 29520/30000  training loss: 0.07871872931718826 Validation loss 0.07882658392190933\n",
      "Epoch 29530/30000  training loss: 0.07870473712682724 Validation loss 0.07881271094083786\n",
      "Epoch 29540/30000  training loss: 0.07869075238704681 Validation loss 0.07879884541034698\n",
      "Epoch 29550/30000  training loss: 0.07867679744958878 Validation loss 0.0787849873304367\n",
      "Epoch 29560/30000  training loss: 0.07866283506155014 Validation loss 0.07877112925052643\n",
      "Epoch 29570/30000  training loss: 0.07864886522293091 Validation loss 0.07875727862119675\n",
      "Epoch 29580/30000  training loss: 0.07863490283489227 Validation loss 0.07874342054128647\n",
      "Epoch 29590/30000  training loss: 0.07862094044685364 Validation loss 0.07872957736253738\n",
      "Epoch 29600/30000  training loss: 0.078606978058815 Validation loss 0.0787157192826271\n",
      "Epoch 29610/30000  training loss: 0.07859301567077637 Validation loss 0.07870186120271683\n",
      "Epoch 29620/30000  training loss: 0.07857903838157654 Validation loss 0.07868799567222595\n",
      "Epoch 29630/30000  training loss: 0.0785650834441185 Validation loss 0.07867413759231567\n",
      "Epoch 29640/30000  training loss: 0.07855112105607986 Validation loss 0.07866030186414719\n",
      "Epoch 29650/30000  training loss: 0.07853716611862183 Validation loss 0.07864644378423691\n",
      "Epoch 29660/30000  training loss: 0.07852320373058319 Validation loss 0.07863260060548782\n",
      "Epoch 29670/30000  training loss: 0.07850924879312515 Validation loss 0.07861874997615814\n",
      "Epoch 29680/30000  training loss: 0.07849529385566711 Validation loss 0.07860489189624786\n",
      "Epoch 29690/30000  training loss: 0.07848134636878967 Validation loss 0.07859105616807938\n",
      "Epoch 29700/30000  training loss: 0.07846739143133163 Validation loss 0.07857721298933029\n",
      "Epoch 29710/30000  training loss: 0.07845345139503479 Validation loss 0.07856336981058121\n",
      "Epoch 29720/30000  training loss: 0.07843950390815735 Validation loss 0.07854953408241272\n",
      "Epoch 29730/30000  training loss: 0.0784255713224411 Validation loss 0.07853570580482483\n",
      "Epoch 29740/30000  training loss: 0.07841164618730545 Validation loss 0.07852189242839813\n",
      "Epoch 29750/30000  training loss: 0.0783977210521698 Validation loss 0.07850805670022964\n",
      "Epoch 29760/30000  training loss: 0.07838379591703415 Validation loss 0.07849423587322235\n",
      "Epoch 29770/30000  training loss: 0.0783698707818985 Validation loss 0.07848040759563446\n",
      "Epoch 29780/30000  training loss: 0.07835593819618225 Validation loss 0.07846658676862717\n",
      "Epoch 29790/30000  training loss: 0.0783420130610466 Validation loss 0.07845275849103928\n",
      "Epoch 29800/30000  training loss: 0.07832808792591095 Validation loss 0.07843894511461258\n",
      "Epoch 29810/30000  training loss: 0.0783141702413559 Validation loss 0.07842512428760529\n",
      "Epoch 29820/30000  training loss: 0.07830024510622025 Validation loss 0.07841130346059799\n",
      "Epoch 29830/30000  training loss: 0.07828634232282639 Validation loss 0.0783974900841713\n",
      "Epoch 29840/30000  training loss: 0.07827243208885193 Validation loss 0.078383669257164\n",
      "Epoch 29850/30000  training loss: 0.07825851440429688 Validation loss 0.0783698558807373\n",
      "Epoch 29860/30000  training loss: 0.07824459671974182 Validation loss 0.07835604250431061\n",
      "Epoch 29870/30000  training loss: 0.07823071628808975 Validation loss 0.07834227383136749\n",
      "Epoch 29880/30000  training loss: 0.07821687310934067 Validation loss 0.07832852005958557\n",
      "Epoch 29890/30000  training loss: 0.07820302993059158 Validation loss 0.07831477373838425\n",
      "Epoch 29900/30000  training loss: 0.0781891793012619 Validation loss 0.07830102741718292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29910/30000  training loss: 0.07817532867193222 Validation loss 0.0782872661948204\n",
      "Epoch 29920/30000  training loss: 0.07816149294376373 Validation loss 0.07827351987361908\n",
      "Epoch 29930/30000  training loss: 0.07814767211675644 Validation loss 0.07825978845357895\n",
      "Epoch 29940/30000  training loss: 0.07813384383916855 Validation loss 0.07824604958295822\n",
      "Epoch 29950/30000  training loss: 0.07812002301216125 Validation loss 0.0782323107123375\n",
      "Epoch 29960/30000  training loss: 0.07810619473457336 Validation loss 0.07821858674287796\n",
      "Epoch 29970/30000  training loss: 0.07809237390756607 Validation loss 0.07820484787225723\n",
      "Epoch 29980/30000  training loss: 0.07807855308055878 Validation loss 0.0781911313533783\n",
      "Epoch 29990/30000  training loss: 0.07806473225355148 Validation loss 0.07817739993333817\n"
     ]
    }
   ],
   "source": [
    "loss, loss_val = model.train(X_train, Y_train, X_val, y_val, 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Mean Square Error')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fac4bc60208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss, label=\"Taining Loss\")\n",
    "plt.plot(loss_val, label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Mean Square Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
